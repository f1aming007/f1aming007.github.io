[{"categories":["文档"],"content":"Golang TCP 端口扫描 非并发版 package main import ( \"fmt\" \"net\" ) func main() { for i := 21; i \u003c 120; i++ { address := fmt.Sprintf(\"192.168.0.2:%d\", i) conn, err := net.Dial(\"tcp\", address) if err != nil { fmt.Printf(\"%d 关闭了\\n\", address) continue } conn.Close() fmt.Printf(\"%d dakai了\\n\", address) } } 并发版 package main import ( \"fmt\" \"net\" \"sync\" \"time\" ) func main() { start := time.Now() var wg sync.WaitGroup for i := 21; i \u003c 120; i++ { wg.Add(1) go func(j int) { address := fmt.Sprintf(\"192.168.0.2:%d\", j) conn, err := net.Dial(\"tcp\", address) if err != nil { fmt.Printf(\"%d 关闭了\\n\", address) return } conn.Close() fmt.Printf(\"%d dakai了\\n\", address) }(i) wg.Wait() elapsed := time.Since(start) / 1e9 fmt.Printf(\"\\n\\n%d\\n\", elapsed) } } goroutine 池并发版 TCP 端口扫描器 go-work (figure) package main import ( \"fmt\" \"net\" \"sort\" ) func worker(ports chan int, results chan int) { for p := range ports { address := fmt.Sprintf(\"192.168.1.1:%d\", p) conn, err := net.Dial(\"tcp\", address) if err != nil { results \u003c- 0 fmt.Printf(\"%d 关闭了 \\n\", p) } conn.Close() fmt.Printf(\"%d 打开了\", p) results \u003c- p } } func main() { ports := make(chan int, 100) results := make(chan int) var openports []int var closeports []int for i := 0; i \u003c cap(ports); i++ { go worker(ports, results) } go func() { for i := 1; i \u003c 1024; i++ { ports \u003c- i } }() for i := 1; i \u003c 1024; i++ { port := \u003c- results if port != 0 { openports = append(openports, port) } else { closeports = append(closeports, port) } } close(ports) close(results) sort.Ints(openports) sort.Ints(closeports) for _, port := range openports { fmt.Printf(\"%d open\\n\", port) } for _, port := range closeports { fmt.Printf(\"%d close\\n\", port) } } ","date":"2023-05-26","objectID":"/golang-portscan/:0:0","tags":["golang"],"title":"Golang PortScan","uri":"/golang-portscan/"},{"categories":["文档"],"content":"K8s-GVK\u0026FVR ","date":"2023-05-26","objectID":"/k8s-gvk_gvr/:0:0","tags":["k8s"],"title":"K8s GVK_GVR","uri":"/k8s-gvk_gvr/"},{"categories":["文档"],"content":"GVK \u0026 GVR 介绍 GVK —— group 、version、 kind GVR —— group 、version 、resource ","date":"2023-05-26","objectID":"/k8s-gvk_gvr/:1:0","tags":["k8s"],"title":"K8s GVK_GVR","uri":"/k8s-gvk_gvr/"},{"categories":["文档"],"content":"kind 和 resource 概念 在编码过程中， 资源数据存储都是结构体存储 多版本version存储在（alpha1，beta1，v1等），不同版本中存储结构体的存在着差异 用 Kind 名（如 Deployment），并不能准确获取到其使用哪个版本结构体 采用 GVK 获取到一个具体的 存储结构体，也就是 GVK 的三个信息（group/verion/kind) 确定一个 Go type（结构体） Scheme 存储了 GVK 和 Go type 的映射关系 创建资源， 编写yaml，提交请求 编写 yaml 过程中，我们会写 apiversion 和 kind，其实就是 GVK 与 apiserver 通信是 http 形式，就是将请求发送到某一 http path http path 其实就是 GVR /apis/batch/v1/namespaces/default/job 这个就是表示 default 命名空间的 job 资源 ","date":"2023-05-26","objectID":"/k8s-gvk_gvr/:1:1","tags":["k8s"],"title":"K8s GVK_GVR","uri":"/k8s-gvk_gvr/"},{"categories":["文档"],"content":"相同名称 Kind 可以存在不同组 相同名称的kind不仅可以存在不同的Version， 也可以存在不同的group Ingress, NetworkPolicy同时在这个两个API Group： extensions、 networking.k8s.io Deployment, DaemonSet, ReplicaSet同时在这些API Group中：extensions、 apps Event同时在这些API Group中： core group and events.k8s.io ","date":"2023-05-26","objectID":"/k8s-gvk_gvr/:2:0","tags":["k8s"],"title":"K8s GVK_GVR","uri":"/k8s-gvk_gvr/"},{"categories":["文档"],"content":"API-group 资源分组 各组可以单独打开或者关闭 各组可以有自己独立的版本， 不影响其他组的情况下可以单独衍化 同一个资源可以同时存在于多个不同组中，这样就可以同时支持某个特定资源稳定版本与实验版本 kubernetes API 查看当前 kubernetes 集群支持的 API 版本 $ kubectl api-versions apiextensions.k8s.io/v1beta1 apiregistration.k8s.io/v1beta1 apps/v1beta1 apps/v1beta2 authentication.k8s.io/v1 authentication.k8s.io/v1beta1 authorization.k8s.io/v1 authorization.k8s.io/v1beta1 autoscaling/v1 autoscaling/v2beta1 batch/v1 batch/v1beta1 certificates.k8s.io/v1beta1 custom-metrics.metrics.k8s.io/v1alpha1 extensions/v1beta1 monitoring.coreos.com/v1 networking.k8s.io/v1 policy/v1beta1 rbac.authorization.k8s.io/v1 rbac.authorization.k8s.io/v1beta1 storage.k8s.io/v1 storage.k8s.io/v1beta1 v1 对于同一个资源对象的不同版本，API-Server 负责不同版本之间的无损切换，这点对于客户端来说是完全透明的（无感知）。 事实上，不同版本的同类型的资源在持久化层的数据可能是相同的。 例如，对于同一种资源类型支持 v1 和 v1beta1 两个 API 版本，以 v1beta1 版本创建该资源的对象，后续可以以v1 或者 v1beta1 来更新或者删除该资源对象 ","date":"2023-05-26","objectID":"/k8s-gvk_gvr/:3:0","tags":["k8s"],"title":"K8s GVK_GVR","uri":"/k8s-gvk_gvr/"},{"categories":["文档"],"content":"GVK 与 GVR 映射 kind 是API 顶级 资源对象类型，每个资源对象都需要kind来区分自身代表的类型 kind 字段 该资源对象的类型 单个的资源对象类型（pod） 资源对象列表类型， （podlist 或者 nodelist） 特殊类型以及非吃就好类型 （很多这种类型的资源是 subresource， 例如用于绑定资源的 /binding、更新资源状态的 /status 以及读写资源实例数量的 /scale） Resource 通过http协议以JSON格式发送或者读取的资源展现形式 单个资源 （…/namespaces/default） 列表资源（…/jobs） GVR 常用于组合成 RESTful API 请求路径 GET /apis/apps/v1/namespaces/{namespace}/deployments/{name} ","date":"2023-05-26","objectID":"/k8s-gvk_gvr/:4:0","tags":["k8s"],"title":"K8s GVK_GVR","uri":"/k8s-gvk_gvr/"},{"categories":["文档"],"content":"API 存储 API-Server 是无状态的，它需要与分布式存储系统 etcd 交互来实现资源对象的持久化操作 Kubernetes 资源对象是以JSON或 Protocol Buffers 格式存储在 etcd 中 以通过配置 kube-apiserver 的启动参数 –storage-media-type 来决定想要序列化数据存入 etcd 的格式 创建一个 pod，然后使用 etcdctl 工具来查看存储在 etcd 中数据 $ cat \u003c\u003c EOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: webserver spec: containers: - name: nginx image: nginx ports: - containerPort: 80 EOF pod/webserver created $ etcdctl --endpoints=$ETCD_URL \\ --cert /etc/kubernetes/pki/etcd/server.crt \\ --key /etc/kubernetes/pki/etcd/server.key \\ --cacert /etc/kubernetes/pki/etcd/ca.crt \\ get /registry/pods/default/webserver --prefix -w simple /registry/pods/default/webserver ... 10.244.0.5\" 客户端工具创建资源对象到然后存储到 etcd 的流程 客户端工具(kubectl) 提供一个期望状态的资源对象的序列化表示，是一样yaml格式提供 kubectl 将YAML转换为JSON 格式， 并发送给API -server 对应同类型对象的不同版本，API- SERVER 执行无损转换，对于老版本不存在的字段则存储在annotations中 API- SERVER 将接收到的对象转换为规范存储版本，这个版本由API-Server启动参数指定，通常是最稳定的版本 最后将资源对象通过JSON 或YAML方式解析并通过一个特定的key 存入etcd中 ","date":"2023-05-26","objectID":"/k8s-gvk_gvr/:5:0","tags":["k8s"],"title":"K8s GVK_GVR","uri":"/k8s-gvk_gvr/"},{"categories":["文档"],"content":"Liveness\u0026Readines ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:0:0","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"需求来源 实时观察应用的健康状态 获取应用的资源使用情况 拿到应用的实时日志， 进行问题的诊断和分析 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:1:0","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"Liveness probe 和 Readiness probe 介绍 livenness probe 是就绪指针，判断一个pod是否处于就绪状态。 当一个pod处于就绪状态的时候 才能对外提供对应的服务，接入层的流量才能打入相应的pod 当这个pod不处于就绪状态， 接入层灰吧相应的流量从这个pod上面剔除。 Liveness01 (figure) 这个pod指针一直处于失败， 接入层流量不会打到这个pod 这个 pod 的状态从 FAIL 的状态转换成 success 的状态时，它才能够真实地承载这个流量 这个时候会由上层的判断机制来判断这个 pod 是否需要被重新拉起。那如果上层配置的重启策略是 restart always 的话，那么此时这个 pod 会直接被重新拉起。 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:2:0","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"应用健康状态-使用方式 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:3:0","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"探测方式 liveness指针和Readkiness指针支持三种不同的探测方式 httpGet: 通过发送http 个体请求进行判断， 当返回码是200～399 ，标识这个应用是健康的 Exec: 执行容器中的一个命令判断当前容器是否正常，当返回时0，标识容器时健康的 tcpSocket： 通过探测容器的IP和Port 进行TCP健康检查， TCP能正常建立， 标识这个容器时健康的 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:3:1","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"探测结果 Success: 标识Container 通过健康检查 Failure: container没有通过健康检查，此时会有一个相应的处理，Readiness——（service层将没有通过Readiness的pod进行移除）liveness 将这个pod重新拉起， 或者删除 Unknown: 当前的执行机制没有完成执行（超时或者一些脚本没有及时返回），等待下次的机制来进行校验 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:3:2","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"应用健康状态检查方式-pod Probe spec ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:4:0","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"exec 通过cat 一个具体文件来判断当前Liveness probe 的状态，返回0 这个pod 处于健康状态 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:4:1","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"httpGet ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:4:2","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"tcpSocket 参数说明： initialDelaySeconds： pod启动延迟多久进行一次检查 periodSeconds： 检查的时间间隔，默认值是10秒 timeoutSeconds： 检查的超时时间 successThreshold： pod从探测失败到再一次探测成功， 锁需要的阀值次数 failureThreshold： 探测失败的重启次数， 默认值是3 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:4:3","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"Liveness 与 Readiness 总结 liveness: 判断容器是否处于running ,如果容器不健康，会通过kubelet 杀掉相应的pod Readiness： 判断容器是否启动完成，如果探测一个结果不成功， 会从pod上Endpoint上移除。 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:5:0","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"适用场景 liveness： 支持可以重新拉起的应用 Readiness： 应对 启动后无法立即对外提供服务的应用。 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:5:1","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"应用故障排除——状态机制 k8s 的状态机制， 通过yaml的方式定义个期望到达的状态。 这个yaml的真正执行过程 各种各样的controller来负责整体的状态之间的转换 一个 Pod 的一个生命周期。刚开始它处在一个 pending 的状态，那接下来可能会转换到类似像 running，也可能转换到 Unknown，甚至可以转换到 failed。然后，当 running 执行了一段时间之后，它可以转换到类似像 successded 或者是 failed，然后当出现在 unknown 这个状态时，可能由于一些状态的恢复，它会重新恢复到 running 或者 successded 或者是 failed。 k8s 整体的一个状态就是状态机制的转换 一个pod状态位的展现 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:6:0","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"应用故障排除-场景应用异常 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:7:0","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"pod停留在Pending 标识没有调度器进行介入 可能是资源或端口占用 由于node selector 造成的pod无法调度 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:7:1","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"Pod 停留在 waiting 表示pod的镜像没有正常拉取 可能原因 私有镜像 没有配置pod secret 镜像地址不存在 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:7:2","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"Pod 不断被拉取并且可以看到 crashing 标识pod已经调度完成， 但是启动失败 需要关注应用的自身状态 查看pod的具体日志 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:7:3","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"Pod 处在 Runing 但是没有正常工作 常见问题——yaml信息里可能有拼写错误 apply-validate -f pod.yaml 判断当前的yaml是否正确 如果没有问题，诊断配置的端口是否正常，以及liveness或Readiness 配置是否正确 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:7:4","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"Service 无法正常的工作 因为service和底层pod之间的关联是通过selector的方式进行匹配的， pod上面配置label， 然后service通过match label的方式和pod进行关联 如果lable配置有问题，可能会造成service无法找到后面的endpoint 造成相应的service无法对外提供服务 排除这种问题 首先 看service 后面的是不是有一个真正的endpoint 其次 看这个endpoint 是否可以对外提供正常的服务 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:7:5","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"应用远程调试 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:8:0","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"应用远程调试 - Pod 远程调试 进入容器里进行诊断 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:8:1","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"应用远程调试 - Service 远程调试 service调试 分了两部分 将服务暴露到远程的一个集群之内，让远程集群的一些应用去调用本地的一个服务，这是一条反向的一个链路 让本地的服务能够远程调用服务， 这是一条正向的链路 将 Telepresence 的一个 Proxy 应用部署到远程的 K8s 集群里面。然后将远程单一个 deployment swap 到本地的一个 application，使用的命令就是 Telepresence-swap-deployment 然后以及远程的 DEPLOYMENT_NAME。通过这种方式就可以将本地一个 application 代理到远程的 service 之上、可以将应用在远程集群里面进行本地调试 使用方式是 kubectl port-forward，然后 service 加上远程的 service name，再加上相应的 namespace，后面还可以加上一些额外的参数，比如说端口的一个映射，通过这种机制就可以把远程的一个应用代理到本地的端口之上，此时通过访问本地端口就可以访问远程的服务。 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:8:2","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"kubectl-debug kubectl-debug 这个工具是依赖于 Linux namespace 的方式来去做的，它可以 datash 一个 Linux namespace 到一个额外的 container，然后在这个 container 里面执行任何的 debug 动作，其实和直接去 debug 这个 Linux namespace 是一致的。这里有一个简单的操作 通过 kubectl-debug 这条命令来去诊断远程的一个 pod 执行debug的时候，首先会拉取一些镜像， 这个镜像里会默认带一些诊断工具 当这个镜像启用的时候，它会把这个 debug container 进行启动 这个 container 和相应的你要诊断的这个 container 的 namespace 进行挂靠，类似像网络站，或者是类似像内核的一些参数，可以在这个 debug container 里面实时地进行查看。 去查看类似像 hostname、进程、netstat 等等，这个容器和需要debug的pod在同一个环境里 进行 logout 的话，相当于会把相应的这个 debug pod 杀掉 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:9:0","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"Etcd etcd 是一个开源的， 高可用的分布式的 key-value 存储系统， 可以用于配制共享和服务的注册和发现 etcd具有的特点 完全复制： 集群中的每个阶段都可以使用完整的文档 高可用性： Etcd可用于避免硬件的单点故障或网络问题 一致性： 每次读取都会返回跨多主机的最新写入 简单： 包括一个定义良好、面向用户的API（gRPC） 安全： 实现了带有可选的客户端证书身份验证的自动化TLS 快速： 每秒10000次写入的基准速度 可靠： 用Raft算法实现了强一致、高可用的服务存储目录 ","date":"2023-05-23","objectID":"/golang-etcd/:0:0","tags":["golang","etcd"],"title":"Golang Etcd","uri":"/golang-etcd/"},{"categories":["文档"],"content":"etcd 的应用场景 服务发现 服务发现要解决的也是分布式系统中最常见的问题之一，即在同一个分布式集群中的进程或服务，要如何才能找到对方并建立连接。本质上来说，服务发现就是想要了解集群中是否有进程在监听 udp 或 tcp 端口，并且通过名字就可以查找和连接。 Registry (figure) 配置中心 将一些配置存放在etcd 上集中管理 这类场景的使用方式通常是这样：应用在启动的时候主动从 etcd 获取一次配置信息，同时，在 etcd 节点上注册一个 Watcher 并等待，以后每次配置有更新的时候，etcd 都会实时通知订阅者，以此达到获取最新配置信息的目的 分布式锁 因为 etcd 使用 Raft 算法保持了数据的强一致性，某次操作存储到集群中的值必然是全局一致的，所以很容易实现分布式锁。锁服务有两种使用方式，一是保持独占，二是控制时序。 保持独占即所有获取锁的用户最终只有一个可以得到。etcd 为此提供了一套实现分布式锁原子操作 CAS（CompareAndSwap）的 API。通过设置prevExist值，可以保证在多个节点同时去创建某个目录时，只有一个成功。而创建成功的用户就可以认为是获得了锁。 控制时序，即所有想要获得锁的用户都会被安排执行，但是获得锁的顺序也是全局唯一的，同时决定了执行顺序。etcd 为此也提供了一套 API（自动创建有序键），对一个目录建值时指定为POST动作，这样 etcd 会自动在目录下生成一个当前最大的值为键，存储这个新的值（客户端编号）。同时还可以使用 API 按顺序列出所有当前目录下的键值。此时这些键的值就是客户端的时序，而这些键中存储的值可以是代表客户端的编号 etcd_lock (figure) ","date":"2023-05-23","objectID":"/golang-etcd/:1:0","tags":["golang","etcd"],"title":"Golang Etcd","uri":"/golang-etcd/"},{"categories":["文档"],"content":"go 操作etcd ","date":"2023-05-23","objectID":"/golang-etcd/:2:0","tags":["golang","etcd"],"title":"Golang Etcd","uri":"/golang-etcd/"},{"categories":["文档"],"content":"安装 go get go.etcd.io/etcd/client/v3 ","date":"2023-05-23","objectID":"/golang-etcd/:3:0","tags":["golang","etcd"],"title":"Golang Etcd","uri":"/golang-etcd/"},{"categories":["文档"],"content":"put和get 操作 package main import ( \"context\" \"fmt\" \"time\" clientv3 \"go.etcd.io/etcd/client/v3\" ) // etcd client put/get demo // use etcd/clientv3 func main() { cli, err := clientv3.New(clientv3.Config{ Endpoints: []string{\"127.0.0.1:2379\"}, DialTimeout: 5 * time.Second, }) if err != nil { // handle error! fmt.Printf(\"connect to etcd failed, err:%v\\n\", err) return } fmt.Println(\"connect to etcd success\") defer cli.Close() // put ctx, cancel := context.WithTimeout(context.Background(), time.Second) _, err = cli.Put(ctx, \"demo\", \"dsb\") cancel() if err != nil { fmt.Printf(\"put to etcd failed, err:%v\\n\", err) return } // get ctx, cancel = context.WithTimeout(context.Background(), time.Second) resp, err := cli.Get(ctx, \"demo\") cancel() if err != nil { fmt.Printf(\"get from etcd failed, err:%v\\n\", err) return } for _, ev := range resp.Kvs { fmt.Printf(\"%s:%s\\n\", ev.Key, ev.Value) } } ","date":"2023-05-23","objectID":"/golang-etcd/:4:0","tags":["golang","etcd"],"title":"Golang Etcd","uri":"/golang-etcd/"},{"categories":["文档"],"content":"watch操作 watch 用来获取未来更改的通知 package main import ( \"context\" \"fmt\" \"time\" clientv3 \"go.etcd.io/etcd/client/v3\" ) // watch demo func main() { cli, err := clientv3.New(clientv3.Config{ Endpoints: []string{\"127.0.0.1:2379\"}, DialTimeout: 5 * time.Second, }) if err != nil { fmt.Printf(\"connect to etcd failed, err:%v\\n\", err) return } fmt.Println(\"Connect to etcd successful\") defer cli.Close() // watch keys:demo changes rch := cli.Watch(context.Background(), \"demo\") for wresp := range rch { for _, ev := range wresp.Events { fmt.Printf(\"Type: %s Key:%s Value:%s\\n\", ev.Type, ev.Kv.Key, ev.Kv.Value) } } } 这个程序会等待 etcd 中 demo 这个key的变化 打开终端对 demo 这个命令 设置 etcdctl put demo \"dsb01\" OK etcdctl del demo 1 etcdctl put demo \"ddd03\" OK watch 会收到的通知 Connect to etcd successful Type: PUT Key:demo Value:dsb01 Type: DELETE Key:demo Value: Type: PUT Key:demo Value:ddd03 ","date":"2023-05-23","objectID":"/golang-etcd/:4:1","tags":["golang","etcd"],"title":"Golang Etcd","uri":"/golang-etcd/"},{"categories":["文档"],"content":"lease 租约 package main import ( \"context\" \"fmt\" \"log\" \"time\" clientv3 \"go.etcd.io/etcd/client/v3\" ) // watch demo func main() { cli, err := clientv3.New(clientv3.Config{ Endpoints: []string{\"127.0.0.1:2379\"}, DialTimeout: 5 * time.Second, }) if err != nil { fmt.Printf(\"connect to etcd failed, err:%v\\n\", err) return } fmt.Println(\"Connect to etcd successful\") defer cli.Close() //etcd lease // 创建一个5s的租约 resp, err := cli.Grant(context.TODO(), 5) if err != nil { log.Fatal(err.Error()) } // 5s之后， /demo/ 这个key就会被移除 _, err = cli.Put(context.TODO(), \"/demo/\", \"bbb\", clientv3.WithLease(resp.ID)) if err != nil { log.Fatal(err.Error()) } } ","date":"2023-05-23","objectID":"/golang-etcd/:5:0","tags":["golang","etcd"],"title":"Golang Etcd","uri":"/golang-etcd/"},{"categories":["文档"],"content":"keepAlive package main import ( \"context\" \"fmt\" \"log\" \"time\" clientv3 \"go.etcd.io/etcd/client/v3\" ) // watch demo func main() { cli, err := clientv3.New(clientv3.Config{ Endpoints: []string{\"127.0.0.1:2379\"}, DialTimeout: 5 * time.Second, }) if err != nil { fmt.Printf(\"connect to etcd failed, err:%v\\n\", err) return } fmt.Println(\"Connect to etcd successful\") defer cli.Close() //etcd lease // 创建一个5s的租约 resp, err := cli.Grant(context.TODO(), 5) if err != nil { log.Fatal(err.Error()) } // 5s之后， /demo/ 这个key就会被移除 _, err = cli.Put(context.TODO(), \"/demo/\", \"bbb\", clientv3.WithLease(resp.ID)) if err != nil { log.Fatal(err.Error()) } // the key 'foo' will be kept forever ch, haerr := cli.KeepAlive(context.TODO(), resp.ID) if haerr != nil { log.Fatal(err.Error()) } for { ha := \u003c-ch fmt.Println(\"ttl: \", ha.TTL) } } ","date":"2023-05-23","objectID":"/golang-etcd/:6:0","tags":["golang","etcd"],"title":"Golang Etcd","uri":"/golang-etcd/"},{"categories":["文档"],"content":"基于etcd 实现分布式锁 go.etcd.io/etcd/client/v3/concurrency 在etcd之上实现并发操作， 如分布式锁/屏障和选举 倒入包 import \"go.etcd.io/etcd/client/v3/concurrency\" 基于etcd 实现分布式锁的示例 package main import ( \"context\" \"fmt\" \"log\" \"time\" clientv3 \"go.etcd.io/etcd/client/v3\" \"go.etcd.io/etcd/client/v3/concurrency\" ) // watch demo func main() { cli, err := clientv3.New(clientv3.Config{ Endpoints: []string{\"127.0.0.1:2379\"}, DialTimeout: 5 * time.Second, }) if err != nil { fmt.Printf(\"connect to etcd failed, err:%v\\n\", err) return } fmt.Println(\"Connect to etcd successful\") defer cli.Close() // 创建两个单独的会话来演示竞争锁 s1, err := concurrency.NewSession(cli) if err != nil { log.Fatal(err.Error()) } defer s1.Close() m1 := concurrency.NewMutex(s1, \"/my-lock/\") s2, err := concurrency.NewSession(cli) if err != nil { log.Fatal(err.Error()) } defer s2.Close() m2 := concurrency.NewMutex(s2, \"/my-lock/\") // 会话s1 获取锁 if err := m1.Lock(context.TODO()); err != nil { log.Fatal(err.Error()) } fmt.Println(\"acquired lock for s1\") m2Locked := make(chan struct{}) go func() { defer close(m2Locked) // 等待之后会话s1 释放了/my-lock/的锁 if err := m2.Lock(context.TODO()); err != nil { log.Fatal(err.Error()) } fmt.Println(\"acquired lock for s2\") }() if err := m1.Unlock(context.TODO()); err != nil { log.Fatal(err.Error()) } fmt.Println(\"release lock for s1\") \u003c-m2Locked fmt.Println(\"release lock for s2\") } 输出结果 Connect to etcd successful acquired lock for s1 release lock for s1 acquired lock for s2 release lock for s2 参考链接 https://pkg.go.dev/go.etcd.io/etcd/clientv3/concurrency https://github.com/etcd-io/etcd/tree/main/client/v3 ","date":"2023-05-23","objectID":"/golang-etcd/:7:0","tags":["golang","etcd"],"title":"Golang Etcd","uri":"/golang-etcd/"},{"categories":["文档"],"content":"MongoDB ","date":"2023-05-23","objectID":"/golang-mongodb/:0:0","tags":["golang","mongoDB"],"title":"Golang MangoDB","uri":"/golang-mongodb/"},{"categories":["文档"],"content":"MongoDB介绍 mongoDB是基于分布式文件存储的数据库，是一个介于关系型数据库和非关系数据库之间的产品 mongoDB将一条数据存储为一个文档， 数据结构由键值对组成， 其中文档类似我们编辑中用到的JSON对象，文档中的字段值可以包含其他文档/数组及文档数组 MongoDB术语 说明 SQL术语 database 数据库 database collection 集合 table document 文档 row field 字段 column index index 索引 primary key 主键 MongoDB自动将_id字段设置为主键 primary key ","date":"2023-05-23","objectID":"/golang-mongodb/:1:0","tags":["golang","mongoDB"],"title":"Golang MangoDB","uri":"/golang-mongodb/"},{"categories":["文档"],"content":"mongoDB安装 docker pull mongo:latest docker run -itd --name mongo -p 27017:27017 mongo --auth 参数说明 -p 27017:27017 ：映射容器服务的 27017 端口到宿主机的 27017 端口。外部可以直接通过 宿主机 ip:27017 访问到 mongo 的服务。 –auth：需要密码才能访问容器服务。 使用以下命令添加用户和设置密码，并且尝试连接 $ docker exec -it mongo mongosh admin # 创建一个名为 admin，密码为 123456 的用户。 \u003e db.createUser({ user:'admin',pwd:'123456',roles:[ { role:'userAdminAnyDatabase', db: 'admin'},\"readWriteAnyDatabase\"]}); # 尝试使用上面创建的用户信息进行连接。 \u003e db.auth('admin', '123456') 连接 mongo docker exec -it mongo mongosh admin ","date":"2023-05-23","objectID":"/golang-mongodb/:2:0","tags":["golang","mongoDB"],"title":"Golang MangoDB","uri":"/golang-mongodb/"},{"categories":["文档"],"content":"golang 操作mongo DB 安装mongoDB Go驱动包 go get github.com/mongodb/mongo-go-driver Go代码连接mongoDB package main import ( \"context\" \"fmt\" \"log\" \"go.mongodb.org/mongo-driver/mongo\" \"go.mongodb.org/mongo-driver/mongo/options\" ) func main() { // 设置客户端连接配置 clientOptions := options.Client().ApplyURI(\"mongodb://192.168.0.9:27017\") // 连接到MongoDB client, err := mongo.Connect(context.TODO(), clientOptions) if err != nil { log.Fatal(err) } // 检查连接 err = client.Ping(context.TODO(), nil) if err != nil { log.Fatal(err) } fmt.Println(\"Connected to MongoDB!\") } 处理数据集 // 指定获取要操作的数据集 collection := client.Database(\"demo\").Collection(\"student\") 处理完任务之后可以通过下面的命令断开与MongoDB的连接 // 断开连接 err = client.Disconnect(context.TODO()) if err != nil { log.Fatal(err) } fmt.Println(\"Connection to MongoDB closed.\") ","date":"2023-05-23","objectID":"/golang-mongodb/:3:0","tags":["golang","mongoDB"],"title":"Golang MangoDB","uri":"/golang-mongodb/"},{"categories":["文档"],"content":"连接池模式 import ( \"context\" \"time\" \"go.mongodb.org/mongo-driver/mongo\" \"go.mongodb.org/mongo-driver/mongo/options\" ) func ConnectToDB(uri, name string, timeout time.Duration, num uint64) (*mongo.Database, error) { ctx, cancel := context.WithTimeout(context.Background(), timeout) defer cancel() o := options.Client().ApplyURI(uri) o.SetMaxPoolSize(num) client, err := mongo.Connect(ctx, o) if err != nil { return nil, err } return client.Database(name), nil } ","date":"2023-05-23","objectID":"/golang-mongodb/:4:0","tags":["golang","mongoDB"],"title":"Golang MangoDB","uri":"/golang-mongodb/"},{"categories":["文档"],"content":"BSON mongoDB中的JSON文档存储在BSON的二进制表示中 BSON编码拓展了JSON表示，使其包含额外的类型如 int\\long\\date\\浮点数和decimal128， 是应用程序更容易可靠的处理、排序和比较数据 要使用BSON 需要倒入包 import \"go.mongodb.org/mongo-driver/bson\" 使用D类型构建的过滤文档的理智， 它可以用来查询name字段与’张三’或’李四’匹配的文档 bson.D{{ \"name\", bson.D{{ \"$in\", bson.A{\"张三\", \"李四\"}, }}, }} ","date":"2023-05-23","objectID":"/golang-mongodb/:5:0","tags":["golang","mongoDB"],"title":"Golang MangoDB","uri":"/golang-mongodb/"},{"categories":["文档"],"content":"CURD 定义一个Student类型 type Student struct { Name string Age int } 准备数据 s1 := Student{\"张三\", 12} s2 := Student{\"李四\", 10} s3 := Student{\"王五\", 11} 插入一条文档记录 insertResult, err := collection.InsertOne(context.TODO(), s1) if err != nil { log.Fatal(err) } fmt.Println(\"Inserted a single document: \", insertResult.InsertedID) 插入多条文档 students := []interface{}{s2, s3} insertManyResult, err := collection.InsertMany(context.TODO(), students) if err != nil { log.Fatal(err) } fmt.Println(\"Inserted multiple documents: \", insertManyResult.InsertedIDs) ","date":"2023-05-23","objectID":"/golang-mongodb/:6:0","tags":["golang","mongoDB"],"title":"Golang MangoDB","uri":"/golang-mongodb/"},{"categories":["文档"],"content":"更新文档 updateone() 方法允许你更新当个文档，需要一个筛选器文档来批评数据库中的文档， 并需要一个更新文档来描述更新操作， 使用bson.D 类型来构建筛选文档和更新文档 filter := bson.D{{\"name\", \"小兰\"}} update := bson.D{ {\"$inc\", bson.D{ {\"age\", 1}, }}, } updateResult, err := collection.UpdateOne(context.TODO(), filter, update) if err != nil { log.Fatal(err) } fmt.Printf(\"Matched %v documents and updated %v documents.\\n\", updateResult.MatchedCount, updateResult.ModifiedCount) ","date":"2023-05-23","objectID":"/golang-mongodb/:6:1","tags":["golang","mongoDB"],"title":"Golang MangoDB","uri":"/golang-mongodb/"},{"categories":["文档"],"content":"查找文档 找到一个文档，需要一个filter文档， 以及一个指向可以将结构解码为其值的指针 要查找单个文档， 使用collection.FindOne()。这个方法返回一个可以解码为值的结果 // 创建一个Student变量用来接收查询的结果 var result Student err = collection.FindOne(context.TODO(), filter).Decode(\u0026result) if err != nil { log.Fatal(err) } fmt.Printf(\"Found a single document: %+v\\n\", result) 要查找多个文档，请使用collection.Find()。此方法返回一个游标。游标提供了一个文档流，你可以通过它一次迭代和解码一个文档。当游标用完之后，应该关闭游标。下面的示例将使用options包设置一个限制以便只返回两个文档。 // 查询多个 // 将选项传递给Find() findOptions := options.Find() findOptions.SetLimit(2) // 定义一个切片用来存储查询结果 var results []*Student // 把bson.D{{}}作为一个filter来匹配所有文档 cur, err := collection.Find(context.TODO(), bson.D{{}}, findOptions) if err != nil { log.Fatal(err) } // 查找多个文档返回一个光标 // 遍历游标允许我们一次解码一个文档 for cur.Next(context.TODO()) { // 创建一个值，将单个文档解码为该值 var elem Student err := cur.Decode(\u0026elem) if err != nil { log.Fatal(err) } results = append(results, \u0026elem) } if err := cur.Err(); err != nil { log.Fatal(err) } // 完成后关闭游标 cur.Close(context.TODO()) fmt.Printf(\"Found multiple documents (array of pointers): %#v\\n\", results) ","date":"2023-05-23","objectID":"/golang-mongodb/:7:0","tags":["golang","mongoDB"],"title":"Golang MangoDB","uri":"/golang-mongodb/"},{"categories":["文档"],"content":"删除文档 使用collection.DeleteOne()或collection.DeleteMany()删除文档。如果你传递bson.D{{}}作为过滤器参数，它将匹配数据集中的所有文档。还可以使用collection. drop()删除整个数据集。 // 删除名字是小黄的那个 deleteResult1, err := collection.DeleteOne(context.TODO(), bson.D{{\"name\",\"小黄\"}}) if err != nil { log.Fatal(err) } fmt.Printf(\"Deleted %v documents in the trainers collection\\n\", deleteResult1.DeletedCount) // 删除所有 deleteResult2, err := collection.DeleteMany(context.TODO(), bson.D{{}}) if err != nil { log.Fatal(err) } fmt.Printf(\"Deleted %v documents in the trainers collection\\n\", deleteResult2.DeletedCount) 参考链接 https://pkg.go.dev/go.mongodb.org/mongo-driver/mongo ","date":"2023-05-23","objectID":"/golang-mongodb/:8:0","tags":["golang","mongoDB"],"title":"Golang MangoDB","uri":"/golang-mongodb/"},{"categories":["文档"],"content":"Redis ","date":"2023-05-23","objectID":"/golang-redis/:0:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"Redis 介绍 Redis是一个开源的内存数据库，提供了多种不同的数据结构， 通过复制、持久化/客户端分片等特性， 方便的将Redis 拓展成一个包含数百GB的数据， 处理上百万次请求 Redis 支持的数据结构 string hashe list set sortedset bitmap stream Redis 应用场景 缓存系统，减轻mysql的压力 计数系统， 热门排行榜 利用LIST 实现队列功能 利用HyperLog 统计UV、PV等数据 使用geospatial index 进行地理位置相关的查询 ","date":"2023-05-23","objectID":"/golang-redis/:1:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"docker 安装Reids 启动redis docker run --name redis507 -p 6379:6379 -d redis:5.0.7 连接redis server docker run -it --network host --rm redis:5.0.7 redis-cli ","date":"2023-05-23","objectID":"/golang-redis/:2:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"go-redis库 安装 go get github.com/go-redis/redis/v8 连接 ","date":"2023-05-23","objectID":"/golang-redis/:3:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"普通模式连接 使用redis.NewClient 函数连接Redis服务器 rdb := redis.NewClient(\u0026redis.Options{ Addr: \"localhost:6379\", Password: \"\", // 密码 DB: 0, // 数据库 PoolSize: 20, // 连接池大小 }) 谁也redis.ParseURL 函数表示数据源的字符串解析Redis opt, err := redis.ParseURL(\"redis://\u003cuser\u003e:\u003cpass\u003e@localhost:6379/\u003cdb\u003e\") if err != nil { panic(err) } rdb := redis.NewClient(opt) TLS 模式 rdb := redis.NewClient(\u0026redis.Options{ TLSConfig: \u0026tls.Config{ MinVersion: tls.VersionTLS12, // Certificates: []tls.Certificate{cert}, // ServerName: \"your.domain.com\", }, }) Redis Sentinel模式 rdb := redis.NewFailoverClient(\u0026redis.FailoverOptions{ MasterName: \"master-name\", SentinelAddrs: []string{\":9126\", \":9127\", \":9128\"}, }) Redis Cluster模式 rdb := redis.NewClusterClient(\u0026redis.ClusterOptions{ Addrs: []string{\":7000\", \":7001\", \":7002\", \":7003\", \":7004\", \":7005\"}, // 若要根据延迟或随机路由命令，请启用以下命令之一 // RouteByLatency: true, // RouteRandomly: true, }) ","date":"2023-05-23","objectID":"/golang-redis/:3:1","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"基本使用 redis基础操作 // doCommand go-redis基本使用示例 func doCommand() { ctx, cancel := context.WithTimeout(context.Background(), 500*time.Millisecond) defer cancel() // 执行命令获取结果 val, err := rdb.Get(ctx, \"key\").Result() fmt.Println(val, err) // 先获取到命令对象 cmder := rdb.Get(ctx, \"key\") fmt.Println(cmder.Val()) // 获取值 fmt.Println(cmder.Err()) // 获取错误 // 直接执行命令获取错误 err = rdb.Set(ctx, \"key\", 10, time.Hour).Err() // 直接执行命令获取值 value := rdb.Get(ctx, \"key\").Val() fmt.Println(value) } 执行任意命令 Do方法可以形象任意命令 // doDemo rdb.Do 方法使用示例 func doDemo() { ctx, cancel := context.WithTimeout(context.Background(), 500*time.Millisecond) defer cancel() // 直接执行命令获取错误 err := rdb.Do(ctx, \"set\", \"key\", 10, \"EX\", 3600).Err() fmt.Println(err) // 执行命令获取结果 val, err := rdb.Do(ctx, \"get\", \"key\").Result() fmt.Println(val, err) } Redis.Nil Nil错误来表示Key不存在的错误 // getValueFromRedis redis.Nil判断 func getValueFromRedis(key, defaultValue string) (string, error) { ctx, cancel := context.WithTimeout(context.Background(), 500*time.Millisecond) defer cancel() val, err := rdb.Get(ctx, key).Result() if err != nil { // 如果返回的错误是key不存在 if errors.Is(err, redis.Nil) { return defaultValue, nil } // 出其他错了 return \"\", err } return val, nil } ","date":"2023-05-23","objectID":"/golang-redis/:4:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"zset示例 go-redis 操作zset // zsetDemo 操作zset示例 func zsetDemo() { // key zsetKey := \"language_rank\" // value languages := []*redis.Z{ {Score: 90.0, Member: \"Golang\"}, {Score: 98.0, Member: \"Java\"}, {Score: 95.0, Member: \"Python\"}, {Score: 97.0, Member: \"JavaScript\"}, {Score: 99.0, Member: \"C/C++\"}, } ctx, cancel := context.WithTimeout(context.Background(), 500*time.Millisecond) defer cancel() // ZADD err := rdb.ZAdd(ctx, zsetKey, languages...).Err() if err != nil { fmt.Printf(\"zadd failed, err:%v\\n\", err) return } fmt.Println(\"zadd success\") // 把Golang的分数加10 newScore, err := rdb.ZIncrBy(ctx, zsetKey, 10.0, \"Golang\").Result() if err != nil { fmt.Printf(\"zincrby failed, err:%v\\n\", err) return } fmt.Printf(\"Golang's score is %f now.\\n\", newScore) // 取分数最高的3个 ret := rdb.ZRevRangeWithScores(ctx, zsetKey, 0, 2).Val() for _, z := range ret { fmt.Println(z.Member, z.Score) } // 取95~100分的 op := \u0026redis.ZRangeBy{ Min: \"95\", Max: \"100\", } ret, err = rdb.ZRangeByScoreWithScores(ctx, zsetKey, op).Result() if err != nil { fmt.Printf(\"zrangebyscore failed, err:%v\\n\", err) return } for _, z := range ret { fmt.Println(z.Member, z.Score) } } ","date":"2023-05-23","objectID":"/golang-redis/:5:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"扫描遍历所有Key 使用KEYS prefix:* 获取前缀获取所有key vals, err := rdb.Keys(ctx, \"prefix*\").Result() // scanKeysDemo1 按前缀查找所有key示例 func scanKeysDemo1() { ctx, cancel := context.WithTimeout(context.Background(), 500*time.Millisecond) defer cancel() var cursor uint64 for { var keys []string var err error // 按前缀扫描key keys, cursor, err = rdb.Scan(ctx, cursor, \"prefix:*\", 0).Result() if err != nil { panic(err) } for _, key := range keys { fmt.Println(\"key\", key) } if cursor == 0 { // no more keys break } } } 简化 // scanKeysDemo2 按前缀扫描key示例 func scanKeysDemo2() { ctx, cancel := context.WithTimeout(context.Background(), 500*time.Millisecond) defer cancel() // 按前缀扫描key iter := rdb.Scan(ctx, 0, \"prefix:*\", 0).Iterator() for iter.Next(ctx) { fmt.Println(\"keys\", iter.Val()) } if err := iter.Err(); err != nil { panic(err) } } 支持将所有匹配指定的模式key删除的示例 // delKeysByMatch 按match格式扫描所有key并删除 func delKeysByMatch(match string, timeout time.Duration) { ctx, cancel := context.WithTimeout(context.Background(), timeout) defer cancel() iter := rdb.Scan(ctx, 0, match, 0).Iterator() for iter.Next(ctx) { err := rdb.Del(ctx, iter.Val()).Err() if err != nil { panic(err) } } if err := iter.Err(); err != nil { panic(err) } } 支持 set 、 hash、zset 数据类型， 遍历 iter := rdb.SScan(ctx, \"set-key\", 0, \"prefix:*\", 0).Iterator() iter := rdb.HScan(ctx, \"hash-key\", 0, \"prefix:*\", 0).Iterator() iter := rdb.ZScan(ctx, \"sorted-hash-key\", 0, \"prefix:*\", 0).Iterator( ","date":"2023-05-23","objectID":"/golang-redis/:6:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"Pipeline Redis Pipeline 允许通过使用单个 client-server-client 往返执行多个命令来提高性能。区别于一个接一个地执行100个命令，你可以将这些命令放入 pipeline 中，然后使用1次读写操作像执行单个命令一样执行它们。这样做的好处是节省了执行命令的网络往返时间（RTT）。 pipe := rdb.Pipeline() incr := pipe.Incr(ctx, \"pipeline_counter\") pipe.Expire(ctx, \"pipeline_counter\", time.Hour) cmds, err := pipe.Exec(ctx) if err != nil { panic(err) } // 在执行pipe.Exec之后才能获取到结果 fmt.Println(incr.Val()) 使用Pipelined 方法，它会在函数退出时调用 Exec var incr *redis.IntCmd cmds, err := rdb.Pipelined(ctx, func(pipe redis.Pipeliner) error { incr = pipe.Incr(ctx, \"pipelined_counter\") pipe.Expire(ctx, \"pipelined_counter\", time.Hour) return nil }) if err != nil { panic(err) } // 在pipeline执行后获取到结果 fmt.Println(incr.Val()) 遍历 pipeline 命令的返回值依次获取每个命令的结果。下方的示例代码中使用pipiline一次执行了100个 Get 命令，在pipeline 执行后遍历取出100个命令的执行结果。 cmds, err := rdb.Pipelined(ctx, func(pipe redis.Pipeliner) error { for i := 0; i \u003c 100; i++ { pipe.Get(ctx, fmt.Sprintf(\"key%d\", i)) } return nil }) if err != nil { panic(err) } for _, cmd := range cmds { fmt.Println(cmd.(*redis.StringCmd).Val()) } ","date":"2023-05-23","objectID":"/golang-redis/:7:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"事务 使用 TxPipeline 或 TxPipelined 方法将 pipeline 命令使用 MULTI 和EXEC包裹起来。 // TxPipeline demo pipe := rdb.TxPipeline() incr := pipe.Incr(ctx, \"tx_pipeline_counter\") pipe.Expire(ctx, \"tx_pipeline_counter\", time.Hour) _, err := pipe.Exec(ctx) fmt.Println(incr.Val(), err) // TxPipelined demo var incr2 *redis.IntCmd _, err = rdb.TxPipelined(ctx, func(pipe redis.Pipeliner) error { incr2 = pipe.Incr(ctx, \"tx_pipeline_counter\") pipe.Expire(ctx, \"tx_pipeline_counter\", time.Hour) return nil }) fmt.Println(incr2.Val(), err) ","date":"2023-05-23","objectID":"/golang-redis/:8:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"Watch 搭配 WATCH命令来执行事务操作。从使用WATCH命令监视某个 key 开始，直到执行EXEC命令的这段时间里，如果有其他用户抢先对被监视的 key 进行了替换、更新、删除等操作，那么当用户尝试执行EXEC的时候，事务将失败并返回一个错误，用户可以根据这个错误选择重试事务或者放弃事务。 Watch方法接收一个函数和一个或多个key作为参数。 Watch(fn func(*Tx) error, keys ...string) error Watch 方法搭配 TxPipelined 的使用示例 // watchDemo 在key值不变的情况下将其值+1 func watchDemo(ctx context.Context, key string) error { return rdb.Watch(ctx, func(tx *redis.Tx) error { n, err := tx.Get(ctx, key).Int() if err != nil \u0026\u0026 err != redis.Nil { return err } // 假设操作耗时5秒 // 5秒内我们通过其他的客户端修改key，当前事务就会失败 time.Sleep(5 * time.Second) _, err = tx.TxPipelined(ctx, func(pipe redis.Pipeliner) error { pipe.Set(ctx, key, n+1, time.Hour) return nil }) return err }, key) } ","date":"2023-05-23","objectID":"/golang-redis/:9:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"一个 go-redis 官方文档中使用 GET 、SET和WATCH命令实现一个 INCR 命令的完整示例。 const routineCount = 100 increment := func(key string) error { txf := func(tx *redis.Tx) error { // 获得当前值或零值 n, err := tx.Get(key).Int() if err != nil \u0026\u0026 err != redis.Nil { return err } // 实际操作（乐观锁定中的本地操作） n++ // 仅在监视的Key保持不变的情况下运行 _, err = tx.Pipelined(func(pipe redis.Pipeliner) error { // pipe 处理错误情况 pipe.Set(key, n, 0) return nil }) return err } for retries := routineCount; retries \u003e 0; retries-- { err := rdb.Watch(txf, key) if err != redis.TxFailedErr { return err } // 乐观锁丢失 } return errors.New(\"increment reached maximum number of retries\") } var wg sync.WaitGroup wg.Add(routineCount) for i := 0; i \u003c routineCount; i++ { go func() { defer wg.Done() if err := increment(\"counter3\"); err != nil { fmt.Println(\"increment error:\", err) } }() } wg.Wait() n, err := rdb.Get(\"counter3\").Int() fmt.Println(\"ended with\", n, err) 在这个示例中使用了 redis.TxFailedErr 来检查事务是否失败。 参考链接 https://pkg.go.dev/github.com/go-redis/redis https://www.liwenzhou.com/posts/Go/redis/#autoid-1-2-0 ","date":"2023-05-23","objectID":"/golang-redis/:10:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"sqlx 库的使用 ","date":"2023-05-23","objectID":"/golang-sqlx/:0:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"sqlx 的介绍 sqlx 是内置database/sql 软件包的基础上提供了一组拓展 兼容sql原生包，提供了更加优雅的查询，插入函数 ","date":"2023-05-23","objectID":"/golang-sqlx/:1:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"sqlx 安装 go get github.com/jmoiron/sqlx ","date":"2023-05-23","objectID":"/golang-sqlx/:2:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"连接数据库 var db *sqlx.DB func initDB() (err error) { dsn := \"user:password@tcp(127.0.0.1:3306)/sql_test?charset=utf8mb4\u0026parseTime=True\" // 也可以使用MustConnect连接不成功就panic db, err = sqlx.Connect(\"mysql\", dsn) if err != nil { fmt.Printf(\"connect DB failed, err:%v\\n\", err) return } // 设置与数据库最大的打开连接数 db.SetMaxOpenConns(20) // 设置空闲的最大连接数 db.SetMaxIdleConns(10) return } ","date":"2023-05-23","objectID":"/golang-sqlx/:3:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"查询 查询单行的数据示例 // 查询单条数据示例 func queryRowDemo() { sqlStr := \"select id, name, age from user where id=?\" var u user err := db.Get(\u0026u, sqlStr, 1) if err != nil { fmt.Printf(\"get failed, err:%v\\n\", err) return } fmt.Printf(\"id:%d name:%s age:%d\\n\", u.ID, u.Name, u.Age) } 查询多条数据 // 查询多条数据示例 func queryMultiRowDemo() { sqlStr := \"select id, name, age from user where id \u003e ?\" var users []user err := db.Select(\u0026users, sqlStr, 0) if err != nil { fmt.Printf(\"query failed, err:%v\\n\", err) return } fmt.Printf(\"users:%#v\\n\", users) } ","date":"2023-05-23","objectID":"/golang-sqlx/:4:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"插入、更新和删除 sqlx的exec方法与原生sql中的exec使用一致 插入数据 // 插入数据 func insertRowDemo() { sqlStr := \"insert into user(name, age) values (?,?)\" ret, err := db.Exec(sqlStr, \" 张三\", 18) if err != nil { fmt.Printf(\"insert failed, err:%v\\n\", err) return } theID, err := ret.LastInsertId() // 新插入数据的id if err != nil { fmt.Printf(\"get lastinsert ID failed, err:%v\\n\", err) return } fmt.Printf(\"insert success, the id is %d.\\n\", theID) } 更新数据 // 更新数据 func updateRowDemo() { sqlStr := \"update user set age=? where id = ?\" ret, err := db.Exec(sqlStr, 18, 1) if err != nil { fmt.Printf(\"update failed, err:%v\\n\", err) return } n, err := ret.RowsAffected() // 操作影响的行数 if err != nil { fmt.Printf(\"get RowsAffected failed, err:%v\\n\", err) return } fmt.Printf(\"update success, affected rows:%d\\n\", n) } 删除数据 // 删除数据 func deleteRowDemo() { sqlStr := \"delete from user where id = ?\" ret, err := db.Exec(sqlStr, 6) if err != nil { fmt.Printf(\"delete failed, err:%v\\n\", err) return } n, err := ret.RowsAffected() // 操作影响的行数 if err != nil { fmt.Printf(\"get RowsAffected failed, err:%v\\n\", err) return } fmt.Printf(\"delete success, affected rows:%d\\n\", n) } ","date":"2023-05-23","objectID":"/golang-sqlx/:5:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"NameExec DB.NameExec 方法用来绑定的SQL语句与结构体或map中的同名字段 func insertUserDemo()(err error){ sqlStr := \"INSERT INTO user (name,age) VALUES (:name,:age)\" _, err = db.NamedExec(sqlStr, map[string]interface{}{ \"name\": \"张三\", \"age\": 18, }) return } ","date":"2023-05-23","objectID":"/golang-sqlx/:6:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"NameQuery 使用此数据库进行命名查询。任何命名的占位符参数都被arg中的字段替换。 func namedQuery() { sqlStr := \"SELECT * FROM user WHERE name=:name\" // 使用map做命名查询 rows, err := db.NamedQuery(sqlStr, map[string]interface{}{\"name\": \"张三\"}) if err != nil { fmt.Printf(\"db.NamedQuery failed, err:%v\\n\", err) return } defer rows.Close() for rows.Next() { var u user err := rows.StructScan(\u0026u) if err != nil { fmt.Printf(\"scan failed, err:%v\\n\", err) continue } fmt.Printf(\"user:%#v\\n\", u) } u := user{ Name: \"张三\", } // 使用结构体命名查询，根据结构体字段的 db tag进行映射 rows, err = db.NamedQuery(sqlStr, u) if err != nil { fmt.Printf(\"db.NamedQuery failed, err:%v\\n\", err) return } defer rows.Close() for rows.Next() { var u user err := rows.StructScan(\u0026u) if err != nil { fmt.Printf(\"scan failed, err:%v\\n\", err) continue } fmt.Printf(\"user:%#v\\n\", u) } } ","date":"2023-05-23","objectID":"/golang-sqlx/:7:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"事务操作 对于事物操作， 使用sqlx中提供 db.Begin() 和 tx.Exec() 方法 func transactionDemo2()(err error) { tx, err := db.Beginx() // 开启事务 if err != nil { fmt.Printf(\"begin trans failed, err:%v\\n\", err) return err } defer func() { if p := recover(); p != nil { tx.Rollback() panic(p) // re-throw panic after Rollback } else if err != nil { fmt.Println(\"rollback\") tx.Rollback() // err is non-nil; don't change it } else { err = tx.Commit() // err is nil; if Commit returns error update err fmt.Println(\"commit\") } }() sqlStr1 := \"Update user set age=20 where id=?\" rs, err := tx.Exec(sqlStr1, 1) if err!= nil{ return err } n, err := rs.RowsAffected() if err != nil { return err } if n != 1 { return errors.New(\"exec sqlStr1 failed\") } sqlStr2 := \"Update user set age=50 where i=?\" rs, err = tx.Exec(sqlStr2, 5) if err!=nil{ return err } n, err = rs.RowsAffected() if err != nil { return err } if n != 1 { return errors.New(\"exec sqlStr1 failed\") } return err } ","date":"2023-05-23","objectID":"/golang-sqlx/:8:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"sqlx.In sqlx.In的批量插入示例 创建一个user表 CREATE TABLE `user` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `name` VARCHAR(20) DEFAULT '', `age` INT(11) DEFAULT '0', PRIMARY KEY(`id`) )ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4; ","date":"2023-05-23","objectID":"/golang-sqlx/:9:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"结构体 定义一个结构体 type User struct { Name string `db:\"name\"` Age int `db:\"age\"` } ","date":"2023-05-23","objectID":"/golang-sqlx/:9:1","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"bingvars (绑定变量) 查询占位符**？**在内部成为 bindvars（查询占位符） MySQL中使用? PostgreSQL使用枚举的$1、$2等bindvar语法 SQLite中?和$1的语法都支持 Oracle中使用:name的语法 ","date":"2023-05-23","objectID":"/golang-sqlx/:9:2","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"自己拼接语句 // BatchInsertUsers 自行构造批量插入的语句 func BatchInsertUsers(users []*User) error { // 存放 (?, ?) 的slice valueStrings := make([]string, 0, len(users)) // 存放values的slice valueArgs := make([]interface{}, 0, len(users) * 2) // 遍历users准备相关数据 for _, u := range users { // 此处占位符要与插入值的个数对应 valueStrings = append(valueStrings, \"(?, ?)\") valueArgs = append(valueArgs, u.Name) valueArgs = append(valueArgs, u.Age) } // 自行拼接要执行的具体语句 stmt := fmt.Sprintf(\"INSERT INTO user (name, age) VALUES %s\", strings.Join(valueStrings, \",\")) _, err := DB.Exec(stmt, valueArgs...) return err } ","date":"2023-05-23","objectID":"/golang-sqlx/:9:3","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"使用sqlx.In 实现批量插入 实现结构体 driver.Valuer 接口 func (u User) Value() (driver.Value, error) { return []interface{}{u.Name, u.Age}, nil } // BatchInsertUsers2 使用sqlx.In帮我们拼接语句和参数, 注意传入的参数是[]interface{} func BatchInsertUsers2(users []interface{}) error { query, args, _ := sqlx.In( \"INSERT INTO user (name, age) VALUES (?), (?), (?)\", users..., // 如果arg实现了 driver.Valuer, sqlx.In 会通过调用 Value()来展开它 ) fmt.Println(query) // 查看生成的querystring fmt.Println(args) // 查看生成的args _, err := DB.Exec(query, args...) return err } ","date":"2023-05-23","objectID":"/golang-sqlx/:9:4","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"使用NamedExec实现批量插入 // BatchInsertUsers3 使用NamedExec实现批量插入 func BatchInsertUsers3(users []*User) error { _, err := DB.NamedExec(\"INSERT INTO user (name, age) VALUES (:name, :age)\", users) return err } ","date":"2023-05-23","objectID":"/golang-sqlx/:9:5","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"三种方法的示例 func main() { err := initDB() if err != nil { panic(err) } defer DB.Close() u1 := User{Name: \"张三\", Age: 18} u2 := User{Name: \"李四\", Age: 19} u3 := User{Name: \"王五\", Age: 20} // 方法1 users := []*User{\u0026u1, \u0026u2, \u0026u3} err = BatchInsertUsers(users) if err != nil { fmt.Printf(\"BatchInsertUsers failed, err:%v\\n\", err) } // 方法2 users2 := []interface{}{u1, u2, u3} err = BatchInsertUsers2(users2) if err != nil { fmt.Printf(\"BatchInsertUsers2 failed, err:%v\\n\", err) } // 方法3 users3 := []*User{\u0026u1, \u0026u2, \u0026u3} err = BatchInsertUsers3(users3) if err != nil { fmt.Printf(\"BatchInsertUsers3 failed, err:%v\\n\", err) } } ","date":"2023-05-23","objectID":"/golang-sqlx/:10:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"sqlx.in 的查询示例 ","date":"2023-05-23","objectID":"/golang-sqlx/:11:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"in查询 查询id在给定id集合的数据 // QueryByIDs 根据给定ID查询 func QueryByIDs(ids []int)(users []User, err error){ // 动态填充id query, args, err := sqlx.In(\"SELECT name, age FROM user WHERE id IN (?)\", ids) if err != nil { return } // sqlx.In 返回带 `?` bindvar的查询语句, 我们使用Rebind()重新绑定它 query = DB.Rebind(query) err = DB.Select(\u0026users, query, args...) return } ","date":"2023-05-23","objectID":"/golang-sqlx/:11:1","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"in查询和FIND_IN_SET 函数 查询ID在给定ID集合的数据并维持给定id集合的顺序 // QueryAndOrderByIDs 按照指定id查询并维护顺序 func QueryAndOrderByIDs(ids []int)(users []User, err error){ // 动态填充id strIDs := make([]string, 0, len(ids)) for _, id := range ids { strIDs = append(strIDs, fmt.Sprintf(\"%d\", id)) } query, args, err := sqlx.In(\"SELECT name, age FROM user WHERE id IN (?) ORDER BY FIND_IN_SET(id, ?)\", ids, strings.Join(strIDs, \",\")) if err != nil { return } // sqlx.In 返回带 `?` bindvar的查询语句, 我们使用Rebind()重新绑定它 query = DB.Rebind(query) err = DB.Select(\u0026users, query, args...) return } 参考链接 https://www.liwenzhou.com/posts/Go/sqlx/ https://pkg.go.dev/github.com/jmoiron/sqlx ","date":"2023-05-23","objectID":"/golang-sqlx/:12:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"Go 操作 sql server docker 安装 sqlserver #拉取镜像 docker pull mcr.microsoft.com/azure-sql-edge #启动容器 docker run --name azuresqledge -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=yourStrong(!)Password' -d -p 1433:1433 mcr.microsoft.com/azure-sql-edge 连接SQL数据库，要加载目标数据库的驱动，驱动里面包含了与数据库交互的逻辑 sql.Open() 数据库驱动的名称 数据源名称 得到一个指向sql.DB这个struct的指针 sql.DB 时用来操作数据库的，它代码0个或多个底层连接的池，这些连接由sql包来维护，sql包自动的创建和释放这些连接 它对于多个goroutine并发的使用时安全的 package main import ( \"context\" \"database/sql\" \"fmt\" \"log\" _ \"github.com/denisenkom/go-mssqldb\" ) var db *sql.DB const ( server = \"localhost\" port = \"1433\" user = \"sa\" password = \"Password\" database = \"go-db\" ) func main() { connStr := fmt.Sprintf(\"server=%s;user id=%s;password=%s;port=%s;database=%s;\", server, user, password, port, database) db, err := sql.Open(\"sqlserver\", connStr) if err != nil { log.Fatalln(err.Error()) } ctx := context.Background() err = db.PingContext(ctx) if err != nil { log.Fatalln(err.Error()) } fmt.Println(\"Connected!\") } note Open() 函数并不会连接到数据库，甚至不会验证参数，它只是把后续连接到数据库锁必须的struct给设置好 而真正的连接时在被需要的时候才进行懒设置的 sql.DB 不需要进行关闭 它就是用来连接数据库的， 而不是实际的连接 这个抽象保护了数据库连的池， 对此进行维护 使用sql.DB 的时候，可以定义它的全局变量进行使用， 也可以将它传递给函数/方法里 ","date":"2023-05-21","objectID":"/golang-mysql-crud/:0:0","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"如何获取驱动 正常的做法是使用sql.Register() 函数， 数据库驱动的名称和一个实现了driver.Driver 接口的struct， 来注册数据的驱动 sql.Register(“sqlserver”, \u0026drv{}) 为什么例子中没有这句话 因为 sql 驱动， 在这个包别引入的时候进行自我注册 _ \"github.com/denisenkom/go-mssqldb\" ","date":"2023-05-21","objectID":"/golang-mysql-crud/:1:0","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"驱动自我注册 在go-mssqldb包被引入的时候， 它的init函数将会运行并进行自我注册 在引入go-mssqldb 包的时候， 把该包的名称设置为下划线_，这是因为我们不能直接使用数据库驱动 未来升级驱动， 无需改变代码 Go语言没有提供官方的数据库驱动， 所有的数据库驱动都是第三方驱动，都遵循sql.driver包里面定义的接口 ","date":"2023-05-21","objectID":"/golang-mysql-crud/:2:0","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"func (db *DB) PingContext(ctx context.Context) PingContext函数用来验证与数据库的连接是否仍然生效， ，如有必要则建立一个连接 这个韩式需要一个Context（上下文）类型的参数， 这种类型可以携带 截止时间。取消信号和其他请求范围的值，并且可以横跨API边界和进程 上例中， 创建context使用的context.Background()函数， 该函数返回一个非nil的空context，它不会被取消，它没有值，没有截止时间 通常在main函数， 初始化或测试中 作为传入请求的context ","date":"2023-05-21","objectID":"/golang-mysql-crud/:3:0","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"查询 sql.Db 类型用于查询的方法有 Query QueryRow QueryContext QueryRowContext ","date":"2023-05-21","objectID":"/golang-mysql-crud/:4:0","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"Rows 返回类型 type Rows struct { // contains filtered or unexported fields } Rows的方法 func (*Rows) Close error func (rs *Rows) ColumnTypes() ([]*ColumnType, error) func (rs *Rows) Columns() ([]string, error) func (rs *Rows) Err() error func (rs *Rows) Next() bool func (rs *Rows) NextResultSet() bool func (rs *Rows) Scan(dest …any) error ","date":"2023-05-21","objectID":"/golang-mysql-crud/:4:1","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"QueryRow 返回类型是 type Row struct { // contains filtered or unexported fields } Row的方法 func (r *Row) Err() error func (r *Row) Scan(dest …any) error ","date":"2023-05-21","objectID":"/golang-mysql-crud/:4:2","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"单条查询 func getone(id int)(a app,err error) { a = app{} err = db.QueryRow(\"SELECT Id, Name, Status, Level, Order From dbo.App WHERE Id=@Id\", sql.Named(\"Id\", id)). Scan(\u0026a.ID, \u0026a.name, \u0026a.status, \u0026a.level, \u0026a.order) return } ","date":"2023-05-21","objectID":"/golang-mysql-crud/:4:3","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"多条查询 func getMany(id int) (apps []app, err error) { rows, err := db.Query(\"SELECT Id, Name, Status, Level, Order From dbo.App WHERE Id\u003e@Id\", sql.Named(\"Id\", id)) for rows.Next() { a := app{} err = rows.Scan(\u0026a.ID, \u0026a.name, \u0026a.status, \u0026a.level, \u0026a.order) if err != nil { log.Fatalln(err.Error()) } apps = append(apps, a) } return } ","date":"2023-05-21","objectID":"/golang-mysql-crud/:4:4","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"更新 sql.DB 类型上用于更新的方法 Exec ExecContext func (a *app) update() (err error) { _, err = db.Exec(\"UPDATE dbo.App SET Name=@Name, Order=@Order WHERE Id=@Id\", sql.Named(\"Name\", a.name), sql.Named(\"Order\", a.order), sql.Named(\"Id\", a.ID)) if err != nil { log.Fatalln(err.Error()) } return } ","date":"2023-05-21","objectID":"/golang-mysql-crud/:5:0","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"删除 func (a *app) delete() (err error) { _, err = db.Exec(\"DELETE FROM dbo.App WHERE Id=@Id\", sql.Named(\"Id\", a.ID)) if err != nil { log.Fatalln(err.Error()) } return } ","date":"2023-05-21","objectID":"/golang-mysql-crud/:6:0","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"其他 Prepare prepareContext Transactions Begin BeginTx ","date":"2023-05-21","objectID":"/golang-mysql-crud/:7:0","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"Prepare // 预处理查询示例 func prepareQueryDemo() { sqlStr := \"select id, name, age from user where id \u003e ?\" stmt, err := db.Prepare(sqlStr) if err != nil { fmt.Printf(\"prepare failed, err:%v\\n\", err) return } defer stmt.Close() rows, err := stmt.Query(0) if err != nil { fmt.Printf(\"query failed, err:%v\\n\", err) return } defer rows.Close() // 循环读取结果集中的数据 for rows.Next() { var u user err := rows.Scan(\u0026u.id, \u0026u.name, \u0026u.age) if err != nil { fmt.Printf(\"scan failed, err:%v\\n\", err) return } fmt.Printf(\"id:%d name:%s age:%d\\n\", u.id, u.name, u.age) } } ","date":"2023-05-21","objectID":"/golang-mysql-crud/:7:1","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"Transactions // 事务操作示例 func transactionDemo() { tx, err := db.Begin() // 开启事务 if err != nil { if tx != nil { tx.Rollback() // 回滚 } fmt.Printf(\"begin trans failed, err:%v\\n\", err) return } sqlStr1 := \"Update user set age=30 where id=?\" ret1, err := tx.Exec(sqlStr1, 2) if err != nil { tx.Rollback() // 回滚 fmt.Printf(\"exec sql1 failed, err:%v\\n\", err) return } affRow1, err := ret1.RowsAffected() if err != nil { tx.Rollback() // 回滚 fmt.Printf(\"exec ret1.RowsAffected() failed, err:%v\\n\", err) return } sqlStr2 := \"Update user set age=40 where id=?\" ret2, err := tx.Exec(sqlStr2, 3) if err != nil { tx.Rollback() // 回滚 fmt.Printf(\"exec sql2 failed, err:%v\\n\", err) return } affRow2, err := ret2.RowsAffected() if err != nil { tx.Rollback() // 回滚 fmt.Printf(\"exec ret1.RowsAffected() failed, err:%v\\n\", err) return } fmt.Println(affRow1, affRow2) if affRow1 == 1 \u0026\u0026 affRow2 == 1 { fmt.Println(\"事务提交啦...\") tx.Commit() // 提交事务 } else { tx.Rollback() fmt.Println(\"事务回滚啦...\") } fmt.Println(\"exec trans success!\") } ","date":"2023-05-21","objectID":"/golang-mysql-crud/:7:2","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"docker常用命令整理 ","date":"2023-05-21","objectID":"/docker-command/:0:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker images docker image pull ：下载镜像 docker image ls：列出本地存储的镜像,参数 –digests查看镜像的SHA26签名 docker image inspect:展示镜像细节。包括镜像层数据和元数据。 docker image rm：删除镜像。 docker提供参数 –filter 来过滤docker image ls 命令返回镜像列表的内容 返回悬虚(dangling)镜像 lhf@lhf-virtual-machine:~$ docker image ls --filter dangling=true REPOSITORY TAG IMAGE ID CREATED SIZE \u003cnone\u003e \u003cnone\u003e 没有标签的镜像称为悬虚镜像，列表显示: 使用docker image prune 移除全部悬虚镜像。加-a参数，Docker会额外移除没有被使用的镜像。 ","date":"2023-05-21","objectID":"/docker-command/:1:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker支持的过滤方式 dangling： 可以指定true或false，返回悬虚镜像(true)和非悬虚镜像(false). before: 需要镜像名称和ID作为参数，返回在之前被创建的镜像 since: before类似，返回需要指定镜像之后创建的全部镜像 label: 根据备注（label）的名称或者值 reference: 过滤标签lastest镜像 lhf@lhf-virtual-machine:~$ docker image ls --filter=reference=\"*:latest\" REPOSITORY TAG IMAGE ID CREATED SIZE test latest e63fd667d16a 2 days ago 71.4MB alpine latest 965ea09ff2eb 4 days ago 5.55MB ubuntu latest cf0f3ca922e0 7 days ago 64.2MB centos latest 0f3e07c0138f 3 weeks ago 220MB 通过–format 参数来通过go模板对输出内容格式化 只返回docker主机上镜像的大小属性 lhf@lhf-virtual-machine:~$ docker image ls --format \"{{.Size}}\" 71.4MB 5.55MB 64.2MB 220MB 只返回显示仓库、标签和大小的信息 lhf@lhf-virtual-machine:~$ docker image ls --format \"{{.Repository}}:{{.Tag}}:{{.Size}}\" test:latest:71.4MB alpine:latest:5.55MB ubuntu:latest:64.2MB centos:latest:220MB ","date":"2023-05-21","objectID":"/docker-command/:1:1","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"通过CLI方式搜索Docker Hub lhf@lhf-virtual-machine:~$ docker search alpine NAME DESCRIPTION STARS OFFICIAL AUTOMATED alpine A minimal Docker image based on Alpine Linux… 5757 [OK] mhart/alpine-node Minimal Node.js built on Alpine Linux 444 anapsix/alpine-java Oracle Java 8 (and 7) with GLIBC 2.28 over A… 427 \u003csnip\u003e lhf@lhf-virtual-machine:~$ docker search alpine --filter \"is-official=true\" NAME DESCRIPTION STARS OFFICIAL AUTOMATED alpine A minimal Docker image based on Alpine Linux… 5757 [OK] 使用参数 –digests 在本地查看镜像摘要 lhf@lhf-virtual-machine:~$ docker image ls --digests alpine REPOSITORY TAG DIGEST IMAGE ID CREATED SIZE alpine latest sha256:c19173c5ada610a5989151111163d28a67368362762534d8a8121ce95cf2bd5a 965ea09ff2eb 4 days ago 5.55MB 删除docker主机的全部镜像 lhf@lhf-virtual-machine:~$ docker image rm $(docker image ls -q) -f ","date":"2023-05-21","objectID":"/docker-command/:1:2","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker container docker container run:启动rongq Ctrl-PQ:断开Shell与容器的连接 docker container ls:列出运行状态的容器 docker container exec:允许用户在运行状态的容器，启动一个新的进程。 docker container stop：停止运行中的容器。 docker container start:重启处于停止状态的容器。 docker container rm：删除停止状态的容器。 docker container inspect:显示容器的配置细节和运行时信息。 -it参数： 使当前重点连接到容器的shell终端上 $ docker container run -it ubuntu /bin/bash 快速清理容器 $ docker container rm $(docker container ls -aq) -f ","date":"2023-05-21","objectID":"/docker-command/:2:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker应用容器化 docker image build ：读取Dockerfile文件,将应用程序容器化 使用-t参数文件镜像打标签 使用-f参数指定任意路径下的Dockerfile Dockerfile中FROM指令，指定构建镜像的一个基础层 Dockerfile中RUN指令，在镜像中执行命令，创建新的镜像层 Dockerfile中COPY指令，将文件作为新的层添加到镜像中 Dockerfile中EXPOSR指令，记录应用所使用的的网络端口 Dockerfile中ENTRYPOINT指令，指定镜像已容器的方式启动后默认运行程序 查看镜像构建执行了那些指令 $ docker image history lhfdocker/web:latest 查看镜像的构建详情 $ docker image inspect lhfdocker/web:latest ","date":"2023-05-21","objectID":"/docker-command/:3:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"Docker Compose docker-compose up ：部署一个compose应用。默认读取docker-compose.yml文件，可以使有-f参数指定文件,-d 参数在后台启动 docker-compose stop：停止compose应用的相关容器。可以通过docker-compose restart重新启动 docker-compose rm：删除已停止的compose应用的容器，会删除容器和网络，不会删除卷和镜像。 docker-compose restart 重启compose应用 如果compose应用进行了变更,需要重启才能生效 docker-compose ps:列出compose应用的容器 输出内容包括：状态、容器的运行命令，已经网络端口 docker-compose down:停止并删除运行中compose 的应用。会删除容器和网络，不会删除卷和镜像 ","date":"2023-05-21","objectID":"/docker-command/:4:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker Swarm docker swarm init 创建一个新的swarm,执行这个命令的节点称为管理节点 docker swarm join-token 查询接入管理节点和工作节点到现有swarm时所使用的命令和Token 增加管理节点——docker swarm join-token manager 增加工作节点——docker swarm join-token work docker node ls 列出swarm所有节点及相关信息 docker service create 创建新服务 docker service ls 列出swarm中运行的服务 docker service ps 获取更多关于服务服务的信息 docker service inspect 获取服务的详细信息 docker service scale 用于对服务副本数量进行增减 docker service update 对运行中的服务进行属性变更 docker service logs 查看服务的日志 docker service rm 从swarm删除服务 ","date":"2023-05-21","objectID":"/docker-command/:5:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker network docker network ls ：列出运行在本地的docker主机的全部网络 docker network create :创建新的docker网络。默认采用的是bridge 加-d参数指定(网络类型) docker network inspect:提供docker网络的详细配置信息。 docker network prune:删除docker主机上全部未使用的网络。 docker network rm :删除docker主机上指定的网络 ","date":"2023-05-21","objectID":"/docker-command/:6:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker volume docker volume create ：创建新卷，默认使用的local驱动，加-d参数指定不同驱动 docker volume ls :查看docker主机的全部卷 docker volume inspect: 查看卷的详细信息 docker volume prune ：删除未被容器和服务使用的卷 docker volume rm:删除指定卷 ","date":"2023-05-21","objectID":"/docker-command/:7:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"Docker Stack docker stack deploy： 用于根据stack文件部署和更新stack服务 docker stack ls ：列出swarm集群中所有的stack docker stack ps: 列出某个已经部署的stack的相关信息。 docker stack rm：从swarm集群中移除stack ","date":"2023-05-21","objectID":"/docker-command/:8:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":" Lighthouse (figure) PV、PVC、StorageClass Kubernetes 处理容器持久化存储的核心原理 PV： 持久化存储数据卷 pv 一般有运维人员事先创建之后使用， 定义一个NFS类型的PV apiVersion: v1 kind: PersistentVolume metadata: name: nfs spec: storageClassName: manual capacity: storage: 1Gi accessModes: - ReadWriteMany nfs: server: 10.244.1.4 path: \"/\" PVC： pod所希望使用的持久化存储的属性 一般有开发人员创建， Volume存储的大小，可读写的权限 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs spec: accessModes: - ReadWriteMany storageClassName: manual resources: requests: storage: 1Gi ","date":"2023-05-20","objectID":"/sc-pv-pvc/:0:0","tags":["k8s"],"title":"Kubernets-Sc\u0026Pv\u0026Pvc","uri":"/sc-pv-pvc/"},{"categories":["文档"],"content":"PVC与PV绑定使用的条件 PV和PVC的spec字段，PV的存储大小， 就必须满足PVC的要求 PV 和 PVC 的 storageClassName 字段必须一样 YAML 文件里声明使用这个 PVC 了 apiVersion: v1 kind: Pod metadata: labels: role: web-frontend spec: containers: - name: web image: nginx ports: - name: web containerPort: 80 volumeMounts: - name: nfs mountPath: \"/usr/share/nginx/html\" volumes: - name: nfs persistentVolumeClaim: claimName: nfs ","date":"2023-05-20","objectID":"/sc-pv-pvc/:1:0","tags":["k8s"],"title":"Kubernets-Sc\u0026Pv\u0026Pvc","uri":"/sc-pv-pvc/"},{"categories":["文档"],"content":"PersistentVolumeController 专门处理持久化存储的控制器， 会不断地查看当前每一个 PVC，是不是已经处于 Bound（已绑定）状态。如果不是，那它就会遍历所有的、可用的 PV，并尝试将其与这个“单身”的 PVC 进行绑定 所谓容器的 Volume，其实就是将一个宿主机上的目录，跟一个容器里的目录绑定挂载在了一起 而所谓的“持久化 Volume”，指的就是这个宿主机上的目录，具备“持久性” 这个准备“持久化”宿主机目录的过程，我们可以形象地称为“两阶段处理 一个Pod调度到一个节点上之后， kubelet就要负责这个Pod创建的它的Volume目录，默认这个情况下，kubelet 为 Volume 创建的目录是如下所示的一个宿主机上的路径： /var/lib/kubelet/pods/\u003cPod 的 ID\u003e/volumes/kubernetes.io~\u003cVolume 类型 \u003e/\u003cVolume 名字 \u003e 这一步为虚拟机挂载远程磁盘的操作，对应的正是“两阶段处理”的第一阶段。在 Kubernetes 中，我们把这个阶段称为 Attach 将磁盘设备格式化并挂载到 Volume 宿主机目录的操作，对应的正是“两阶段处理”的第二个阶段，我们一般称为：Mount。 在这一步，kubelet 需要作为 client，将远端 NFS 服务器的目录（比如：“/”目录），挂载到 Volume 的宿主机目录上，即相当于执行如下所示的命令： $ mount -t nfs \u003cNFS 服务器地址 \u003e:/ /var/lib/kubelet/pods/\u003cPod 的 ID\u003e/volumes/kubernetes.io~\u003cVolume 类型 \u003e/\u003cVolume 名字 \u003e kubelet 只要把这个 Volume 目录通过 CRI 里的 Mounts 参数，传递给 Docker，然后就可以为 Pod 里的容器挂载这个“持久化”的 Volume 了。其实，这一步相当于执行了如下所示的命令 $ docker run -v /var/lib/kubelet/pods/\u003cPod 的 ID\u003e/volumes/kubernetes.io~\u003cVolume 类型 \u003e/\u003cVolume 名字 \u003e:/\u003c 容器内的目标目录 \u003e 我的镜像 ... PV 的“两阶段处理”流程，是靠独立于 kubelet 主控制循环（Kubelet Sync Loop）之外的两个控制循环来实现的 StorageClass k8s提供了一套自动创建PV的机制， Dynamic Provisioning storageClass对象的作用， 其实就是创建PV的模板 主要定义两部分 PV的属性， （存储类型， Volume的大小） 创建这种PV需要用到的存储插件， （nfs、Ceph） apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: block-service provisioner: kubernetes.io/gce-pd parameters: type: pd-ssd apiVersion: ceph.rook.io/v1beta1 kind: Pool metadata: name: replicapool namespace: rook-ceph spec: replicated: size: 3 --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: block-service provisioner: ceph.rook.io/block parameters: pool: replicapool #The value of \"clusterNamespace\" MUST be the same as the one in which your rook cluster exist clusterNamespace: rook-ceph Lighthouse (figure) PVC描述的， 是Pod想要使用的持久化存储的属性（存储的大小、读写权限） PV的描述， 一个具体的Volume的属性， （Volume的类型， 挂载目录。 远程存储服务地址） StorageClass 的作用， 充当PV的模板， 只有属于一个StorageClass的PV和PVC才可以绑定子啊一起 ","date":"2023-05-20","objectID":"/sc-pv-pvc/:2:0","tags":["k8s"],"title":"Kubernets-Sc\u0026Pv\u0026Pvc","uri":"/sc-pv-pvc/"},{"categories":["文档"],"content":"Operator 工作原理解读 operator的工作原理和编写方法 ","date":"2023-05-20","objectID":"/operator/:0:0","tags":["k8s"],"title":"Kubernets-Operator","uri":"/operator/"},{"categories":["文档"],"content":"第一步，将这个 Operator 的代码 Clone 到本地： git clone https://github.com/coreos/etcd-operator ","date":"2023-05-20","objectID":"/operator/:0:1","tags":["k8s"],"title":"Kubernets-Operator","uri":"/operator/"},{"categories":["文档"],"content":"第二步，将这个 Etcd Operator 部署在 Kubernetes 集群里 example/rbac/create_role.sh 这个脚本为 Etcd Operator 创建 RBAC 规则， 因为 Etcd Operator 需要访问Kubernetes的APIServer来创建对象 对Pod， service，PVC， Deployment， Secret等API对象， 有所有权限 对CRD对象， 有所有权限 对属于etcd.database.coreos.com 这个 API Group 的 CR（Custom Resource）对象，有所有权限。 Etcd Operator 本身 是一个Deployment apiVersion: extensions/v1beta1 kind: Deployment metadata: name: etcd-operator spec: replicas: 1 template: metadata: labels: name: etcd-operator spec: containers: - name: etcd-operator image: quay.io/coreos/etcd-operator:v0.9.2 command: - etcd-operator env: - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name 创建 Etcd Operator kubectl create -f example/deployment.yaml pod进入Running 状态， 有一个CRD被自动创建出来 $ kubectl get pods NAME READY STATUS RESTARTS AGE etcd-operator-649dbdb5cb-bzfzp 1/1 Running 0 20s $ kubectl get crd NAME CREATED AT etcdclusters.etcd.database.coreos.com 2018-09-18T11:42:55Z 通过 kubectl describe 命令看到它的细节，如下所示： $ kubectl describe crd etcdclusters.etcd.database.coreos.com ... Group: etcd.database.coreos.com Names: Kind: EtcdCluster List Kind: EtcdClusterList Plural: etcdclusters Short Names: etcd Singular: etcdcluster Scope: Namespaced Version: v1beta2 ... ","date":"2023-05-20","objectID":"/operator/:1:0","tags":["k8s"],"title":"Kubernets-Operator","uri":"/operator/"},{"categories":["文档"],"content":"编写EtcdClutser的Yaml， $ kubectl apply -f example/example-etcd-cluster.yaml 这个 example-etcd-cluster.yaml 文件里描述的，是一个 3 个节点的 Etcd 集群 kubectl get pods NAME READY STATUS RESTARTS AGE example-etcd-cluster-dp8nqtjznc 1/1 Running 0 1m example-etcd-cluster-mbzlg6sd56 1/1 Running 0 2m example-etcd-cluster-v6v6s6stxd 1/1 Running 0 2m apiVersion: \"etcd.database.coreos.com/v1beta2\" kind: \"EtcdCluster\" metadata: name: \"example-etcd-cluster\" spec: size: 3 version: \"3.2.13\" ","date":"2023-05-20","objectID":"/operator/:2:0","tags":["k8s"],"title":"Kubernets-Operator","uri":"/operator/"},{"categories":["文档"],"content":"Operator 的工作原理： 实际上利用了Kubernetes的自定义API资源（CRD），来描述我们想要不熟的“有状态应用”，然后再自定义控制器里， 根据自定义API对象的变化， 来完成具体的部署和运维工作 tcd Operator 在业务逻辑的实现方式上 第一个工作只在该CLuster对象第一次被创建的时候会执行， 这个工作， 就是我们前面提到Bootstrap，即：创建一个单节点的种子集群。 Bootstrap，即：创建一个单节点的种子集群。 ","date":"2023-05-20","objectID":"/operator/:3:0","tags":["k8s"],"title":"Kubernets-Operator","uri":"/operator/"},{"categories":["文档"],"content":"基于角色的权限控制：RBAC 在kubernetes项目中 负责完成授权的（Authorization）工作的记住， 就是RBAC， 基于角色的访问控制（Role-based Access Control） 三个基本概念 Role： 角色，它其实是一组规则， 定义了一组对Kubernetes API对象的操作权限 Subject： 被作用者，即可以是“人”， 也可以是“机器” RoleBinding： 定义了“被作用者”和“角色”的绑定关系 ","date":"2023-05-20","objectID":"/rbac/:0:0","tags":["k8s"],"title":"Kubernets-Rbac","uri":"/rbac/"},{"categories":["文档"],"content":"Role kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: mynamespace name: example-role rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] ","date":"2023-05-20","objectID":"/rbac/:0:1","tags":["k8s"],"title":"Kubernets-Rbac","uri":"/rbac/"},{"categories":["文档"],"content":"Subject是如何指定的 通过RoleBinding来实现的 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-rolebinding namespace: mynamespace subjects: - kind: User name: example-user apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: example-role apiGroup: rbac.authorization.k8s.io ","date":"2023-05-20","objectID":"/rbac/:1:0","tags":["k8s"],"title":"Kubernets-Rbac","uri":"/rbac/"},{"categories":["文档"],"content":"User是从哪里来的？ kubernetes 的User ， 只是一个授权系统的系统里的逻辑概念。 通过外部认证服务，比如 keystone 直接给APIServer指定一个用户名、密码文件 RoleBinding对象就可以直接通过名字， 来引用前面定义的Role对象， 从而定义了“被作用者（Subject）”和“角色（Role）”之间的绑定关系 ","date":"2023-05-20","objectID":"/rbac/:2:0","tags":["k8s"],"title":"Kubernets-Rbac","uri":"/rbac/"},{"categories":["文档"],"content":"ClusterRole 和 ClusterRolebind kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-clusterrole rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] 赋予用户所有权限 verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"] 针对某一具体对象进行权限设置 rules: - apiGroups: [\"\"] resources: [\"configmaps\"] resourceNames: [\"my-config\"] verbs: [\"get\"] kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-clusterrolebinding subjects: - kind: User name: example-user apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: example-clusterrole apiGroup: rbac.authorization.k8s.io 这个由 Kubernetes 负责管理的“内置用户”，正是我们前面曾经提到过的：ServiceAccount。 ","date":"2023-05-20","objectID":"/rbac/:3:0","tags":["k8s"],"title":"Kubernets-Rbac","uri":"/rbac/"},{"categories":["文档"],"content":"定义一个ServiceAccount apiVersion: v1 kind: ServiceAccount metadata: namespace: mynamespace name: example-sa 编写RoleBinding的Yaml 文件， 为这个ServiceAccount分配权限 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-rolebinding namespace: mynamespace subjects: - kind: ServiceAccount name: example-sa namespace: mynamespace roleRef: kind: Role name: example-role apiGroup: rbac.authorization.k8s.io 创建这个对象 $ kubectl create -f svc-account.yaml $ kubectl create -f role-binding.yaml $ kubectl create -f role.yaml 查看详情 $ kubectl get sa -n mynamespace -o yaml - apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: 2018-09-08T12:59:17Z name: example-sa namespace: mynamespace resourceVersion: \"409327\" ... secrets: - name: example-sa-token-vmfg6 ","date":"2023-05-20","objectID":"/rbac/:4:0","tags":["k8s"],"title":"Kubernets-Rbac","uri":"/rbac/"},{"categories":["文档"],"content":"声明式API与Kubernetes编码范式 kubectl create 再replace的操作， 称为命令式配置文件操作 声明式API https://hugbz2.51cg3.co/archives/25451 /https://hugbz2.51cg3.co/archives/4081/ kuberctl apply 命令就是 声明式API ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:0:0","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":["文档"],"content":"apply 与 replace命令的本质区别 replace： 是使用新的YAML文件中的API对象，替换原来的API对象 apply： 执行了一个对原有的API对象的PATCH操作 对于kube-apiserver在响应命令式请求的时候 replace： 只能处理一个写请求， 否则会有产生冲突的可能 apply： 一次能处理多个写操作， 并且具备Merge能力 Istio Istio最根本的组件， 是运行在每一个应用Pod里的Envoy容器 Istio项目， 代理服务以sidecar容器的方式， 运行在每一个被治理的应用Pod中， Envoy容器就能够通过配置Pod里的iptables规则， 把整个Pod的进出流量接管下来 Istio 的控制层里的Pilot组件，就能够调用每个Envoy容器的API， 对这个Envoy代理进行配置， 从而实现微服务治理。 ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:0:1","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":["文档"],"content":"Istio项目使用的， 是kubernetes中一个非常重要的功能Dynamic Adminssion Control Kubernetes 项目为我们额外提供了一种“热插拔”式的 Admission 机制，它就是 Dynamic Admission Control，也叫作：Initializer。 现在，我给你举个例子。比如，我有如下所示的一个应用 Pod： apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! \u0026\u0026 sleep 3600'] Istio项目要做的， 就是在这个Pod Yaml被提交给kubernetes之后， 它对应的API对象字段加上Envoy容器的配置， apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! \u0026\u0026 sleep 3600'] - name: envoy image: lyft/envoy:845747b88f102c0fd262ab234308e9e22f693a1 command: [\"/usr/local/bin/envoy\"] ... Istio 要做的，就是编写一个用来为 Pod“自动注入”Envoy 容器的 Initializer。 首先， Istio会将这个Envoy容器本身的定义， 以ConfigMap方式保存在Kubernetes当前 apiVersion: v1 kind: ConfigMap metadata: name: envoy-initializer data: config: | containers: - name: envoy image: lyft/envoy:845747db88f102c0fd262ab234308e9e22f693a1 command: [\"/usr/local/bin/envoy\"] args: - \"--concurrency 4\" - \"--config-path /etc/envoy/envoy.json\" - \"--mode serve\" ports: - containerPort: 80 protocol: TCP resources: limits: cpu: \"1000m\" memory: \"512Mi\" requests: cpu: \"100m\" memory: \"64Mi\" volumeMounts: - name: envoy-conf mountPath: /etc/envoy volumes: - name: envoy-conf configMap: name: envoy Initializer 更新用户的P大对象的时候， 必须使用PATCH API 来完成， 而这种PATCH API 正式声明式API的主要能力 Istio 将一个编写好的Initializer， 作为一个Pod部署在kubernetes中 apiVersion: v1 kind: Pod metadata: labels: app: envoy-initializer name: envoy-initializer spec: containers: - name: envoy-initializer image: envoy-initializer:0.0.1 imagePullPolicy: Always 不断获取“实际状态”， 然后“期望状态”做对比 initializer的控制器， 不断获取的“实际状态”用户新创建的Pod “期望状态” 就是破的被添加的Envoy容器的定义 有了这个 TwoWayMergePatch 之后，Initializer 的代码就可以使用这个 patch 的数据，调用 Kubernetes 的 Client，发起一个 PATCH 请求 Istio项目的核心， 就是由无数个运行在应用Pod中的Envoy容器组成的服务代理网格。 ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:0:2","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":["文档"],"content":"kubernetes “声明式API”的独到之处 首先， 所谓“声明式”， 我们定义个定义好的API对象来“声明”， 所期望的状态是什么样子的 其次， “声明式API”允许有多个API写端， 以PATCH的方式对API对象进行修改，而无需关心本地原始的YAML文件的内容 最后， Kubernetes项目才可以基于对API对象的增、删、该、查， 在完全无需外界干扰的情况下， 完成对“实际状态”和“期望状态”的协调过程 ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:1:0","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":["文档"],"content":"Kubernetes 编程范式 如何使用控制器模式，同Kubernetes里API对象“增、删、改、查”进行协作，完成用户业务逻辑的编写过程 ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:2:0","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":["文档"],"content":"为 Network 这个自定义 API 对象编写一个自定义控制器（Custom Controller） 你好， 监控看到15、16、17这三台机器资源使用率和负载都太不高。 目前现在我们这边资源需求较多，计划3月8号（下周三）回收这3台GPU机器。请知悉。 ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:3:0","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":["文档"],"content":"自定义控制器的原理 控制器的工作流程 从kubernetes的APISerer里获取它所关心的对象， 就是自定义的Network informer与API 对象时——对应的， 所以我传递给自定义控制器的， 是Network对象informer Network Informer跟APIServer建立连接， 是informer所使用的Reflector包 Reflector 使用的ListAndWatch的方法，来“获取”并“监听”这些Network对象实例的变化 在ListAndWatch机制下， 一旦APIServer端有新的Network实例被创建、删除或者更新Reflector都会收到“事件通知”， 会放进一个Delta FIFO Queue（增量先进先出队列）中 informe会不断从这个Delta FIFO Queue 读取增量， 每拿到一个增量， informer 判断这个增量的事件类型。 然后创建或者更新本地对象的缓存。（这个缓存在kubernetes里叫Store） informer职责 同步本地缓存的工作 根据这些事件的类型，触发事先注册号的ResourceEventHandler informer 是一个钓友本地缓存和索引机制的 可以注册的EventHandler的client ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:4:0","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":["文档"],"content":"Job 与 CronJob ","date":"2023-05-20","objectID":"/job_cronjob/:0:0","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"Job API apiVersion: batch/v1 kind: Job metadata: name: pi spec: template: spec: containers: - name: pi image: resouer/ubuntu-bc command: [\"sh\", \"-c\", \"echo 'scale=10000; 4*a(1)' | bc -l \"] restartPolicy: Never backoffLimit: 4 计算π值的容器。而通过 scale=10000，我指定了输出的小数点后的位数是 10000 ","date":"2023-05-20","objectID":"/job_cronjob/:0:1","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"创建job $ kubectl create -f job.yaml 查看job 对象 $ kubectl describe jobs/pi Name: pi Namespace: default Selector: controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495 Labels: controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495 job-name=pi Annotations: \u003cnone\u003e Parallelism: 1 Completions: 1 .. Pods Statuses: 0 Running / 1 Succeeded / 0 Failed Pod Template: Labels: controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495 job-name=pi Containers: ... Volumes: \u003cnone\u003e Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 1m 1m 1 {job-controller } Normal SuccessfulCreate Created pod: pi-rq5rl pod模板， 会自动加上一个controller-uid= 随机字符串 job对象本身， 也会自动加上label的对应的Selector， 保证job与他所管理的Pod之间的匹配关系 这种自动生成的Label对用户来说并不友好， 所以不太适合推广到Deployment等长作业编排对象上。 事实上， restartPolicy在job对象里只允许被设置为Never 和 OnFailure 而在deployment对象里， restartPolicy则只允许被设置为Always ","date":"2023-05-20","objectID":"/job_cronjob/:0:2","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"查看pod日志 $ kubectl logs pi-rq5rl 3.141592653589793238462643383279... ","date":"2023-05-20","objectID":"/job_cronjob/:0:3","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"离线作业失败了怎么办？ job 定义了 restartPolicy=Never， 那么离线作业失败后 job Controller 就会不断尝试创建一个新的pod 在job对象的spec.backoffLimit字段字段里定义了重试次数为 4（即，backoffLimit=4）， 默认为6 重新创建Pod的间隔是呈指数增加的， 重新创建Pod的发生在 10 s、20 s、40 s 定义 restartPolicy=OnFailure, 那么离线作业失败后， JOb Controller就不会尝试创建新的Pod， 但是会不断尝试重启Pod的容器 在 spec.activeDeadlineSeconds 字段可以设置最长运行时间 spec: backoffLimit: 5 activeDeadlineSeconds: 100 一旦运行了100s, 这个Job的所有Pod都会被终止 你可以在 Pod 的状态里看到终止的原因是 reason: DeadlineExceeded。 ","date":"2023-05-20","objectID":"/job_cronjob/:0:4","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"Job Controller 对并行作业的控制方法 负责控制并行控制的两个参数 spec.parallelism: 定义一个Job在任意时间最多可以启动多少个Pod同事运行 spec.completions: 定义Job至少要完成的Pod数目， 即Job的最小完成数 创建一个参数例子 apiVersion: batch/v1 kind: Job metadata: name: pi spec: parallelism: 2 completions: 4 template: spec: containers: - name: pi image: resouer/ubuntu-bc command: [\"sh\", \"-c\", \"echo 'scale=5000; 4*a(1)' | bc -l \"] restartPolicy: Never backoffLimit: 4 创建job $ kubectl create -f job.yaml 查看job $ kubectl get job NAME DESIRED SUCCESSFUL AGE pi 4 0 3s DESIRED的值 正是completions定义的最小完成数 $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-gmcq5 0/1 Completed 0 40s pi-84ww8 0/1 Pending 0 0s pi-5mt88 0/1 Completed 0 41s pi-62rbt 0/1 Pending 0 0s $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-gmcq5 0/1 Completed 0 40s pi-84ww8 0/1 ContainerCreating 0 0s pi-5mt88 0/1 Completed 0 41s pi-62rbt 0/1 ContainerCreating 0 0s 由于所有Pod均成功退出, job执行完成, 看懂SuCCESSFUL为4 $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-5mt88 0/1 Completed 0 5m pi-62rbt 0/1 Completed 0 4m pi-84ww8 0/1 Completed 0 4m pi-gmcq5 0/1 Completed 0 5m $ kubectl get job NAME DESIRED SUCCESSFUL AGE pi 4 4 5m ","date":"2023-05-20","objectID":"/job_cronjob/:1:0","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"Job Controller的工作原理 首先 job Controller的控制对象直接就是 Pod 其次, job Controller在控制循环中进行的协调操作, 是根据实际的Running状态Pod额数目,已经成功退出的Pod的数目, 以及 parallelism、 conpletions 参数的值共同计算出在这个周期里，应该创建或者删除的Pod数目，然后调用Kubernetes API执行这个操作 ","date":"2023-05-20","objectID":"/job_cronjob/:1:1","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"第一种： 外部管理器+Job模板 apiVersion: batch/v1 kind: Job metadata: name: process-item-$ITEM labels: jobgroup: jobexample spec: template: metadata: name: jobexample labels: jobgroup: jobexample spec: containers: - name: c image: busybox command: [\"sh\", \"-c\", \"echo Processing item $ITEM \u0026\u0026 sleep 5\"] restartPolicy: Never 控制这种 Job 时，我们只要注意如下两个方面即可： 创建 Job 时，替换掉 $ITEM 这样的变量； 所有来自于同一个模板的 Job，都有一个 jobgroup: jobexample 标签，也就是说这一组 Job 使用这样一个相同的标识。 $ mkdir ./jobs $ for i in apple banana cherry do cat job-tmpl.yaml | sed \"s/\\$ITEM/$i/\" \u003e ./jobs/job-$i.yaml done $ kubectl create -f ./jobs $ kubectl get pods -l jobgroup=jobexample NAME READY STATUS RESTARTS AGE process-item-apple-kixwv 0/1 Completed 0 4m process-item-banana-wrsf7 0/1 Completed 0 4m process-item-cherry-dnfu9 0/1 Completed 0 4m ","date":"2023-05-20","objectID":"/job_cronjob/:1:2","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"第二种： 拥有固定任务数目的并行Job apiVersion: batch/v1 kind: Job metadata: name: job-wq-1 spec: completions: 8 parallelism: 2 template: metadata: name: job-wq-1 spec: containers: - name: c image: myrepo/job-wq-1 env: - name: BROKER_URL value: amqp://guest:guest@rabbitmq-service:5672 - name: QUEUE value: job1 restartPolicy: OnFailure ","date":"2023-05-20","objectID":"/job_cronjob/:1:3","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"第三种： 指定并行度（parallelism）， 但不设置固定的completions 这种情况 ， 任务的总数是未知的， 所以不仅需要一个工作队列来负责任务分发， 还需要判断工作列表已经为空。 apiVersion: batch/v1 kind: Job metadata: name: job-wq-2 spec: parallelism: 2 template: metadata: name: job-wq-2 spec: containers: - name: c image: gcr.io/myproject/job-wq-2 env: - name: BROKER_URL value: amqp://guest:guest@rabbitmq-service:5672 - name: QUEUE value: job2 restartPolicy: OnFailure ","date":"2023-05-20","objectID":"/job_cronjob/:1:4","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"CronJob（定时任务） CronJob是一个专门用来管理Job对象的控制器， 它的创建和删除job的依据， 是schedule字段的定义一个标准的Unix Cron格式的表达式。 apiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \"*/1 * * * *\" jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure 由于定时任务的特殊性， 某个job还没有执行完， 另一个新的job就产生了， 通过spec.concurrencyPolicy 字段来定义具体的处理策略 concurrencyPolicy=Allow， 默认情况， 意味着这些job可以同时存在 concurrencyPolicy=Forbid， 意味着不会创建新的pod， 该创建周期被跳过 concurrencyPolicy=Replace， 意味着新产生的job会替换旧的、没有执行完的job 如果某一次 Job 创建失败，这次创建就会被标记为“miss”。当在指定的时间窗口内，miss 的数目达到 100 时，那么 CronJob 会停止再创建这个 Job 这个时间窗口，可以由 spec.startingDeadlineSeconds 字段指定。比如 startingDeadlineSeconds=200，意味着在过去 200 s 里，如果 miss 的数目达到了 100 次，那么这个 Job 就不会被创建执行了。 ","date":"2023-05-20","objectID":"/job_cronjob/:2:0","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"StatefulSet ","date":"2023-05-20","objectID":"/stateful/:0:0","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":["文档"],"content":"概念 StatefulSet 的设计其实非常容器理解， 它把真实世界里的应用状态， 抽象为两个情况 拓扑状态：这种情况意味着， 应用的多个实例之间不是完全对等的关系， 这些应用实例， 不行按照某些顺序启动，比如应用的主节点A要先于节点B启动， 而如果你把A和B两个Pod删除调， 他们再次被创建出来也必须严格安装这个顺序才行， 并且， 新创建出来的Pod，必须和原理的Pod的网络标识一样， 这样原先的访问者才能使用同样的方法， 访问到这个新Pod。 存储状态：这种情况， 应用的多个实例分别绑定了不同的存储数据， 对于这些应用实例来说，Pod A第一次读取到的数据， 和隔了十分钟之后再次读取到的数据， 应该是同一份，哪怕再次期间Pod A被重新创建过， 这个情况 就是一个数据库应用的多个存储实例 StatefulSet的核心功能， 就是通过某种方式记录这些状态，然后再Pod被重新创建时， 能够为新的Pod恢复这些状态 ","date":"2023-05-20","objectID":"/stateful/:0:1","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":["文档"],"content":"Headless Service Service的VIP方式：当我访问 10.0.23.1 这个 Service 的 IP 地址时，10.0.23.1 其实就是一个 VIP，它会把请求转发到该 Service 所代理的某一个 Pod 上 **Service的DNS方式：**这时候，只要我访问“my-svc.my-namespace.svc.cluster.local”这条 DNS 记录，就可以访问到名叫 my-svc 的 Service 所代理的某一个 Pod。 Normal Service： 你访问“my-svc.my-namespace.svc.cluster.local”解析到的，正是 my-svc 这个 Service 的 VIP Headless Service： 你访问“my-svc.my-namespace.svc.cluster.local”解析到的，就是my-svc代理的某个Pod的IP地址， 区别在于Headless Service不需要分配一个VIP， 而是直接以DNS记录方式解析出被代理的Pod的IP地址 ","date":"2023-05-20","objectID":"/stateful/:0:2","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":["文档"],"content":"Headless Service 对应的 YAML 文件： apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx Headless Service \u003cpod-name\u003e.\u003csvc-name\u003e.\u003cnamespace\u003e.svc.cluster.local ","date":"2023-05-20","objectID":"/stateful/:0:3","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":["文档"],"content":"StatefulSet 又是如何使用这个DNS记录来维持Pod的拓扑状态的？ apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \"nginx\" replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.9.1 ports: - containerPort: 80 name: web serviceName=nginx 字段，告诉 StatefulSet控制器， 在执行控制循环（Control Loop）的时候， 请使用nginx 这个 Headless Service 来保证 Pod 的“可解析身份”。 $ kubectl run -i --tty --image busybox dns-test --restart=Never --rm /bin/sh $ nslookup web-0.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-0.nginx Address 1: 10.244.1.8 $ nslookup web-1.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-1.nginx Address 1: 10.244.2.8 Kubernetes 就是成功地将Pod的拓扑状态（比如：哪个节点先启动，哪个节点后启动），按照 Pod 的“名字 + 编号”的方式固定了下来 ","date":"2023-05-20","objectID":"/stateful/:0:4","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":["文档"],"content":"总结 StatefulSet 这个控制器主要作用之一： 就是使用Pod模板创建Pod的时候，对他们进行编号， 并且按照编号顺序逐一完成创建工作， 而当StatefulSet 的”控制循环”发现Pod的“实际状态”与“期望状态”不一致， 需要新建或者删除Pod进行 “调谐”的时候，它会严格按照这些 Pod 编号的顺序，逐一完成这些操作。 深入理解StatefulSet（二）：存储状态 ","date":"2023-05-20","objectID":"/stateful/:0:5","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":["文档"],"content":"工作原理 首信， StatefulSet的控制器直接管理的是Pod， ， 因为StatefulSet里的不同的Pod实例， 不想ReplicaSet中那样都是完全一样的， 而是有了细微区别的， 比如， 每个Pod的hostanme、名字等都是不同的， 携带了标红。 而statefulSet，区分这些实例的方式， 通过在Pod的名字里加上事先约定号的编号。 其次， Kubernetes通过headless ， 为这些有编号的Pod， 在DNS服务器中生成带有同样标红的DNS记录， 只有StatefulSet能够保证这些Pod的名字的编号不变， 那么Service里类似于 web-0.nginx.default.svc.cluster.local 这样的 DNS 记录也就不会变， 而这条记录解析出来的Pod的IP地址， 则会随着后端Pod的删除和再创建而自动更新， 这当然是Service机制本身的能力， 不需要StatefulSet操心 最后， StatefulSet 还为每个Pod分配并创建一个同样编号的PVC，， 这样 kubernetes 就可以通过Persistent Volume 机制为这个PVC绑定上毒药的PV， 从而保证了每个Pode都拥有一个独立的Volume ","date":"2023-05-20","objectID":"/stateful/:0:6","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":["文档"],"content":"总结 StatefulSet 其实就是一种特殊的 Deployment，而其独特之处在于，它的每个 Pod 都被编号了。而且，这个编号会体现在 Pod 的名字和 hostname 等标识信息上，这不仅代表了 Pod 的创建顺序，也是 Pod 的重要网络标识（即：在整个集群里唯一的、可被的访问身份）。 有了这个编号后，StatefulSet 就使用 Kubernetes 里的两个标准功能：Headless Service 和 PV/PVC，实现了对 Pod 的拓扑状态和存储状态的维护。 ","date":"2023-05-20","objectID":"/stateful/:0:7","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":["文档"],"content":"DaemonSet ","date":"2023-05-20","objectID":"/daemonset/:0:0","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":["文档"],"content":"主要作用 在kubernetes集群里， 运行一个Daemon Pod， 所以， 这个Pod有如下三个特征 这个Pod运载kubernetes集群里的每个节点（Node） 上 每个节点上只有一个这样的Pod实例 当有新的节点加入Kubernetes集群后， 该Pod 会自动的再新节点上被创建出来， 而当节点被删除后， 它上面的Pod也相应会被回收调。 ","date":"2023-05-20","objectID":"/daemonset/:0:1","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":["文档"],"content":"Daemon Pod 的例子 各种网络插件的Agent组件， 都必须运行在每个节点上， 用来处理这个节点上的容器网络 各种存储的插件的Agent组件， 也必须运行在每个节点上， 用来在这个节点上挂载远程存储目录，操作容器的Volume目录 各种监控组件和日志组件， 也必须运行在每个节点上，复制这个节点上监控信息和日志搜索 API apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd-elasticsearch image: k8s.gcr.io/fluentd-elasticsearch:1.20 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers ","date":"2023-05-20","objectID":"/daemonset/:0:2","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":["文档"],"content":"Daemonset 如何确保每个Node上有且只有一个被管理的Pod？ Daemonset Controller ， 首先从Etcd 里获取所有的Node列表，然后遍历所有的Node， 这时， 它就可以很容器的去检查， 当前这个Node上是不是携带了name=fluentd-elasticsearch 标签的 Pod 在运行。 检查结果有三种情况 没有每种Pod， 那么就意味着这个Node创建这样一个Pod 有这种Pod， 但是数量大于1， 那就说明 多余的Pod从这个Node删除掉 正好只有一个这种Pod， 说明这个节点是正常的 ","date":"2023-05-20","objectID":"/daemonset/:1:0","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":["文档"],"content":"nodeAffinity apiVersion: v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: metadata.name operator: In values: - node-geektime ","date":"2023-05-20","objectID":"/daemonset/:2:0","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":["文档"],"content":"含义 requiredDuringSchedulingIgnoredDuringExecution：这个nodeAffunuty 必须在每个调度的时候给予考虑， 同时 意味着可以设置在某个情况下不考虑这个nodeAffinity 这个Pod, 将来只允许允许在“metadata.name是“node-geektime”的节点上 Daemonset Controller 会在创建Pod的时候， 自动在这个Pod的API对象里， 加上这样一个nodeAffinity定义 ","date":"2023-05-20","objectID":"/daemonset/:2:1","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":["文档"],"content":"tolerations 这个字段： 意味着这个Pod， 会“容忍”（Toleration）某些Node的污点（Taint） apiVersion: v1 kind: Pod metadata: name: with-toleration spec: tolerations: - key: node.kubernetes.io/unschedulable operator: Exists effect: NoSchedule ","date":"2023-05-20","objectID":"/daemonset/:3:0","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":["文档"],"content":"含义 toleration： “容忍”所有被标记为unschedulable“污点”的Node， “容忍”的效果是允许调度 在 Kubernetes 项目中，当一个节点的网络插件尚未安装时，这个节点就会被自动加上名为node.kubernetes.io/network-unavailable的“污点”。 而通过这样一个 Toleration，调度器在调度这个 Pod 的时候，就会忽略当前节点上的“污点”，从而成功地将网络插件的 Agent 组件调度到这台机器上启动起来。 ","date":"2023-05-20","objectID":"/daemonset/:3:1","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":["文档"],"content":"Deployment ","date":"2023-05-20","objectID":"/deployment/:0:0","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":["文档"],"content":"Deployment Pod 的 “水平扩展、收缩” ","date":"2023-05-20","objectID":"/deployment/:0:1","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":["文档"],"content":"ReplicaSet apiVersion: apps/v1 kind: ReplicaSet metadata: name: nginx-set labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 一个ReplicaSet对象， 其实就是由副本数目的定义和一个Pod模板组合， 更重要的是 Deployment控制器实际操纵的， 正是ReplicasSet对象， 而不是Pod对象 ","date":"2023-05-20","objectID":"/deployment/:0:2","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":["文档"],"content":"Deployment apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 ","date":"2023-05-20","objectID":"/deployment/:0:3","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":["文档"],"content":"Deployment 、 ReplicaSet、 Pod的关系 ","date":"2023-05-20","objectID":"/deployment/:0:4","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":["文档"],"content":"水平扩展、收缩的操作 $ kubectl scale deployment nginx-deployment --replicas=4 deployment.apps/nginx-deployment scaled ","date":"2023-05-20","objectID":"/deployment/:0:5","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":["文档"],"content":"滚动更新 $ kubectl create -f nginx-deployment.yaml --record -record 参数： 记录每次操作的执行命令， 方便后面查看 检查一下 nginx-deployment 创建后的状态信息 $ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 0 0 0 1s 状态含义 DESIERD：用户期望的Pod 副本个数 CURRENT：当前处于Running状态的Pod的个数 UP-TO-DATE： 当前处于最新版的Pod的个数， （Pod的Spec部分与Deployment的Pod模板的定义的一致） AVALABLE： 当前已经可用的Pod的个数， 既是 Running 状态，又是最新版本，并且已经处于 Ready（健康检查正确）状态的 Pod 的个数。 查看Deployment对象的状态变化kubectl rollout status $ kubectl rollout status deployment/nginx-deployment Waiting for rollout to finish: 2 out of 3 new replicas have been updated... deployment.apps/nginx-deployment successfully rolled out ","date":"2023-05-20","objectID":"/deployment/:0:6","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":["文档"],"content":"Deployment对应进行版本控制的具体原理 这个镜像名字修改成为了一个错误的名字，比如：nginx:1.91。这样，这个 Deployment 就会出现一个升级失败的版本。 $ kubectl set image deployment/nginx-deployment nginx=nginx:1.91 deployment.extensions/nginx-deployment image updated 由于这个 nginx:1.91 镜像在 Docker Hub 中并不存在，所以这个 Deployment 的“滚动更新”被触发后，会立刻报错并停止。 这时，我们来检查一下 ReplicaSet 的状态，如下所示： $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-1764197365 2 2 2 24s nginx-deployment-3167673210 0 0 0 35s nginx-deployment-2156724341 2 2 0 7s 回滚到一起的旧版本 $ kubectl rollout undo deployment/nginx-deployment deployment.extensions/nginx-deployment 要使用 kubectl rollout history 命令，查看每次 Deployment 变更对应的版本 $ kubectl rollout history deployment/nginx-deployment deployments \"nginx-deployment\" REVISION CHANGE-CAUSE 1 kubectl create -f nginx-deployment.yaml --record 2 kubectl edit deployment/nginx-deployment 3 kubectl set image deployment/nginx-deployment nginx=nginx:1.91 查看每个版本对应的Deployment的API对象的细节 $ kubectl rollout history deployment/nginx-deployment --revision=2 可以在kubectl roolout undo 命令加上回滚的版本号， 指定版本回滚 $ kubectl rollout undo deployment/nginx-deployment --to-revision=2 deployment.extensions/nginx-deployment 对Deployment的多次更新操作， 最后只生成一个ReplicaSet， $ kubectl rollout pause deployment/nginx-deployment deployment.extensions/nginx-deployment paused 是Deployment进入一个暂定状态， 可以随意使用 kubectl edit 或者 kubectl set image 指令，修改这个 Deployment 的内容了。 操作完成之后将Deployment 恢复回来 $ kubectl rollout resume deploy/nginx-deployment deployment.extensions/nginx-deployment resumed ","date":"2023-05-20","objectID":"/deployment/:0:7","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":["文档"],"content":"如何控制“历史的” ReplicaSet数量 Deployment 对象有一个字段，叫作 spec.revisionHistoryLimit，就是 Kubernetes 为 Deployment 保留的“历史版本”个数。所以，如果把它设置为 0，你就再也不能做回滚操作了 ","date":"2023-05-20","objectID":"/deployment/:0:8","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":["文档"],"content":"pod ","date":"2023-05-17","objectID":"/pod/:0:0","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"pod 的属性 凡是调度、网络、存储、以及安全相关的属性， 都是pod级别的 ","date":"2023-05-17","objectID":"/pod/:0:1","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"NodeSelector 用户将Pod 与Node进行绑定的字段 apiVersion: v1 kind: Pod ... spec: nodeSelector: disktype: ssd ","date":"2023-05-17","objectID":"/pod/:0:2","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"NodeName 一旦pod 这个字段被赋值， Kubernetes项目就会被认为这个POd以及经过调度， 调度的结果就是复制的节点名字 ","date":"2023-05-17","objectID":"/pod/:0:3","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"HostAliases 定义了Pod的Hosts文件**（比如 /etc/hosts）里的内容**，用法如下： apiVersion: v1 kind: Pod ... spec: hostAliases: - ip: \"10.1.2.3\" hostnames: - \"foo.remote\" - \"bar.remote\" ... pod启动之后、 /etc/hosts 文件内容如下 cat /etc/hosts # Kubernetes-managed hosts file. 127.0.0.1 localhost ... 10.244.135.10 hostaliases-pod 10.1.2.3 foo.remote 10.1.2.3 bar.remote ","date":"2023-05-17","objectID":"/pod/:0:4","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"shareProcessNamespace=true： apiVersion: v1 kind: Pod metadata: name: nginx spec: shareProcessNamespace: true containers: - name: nginx image: nginx - name: shell image: busybox stdin: true tty: true pod 的容器共享 PID Namespace ","date":"2023-05-17","objectID":"/pod/:0:5","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"ImagePullPolicy 默认是Always ，每次都出重新拉取 Never： 意味着Pod永远不会主动拉取这个镜像， IfNotPresent： 只在宿主机不存在这个镜像时才拉取 ","date":"2023-05-17","objectID":"/pod/:0:6","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"lifecycle 在容器发生变化的时候触发一系列“钩子” apiVersion: v1 kind: Pod metadata: name: lifecycle-demo spec: containers: - name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: [\"/bin/sh\", \"-c\", \"echo Hello from the postStart handler \u003e /usr/share/message\"] preStop: exec: command: [\"/usr/sbin/nginx\",\"-s\",\"quit\"] ","date":"2023-05-17","objectID":"/pod/:0:7","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"pod对象在kubernetes中的生命周期 Pending：这个状态意味着， Pod的Yaml 文件以及提交给了kubernetes， API对象已经被创建并保存在Etcd 当中， 但是， 这个Pod 里有些容器因为某些原因不能被顺利创建， 比如， 调度不成功 Running： Pod已经调度成功， 跟一个具体的节点绑定，它包含的容器已经被创建，并且至少有一个已经运行成功 Succeeded： Pod的所有容器都运行成功， 并且已经退出， Failed： 这个状态下， Pod 里至少有一个容器以不正常的状态退出， 这个砖头的出现， 意味着你想办法Debug这个容器的应用， unknown： 异常状态， 意味着Pod的状态不能持续的被kubelet汇报给kube-apiserver， 很有可能是主从节点的通信出现了问题 ","date":"2023-05-17","objectID":"/pod/:0:8","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"projected Volume 四种 Secret ConfigMap Downward API ServiceAccountToken ","date":"2023-05-17","objectID":"/pod/:0:9","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"secret 把Pod想要访问的加密数据存在Etcd 中， 可以通过Pod的容器挂载Volume的方式， 访问这些Secret 保存的信息 ","date":"2023-05-17","objectID":"/pod/:0:10","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"configmap 保存的是不需要加密的， 应用所需的配置信息 ","date":"2023-05-17","objectID":"/pod/:0:11","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"Downward API 让Pod里的容器能够直接获取到这个Pod API对象本身的信息 apiVersion: v1 kind: Pod metadata: name: test-downwardapi-volume labels: zone: us-est-coast cluster: test-cluster1 rack: rack-22 spec: containers: - name: client-container image: k8s.gcr.io/busybox command: [\"sh\", \"-c\"] args: - while true; do if [[ -e /etc/podinfo/labels ]]; then echo -en '\\n\\n'; cat /etc/podinfo/labels; fi; sleep 5; done; volumeMounts: - name: podinfo mountPath: /etc/podinfo readOnly: false volumes: - name: podinfo projected: sources: - downwardAPI: items: - path: \"labels\" fieldRef: fieldPath: metadata.labels 支持的字段 spec.nodeName - 宿主机名字 status.hostIP - 宿主机 IP metadata.name - Pod 的名字 metadata.namespace - Pod 的 Namespace status.podIP - Pod 的 IP ","date":"2023-05-17","objectID":"/pod/:0:12","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"Service Account Service Account 对象的作用，就是 Kubernetes 系统内置的一种“服务账户”，它是 Kubernetes 进行权限分配的对象 Service Account 的授权信息和文件，实际上保存在它所绑定的一个特殊的 Secret 对象里的。这个特殊的 Secret 对象，就叫作ServiceAccountToken 这种把 Kubernetes 客户端以容器的方式运行在集群里，然后使用 default Service Account 自动授权的方式，被称作“InClusterConfig”，也是我最推荐的进行 Kubernetes API 编程的授权方式。 ","date":"2023-05-17","objectID":"/pod/:0:13","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"容器的健康检查和恢复机制 容器定义一个监控检查“探针”， kubelet就会根据这个Probe的返回值决定这个容器的状态，而不是直接以容器进行是否运行（来自 Docker 返回的信息）作为依据。这种机制，是生产环境中保证应用健康存活的重要手段。 apiVersion: v1 kind: Pod metadata: labels: test: liveness name: test-liveness-exec spec: containers: - name: liveness image: busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 ## 在容器启动后5s后开始执行 periodSeconds: 5 ## 每5s执行一次 Kubernetes 里的Pod 恢复机制，也叫 restartPolicy ","date":"2023-05-17","objectID":"/pod/:0:14","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"基本原理 只有Pod 的 restartPolicy 指定的策略允许重启的容器， 那么这个Pod就会保持Running状态，并进行容器重启 对于包含多个容器的Pod，只有它里面所有的容器都进入异常状态后，Pod 才会进入 Failed 状态 。在此之前，Pod 都是 Running 状态。此时，Pod 的 READY 字段会显示正常容器的个数 ","date":"2023-05-17","objectID":"/pod/:0:15","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"PodPreset PodPreset里定义的内容， 只会在Pod API对象被创建之前追加这个对象本身上， 而不会影响热河Pod的控制的定义 PodPreset 这样专门用来对 Pod 进行批量化、自动化修改的工具对象 ","date":"2023-05-17","objectID":"/pod/:0:16","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"}]