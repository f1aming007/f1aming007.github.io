[{"categories":["文档"],"content":"docker常用命令整理 ","date":"2023-05-21","objectID":"/docker-command/:0:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker images docker image pull ：下载镜像 docker image ls：列出本地存储的镜像,参数 –digests查看镜像的SHA26签名 docker image inspect:展示镜像细节。包括镜像层数据和元数据。 docker image rm：删除镜像。 docker提供参数 –filter 来过滤docker image ls 命令返回镜像列表的内容 返回悬虚(dangling)镜像 lhf@lhf-virtual-machine:~$ docker image ls --filter dangling=true REPOSITORY TAG IMAGE ID CREATED SIZE \u003cnone\u003e \u003cnone\u003e 没有标签的镜像称为悬虚镜像，列表显示: 使用docker image prune 移除全部悬虚镜像。加-a参数，Docker会额外移除没有被使用的镜像。 ","date":"2023-05-21","objectID":"/docker-command/:1:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker支持的过滤方式 dangling： 可以指定true或false，返回悬虚镜像(true)和非悬虚镜像(false). before: 需要镜像名称和ID作为参数，返回在之前被创建的镜像 since: before类似，返回需要指定镜像之后创建的全部镜像 label: 根据备注（label）的名称或者值 reference: 过滤标签lastest镜像 lhf@lhf-virtual-machine:~$ docker image ls --filter=reference=\"*:latest\" REPOSITORY TAG IMAGE ID CREATED SIZE test latest e63fd667d16a 2 days ago 71.4MB alpine latest 965ea09ff2eb 4 days ago 5.55MB ubuntu latest cf0f3ca922e0 7 days ago 64.2MB centos latest 0f3e07c0138f 3 weeks ago 220MB 通过–format 参数来通过go模板对输出内容格式化 只返回docker主机上镜像的大小属性 lhf@lhf-virtual-machine:~$ docker image ls --format \"{{.Size}}\" 71.4MB 5.55MB 64.2MB 220MB 只返回显示仓库、标签和大小的信息 lhf@lhf-virtual-machine:~$ docker image ls --format \"{{.Repository}}:{{.Tag}}:{{.Size}}\" test:latest:71.4MB alpine:latest:5.55MB ubuntu:latest:64.2MB centos:latest:220MB ","date":"2023-05-21","objectID":"/docker-command/:1:1","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"通过CLI方式搜索Docker Hub lhf@lhf-virtual-machine:~$ docker search alpine NAME DESCRIPTION STARS OFFICIAL AUTOMATED alpine A minimal Docker image based on Alpine Linux… 5757 [OK] mhart/alpine-node Minimal Node.js built on Alpine Linux 444 anapsix/alpine-java Oracle Java 8 (and 7) with GLIBC 2.28 over A… 427 \u003csnip\u003e lhf@lhf-virtual-machine:~$ docker search alpine --filter \"is-official=true\" NAME DESCRIPTION STARS OFFICIAL AUTOMATED alpine A minimal Docker image based on Alpine Linux… 5757 [OK] 使用参数 –digests 在本地查看镜像摘要 lhf@lhf-virtual-machine:~$ docker image ls --digests alpine REPOSITORY TAG DIGEST IMAGE ID CREATED SIZE alpine latest sha256:c19173c5ada610a5989151111163d28a67368362762534d8a8121ce95cf2bd5a 965ea09ff2eb 4 days ago 5.55MB 删除docker主机的全部镜像 lhf@lhf-virtual-machine:~$ docker image rm $(docker image ls -q) -f ","date":"2023-05-21","objectID":"/docker-command/:1:2","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker container docker container run:启动rongq Ctrl-PQ:断开Shell与容器的连接 docker container ls:列出运行状态的容器 docker container exec:允许用户在运行状态的容器，启动一个新的进程。 docker container stop：停止运行中的容器。 docker container start:重启处于停止状态的容器。 docker container rm：删除停止状态的容器。 docker container inspect:显示容器的配置细节和运行时信息。 -it参数： 使当前重点连接到容器的shell终端上 $ docker container run -it ubuntu /bin/bash 快速清理容器 $ docker container rm $(docker container ls -aq) -f ","date":"2023-05-21","objectID":"/docker-command/:2:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker应用容器化 docker image build ：读取Dockerfile文件,将应用程序容器化 使用-t参数文件镜像打标签 使用-f参数指定任意路径下的Dockerfile Dockerfile中FROM指令，指定构建镜像的一个基础层 Dockerfile中RUN指令，在镜像中执行命令，创建新的镜像层 Dockerfile中COPY指令，将文件作为新的层添加到镜像中 Dockerfile中EXPOSR指令，记录应用所使用的的网络端口 Dockerfile中ENTRYPOINT指令，指定镜像已容器的方式启动后默认运行程序 查看镜像构建执行了那些指令 $ docker image history lhfdocker/web:latest 查看镜像的构建详情 $ docker image inspect lhfdocker/web:latest ","date":"2023-05-21","objectID":"/docker-command/:3:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"Docker Compose docker-compose up ：部署一个compose应用。默认读取docker-compose.yml文件，可以使有-f参数指定文件,-d 参数在后台启动 docker-compose stop：停止compose应用的相关容器。可以通过docker-compose restart重新启动 docker-compose rm：删除已停止的compose应用的容器，会删除容器和网络，不会删除卷和镜像。 docker-compose restart 重启compose应用 如果compose应用进行了变更,需要重启才能生效 docker-compose ps:列出compose应用的容器 输出内容包括：状态、容器的运行命令，已经网络端口 docker-compose down:停止并删除运行中compose 的应用。会删除容器和网络，不会删除卷和镜像 ","date":"2023-05-21","objectID":"/docker-command/:4:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker Swarm docker swarm init 创建一个新的swarm,执行这个命令的节点称为管理节点 docker swarm join-token 查询接入管理节点和工作节点到现有swarm时所使用的命令和Token 增加管理节点——docker swarm join-token manager 增加工作节点——docker swarm join-token work docker node ls 列出swarm所有节点及相关信息 docker service create 创建新服务 docker service ls 列出swarm中运行的服务 docker service ps 获取更多关于服务服务的信息 docker service inspect 获取服务的详细信息 docker service scale 用于对服务副本数量进行增减 docker service update 对运行中的服务进行属性变更 docker service logs 查看服务的日志 docker service rm 从swarm删除服务 ","date":"2023-05-21","objectID":"/docker-command/:5:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker network docker network ls ：列出运行在本地的docker主机的全部网络 docker network create :创建新的docker网络。默认采用的是bridge 加-d参数指定(网络类型) docker network inspect:提供docker网络的详细配置信息。 docker network prune:删除docker主机上全部未使用的网络。 docker network rm :删除docker主机上指定的网络 ","date":"2023-05-21","objectID":"/docker-command/:6:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker volume docker volume create ：创建新卷，默认使用的local驱动，加-d参数指定不同驱动 docker volume ls :查看docker主机的全部卷 docker volume inspect: 查看卷的详细信息 docker volume prune ：删除未被容器和服务使用的卷 docker volume rm:删除指定卷 ","date":"2023-05-21","objectID":"/docker-command/:7:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"Docker Stack docker stack deploy： 用于根据stack文件部署和更新stack服务 docker stack ls ：列出swarm集群中所有的stack docker stack ps: 列出某个已经部署的stack的相关信息。 docker stack rm：从swarm集群中移除stack ","date":"2023-05-21","objectID":"/docker-command/:8:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":" Lighthouse (figure) PV、PVC、StorageClass Kubernetes 处理容器持久化存储的核心原理 PV： 持久化存储数据卷 pv 一般有运维人员事先创建之后使用， 定义一个NFS类型的PV apiVersion: v1 kind: PersistentVolume metadata: name: nfs spec: storageClassName: manual capacity: storage: 1Gi accessModes: - ReadWriteMany nfs: server: 10.244.1.4 path: \"/\" PVC： pod所希望使用的持久化存储的属性 一般有开发人员创建， Volume存储的大小，可读写的权限 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs spec: accessModes: - ReadWriteMany storageClassName: manual resources: requests: storage: 1Gi ","date":"2023-05-20","objectID":"/sc-pv-pvc/:0:0","tags":["k8s"],"title":"Kubernets-Sc\u0026Pv\u0026Pvc","uri":"/sc-pv-pvc/"},{"categories":["文档"],"content":"PVC与PV绑定使用的条件 PV和PVC的spec字段，PV的存储大小， 就必须满足PVC的要求 PV 和 PVC 的 storageClassName 字段必须一样 YAML 文件里声明使用这个 PVC 了 apiVersion: v1 kind: Pod metadata: labels: role: web-frontend spec: containers: - name: web image: nginx ports: - name: web containerPort: 80 volumeMounts: - name: nfs mountPath: \"/usr/share/nginx/html\" volumes: - name: nfs persistentVolumeClaim: claimName: nfs ","date":"2023-05-20","objectID":"/sc-pv-pvc/:1:0","tags":["k8s"],"title":"Kubernets-Sc\u0026Pv\u0026Pvc","uri":"/sc-pv-pvc/"},{"categories":["文档"],"content":"PersistentVolumeController 专门处理持久化存储的控制器， 会不断地查看当前每一个 PVC，是不是已经处于 Bound（已绑定）状态。如果不是，那它就会遍历所有的、可用的 PV，并尝试将其与这个“单身”的 PVC 进行绑定 所谓容器的 Volume，其实就是将一个宿主机上的目录，跟一个容器里的目录绑定挂载在了一起 而所谓的“持久化 Volume”，指的就是这个宿主机上的目录，具备“持久性” 这个准备“持久化”宿主机目录的过程，我们可以形象地称为“两阶段处理 一个Pod调度到一个节点上之后， kubelet就要负责这个Pod创建的它的Volume目录，默认这个情况下，kubelet 为 Volume 创建的目录是如下所示的一个宿主机上的路径： /var/lib/kubelet/pods/\u003cPod 的 ID\u003e/volumes/kubernetes.io~\u003cVolume 类型 \u003e/\u003cVolume 名字 \u003e 这一步为虚拟机挂载远程磁盘的操作，对应的正是“两阶段处理”的第一阶段。在 Kubernetes 中，我们把这个阶段称为 Attach 将磁盘设备格式化并挂载到 Volume 宿主机目录的操作，对应的正是“两阶段处理”的第二个阶段，我们一般称为：Mount。 在这一步，kubelet 需要作为 client，将远端 NFS 服务器的目录（比如：“/”目录），挂载到 Volume 的宿主机目录上，即相当于执行如下所示的命令： $ mount -t nfs \u003cNFS 服务器地址 \u003e:/ /var/lib/kubelet/pods/\u003cPod 的 ID\u003e/volumes/kubernetes.io~\u003cVolume 类型 \u003e/\u003cVolume 名字 \u003e kubelet 只要把这个 Volume 目录通过 CRI 里的 Mounts 参数，传递给 Docker，然后就可以为 Pod 里的容器挂载这个“持久化”的 Volume 了。其实，这一步相当于执行了如下所示的命令 $ docker run -v /var/lib/kubelet/pods/\u003cPod 的 ID\u003e/volumes/kubernetes.io~\u003cVolume 类型 \u003e/\u003cVolume 名字 \u003e:/\u003c 容器内的目标目录 \u003e 我的镜像 ... PV 的“两阶段处理”流程，是靠独立于 kubelet 主控制循环（Kubelet Sync Loop）之外的两个控制循环来实现的 StorageClass k8s提供了一套自动创建PV的机制， Dynamic Provisioning storageClass对象的作用， 其实就是创建PV的模板 主要定义两部分 PV的属性， （存储类型， Volume的大小） 创建这种PV需要用到的存储插件， （nfs、Ceph） apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: block-service provisioner: kubernetes.io/gce-pd parameters: type: pd-ssd apiVersion: ceph.rook.io/v1beta1 kind: Pool metadata: name: replicapool namespace: rook-ceph spec: replicated: size: 3 --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: block-service provisioner: ceph.rook.io/block parameters: pool: replicapool #The value of \"clusterNamespace\" MUST be the same as the one in which your rook cluster exist clusterNamespace: rook-ceph Lighthouse (figure) PVC描述的， 是Pod想要使用的持久化存储的属性（存储的大小、读写权限） PV的描述， 一个具体的Volume的属性， （Volume的类型， 挂载目录。 远程存储服务地址） StorageClass 的作用， 充当PV的模板， 只有属于一个StorageClass的PV和PVC才可以绑定子啊一起 ","date":"2023-05-20","objectID":"/sc-pv-pvc/:2:0","tags":["k8s"],"title":"Kubernets-Sc\u0026Pv\u0026Pvc","uri":"/sc-pv-pvc/"},{"categories":null,"content":"Operator 工作原理解读 operator的工作原理和编写方法 ","date":"2023-05-20","objectID":"/operator/:0:0","tags":["k8s"],"title":"Kubernets-Operator","uri":"/operator/"},{"categories":null,"content":"第一步，将这个 Operator 的代码 Clone 到本地： git clone https://github.com/coreos/etcd-operator ","date":"2023-05-20","objectID":"/operator/:0:1","tags":["k8s"],"title":"Kubernets-Operator","uri":"/operator/"},{"categories":null,"content":"第二步，将这个 Etcd Operator 部署在 Kubernetes 集群里 example/rbac/create_role.sh 这个脚本为 Etcd Operator 创建 RBAC 规则， 因为 Etcd Operator 需要访问Kubernetes的APIServer来创建对象 对Pod， service，PVC， Deployment， Secret等API对象， 有所有权限 对CRD对象， 有所有权限 对属于etcd.database.coreos.com 这个 API Group 的 CR（Custom Resource）对象，有所有权限。 Etcd Operator 本身 是一个Deployment apiVersion: extensions/v1beta1 kind: Deployment metadata: name: etcd-operator spec: replicas: 1 template: metadata: labels: name: etcd-operator spec: containers: - name: etcd-operator image: quay.io/coreos/etcd-operator:v0.9.2 command: - etcd-operator env: - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name 创建 Etcd Operator kubectl create -f example/deployment.yaml pod进入Running 状态， 有一个CRD被自动创建出来 $ kubectl get pods NAME READY STATUS RESTARTS AGE etcd-operator-649dbdb5cb-bzfzp 1/1 Running 0 20s $ kubectl get crd NAME CREATED AT etcdclusters.etcd.database.coreos.com 2018-09-18T11:42:55Z 通过 kubectl describe 命令看到它的细节，如下所示： $ kubectl describe crd etcdclusters.etcd.database.coreos.com ... Group: etcd.database.coreos.com Names: Kind: EtcdCluster List Kind: EtcdClusterList Plural: etcdclusters Short Names: etcd Singular: etcdcluster Scope: Namespaced Version: v1beta2 ... ","date":"2023-05-20","objectID":"/operator/:1:0","tags":["k8s"],"title":"Kubernets-Operator","uri":"/operator/"},{"categories":null,"content":"编写EtcdClutser的Yaml， $ kubectl apply -f example/example-etcd-cluster.yaml 这个 example-etcd-cluster.yaml 文件里描述的，是一个 3 个节点的 Etcd 集群 kubectl get pods NAME READY STATUS RESTARTS AGE example-etcd-cluster-dp8nqtjznc 1/1 Running 0 1m example-etcd-cluster-mbzlg6sd56 1/1 Running 0 2m example-etcd-cluster-v6v6s6stxd 1/1 Running 0 2m apiVersion: \"etcd.database.coreos.com/v1beta2\" kind: \"EtcdCluster\" metadata: name: \"example-etcd-cluster\" spec: size: 3 version: \"3.2.13\" ","date":"2023-05-20","objectID":"/operator/:2:0","tags":["k8s"],"title":"Kubernets-Operator","uri":"/operator/"},{"categories":null,"content":"Operator 的工作原理： 实际上利用了Kubernetes的自定义API资源（CRD），来描述我们想要不熟的“有状态应用”，然后再自定义控制器里， 根据自定义API对象的变化， 来完成具体的部署和运维工作 tcd Operator 在业务逻辑的实现方式上 第一个工作只在该CLuster对象第一次被创建的时候会执行， 这个工作， 就是我们前面提到Bootstrap，即：创建一个单节点的种子集群。 Bootstrap，即：创建一个单节点的种子集群。 ","date":"2023-05-20","objectID":"/operator/:3:0","tags":["k8s"],"title":"Kubernets-Operator","uri":"/operator/"},{"categories":null,"content":"基于角色的权限控制：RBAC 在kubernetes项目中 负责完成授权的（Authorization）工作的记住， 就是RBAC， 基于角色的访问控制（Role-based Access Control） 三个基本概念 Role： 角色，它其实是一组规则， 定义了一组对Kubernetes API对象的操作权限 Subject： 被作用者，即可以是“人”， 也可以是“机器” RoleBinding： 定义了“被作用者”和“角色”的绑定关系 ","date":"2023-05-20","objectID":"/rbac/:0:0","tags":["k8s"],"title":"Kubernets-Rbac","uri":"/rbac/"},{"categories":null,"content":"Role kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: mynamespace name: example-role rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] ","date":"2023-05-20","objectID":"/rbac/:0:1","tags":["k8s"],"title":"Kubernets-Rbac","uri":"/rbac/"},{"categories":null,"content":"Subject是如何指定的 通过RoleBinding来实现的 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-rolebinding namespace: mynamespace subjects: - kind: User name: example-user apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: example-role apiGroup: rbac.authorization.k8s.io ","date":"2023-05-20","objectID":"/rbac/:1:0","tags":["k8s"],"title":"Kubernets-Rbac","uri":"/rbac/"},{"categories":null,"content":"User是从哪里来的？ kubernetes 的User ， 只是一个授权系统的系统里的逻辑概念。 通过外部认证服务，比如 keystone 直接给APIServer指定一个用户名、密码文件 RoleBinding对象就可以直接通过名字， 来引用前面定义的Role对象， 从而定义了“被作用者（Subject）”和“角色（Role）”之间的绑定关系 ","date":"2023-05-20","objectID":"/rbac/:2:0","tags":["k8s"],"title":"Kubernets-Rbac","uri":"/rbac/"},{"categories":null,"content":"ClusterRole 和 ClusterRolebind kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-clusterrole rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] 赋予用户所有权限 verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"] 针对某一具体对象进行权限设置 rules: - apiGroups: [\"\"] resources: [\"configmaps\"] resourceNames: [\"my-config\"] verbs: [\"get\"] kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-clusterrolebinding subjects: - kind: User name: example-user apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: example-clusterrole apiGroup: rbac.authorization.k8s.io 这个由 Kubernetes 负责管理的“内置用户”，正是我们前面曾经提到过的：ServiceAccount。 ","date":"2023-05-20","objectID":"/rbac/:3:0","tags":["k8s"],"title":"Kubernets-Rbac","uri":"/rbac/"},{"categories":null,"content":"定义一个ServiceAccount apiVersion: v1 kind: ServiceAccount metadata: namespace: mynamespace name: example-sa 编写RoleBinding的Yaml 文件， 为这个ServiceAccount分配权限 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-rolebinding namespace: mynamespace subjects: - kind: ServiceAccount name: example-sa namespace: mynamespace roleRef: kind: Role name: example-role apiGroup: rbac.authorization.k8s.io 创建这个对象 $ kubectl create -f svc-account.yaml $ kubectl create -f role-binding.yaml $ kubectl create -f role.yaml 查看详情 $ kubectl get sa -n mynamespace -o yaml - apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: 2018-09-08T12:59:17Z name: example-sa namespace: mynamespace resourceVersion: \"409327\" ... secrets: - name: example-sa-token-vmfg6 ","date":"2023-05-20","objectID":"/rbac/:4:0","tags":["k8s"],"title":"Kubernets-Rbac","uri":"/rbac/"},{"categories":null,"content":"声明式API与Kubernetes编码范式 kubectl create 再replace的操作， 称为命令式配置文件操作 声明式API https://hugbz2.51cg3.co/archives/25451 /https://hugbz2.51cg3.co/archives/4081/ kuberctl apply 命令就是 声明式API ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:0:0","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":null,"content":"apply 与 replace命令的本质区别 replace： 是使用新的YAML文件中的API对象，替换原来的API对象 apply： 执行了一个对原有的API对象的PATCH操作 对于kube-apiserver在响应命令式请求的时候 replace： 只能处理一个写请求， 否则会有产生冲突的可能 apply： 一次能处理多个写操作， 并且具备Merge能力 Istio Istio最根本的组件， 是运行在每一个应用Pod里的Envoy容器 Istio项目， 代理服务以sidecar容器的方式， 运行在每一个被治理的应用Pod中， Envoy容器就能够通过配置Pod里的iptables规则， 把整个Pod的进出流量接管下来 Istio 的控制层里的Pilot组件，就能够调用每个Envoy容器的API， 对这个Envoy代理进行配置， 从而实现微服务治理。 ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:0:1","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":null,"content":"Istio项目使用的， 是kubernetes中一个非常重要的功能Dynamic Adminssion Control Kubernetes 项目为我们额外提供了一种“热插拔”式的 Admission 机制，它就是 Dynamic Admission Control，也叫作：Initializer。 现在，我给你举个例子。比如，我有如下所示的一个应用 Pod： apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! \u0026\u0026 sleep 3600'] Istio项目要做的， 就是在这个Pod Yaml被提交给kubernetes之后， 它对应的API对象字段加上Envoy容器的配置， apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! \u0026\u0026 sleep 3600'] - name: envoy image: lyft/envoy:845747b88f102c0fd262ab234308e9e22f693a1 command: [\"/usr/local/bin/envoy\"] ... Istio 要做的，就是编写一个用来为 Pod“自动注入”Envoy 容器的 Initializer。 首先， Istio会将这个Envoy容器本身的定义， 以ConfigMap方式保存在Kubernetes当前 apiVersion: v1 kind: ConfigMap metadata: name: envoy-initializer data: config: | containers: - name: envoy image: lyft/envoy:845747db88f102c0fd262ab234308e9e22f693a1 command: [\"/usr/local/bin/envoy\"] args: - \"--concurrency 4\" - \"--config-path /etc/envoy/envoy.json\" - \"--mode serve\" ports: - containerPort: 80 protocol: TCP resources: limits: cpu: \"1000m\" memory: \"512Mi\" requests: cpu: \"100m\" memory: \"64Mi\" volumeMounts: - name: envoy-conf mountPath: /etc/envoy volumes: - name: envoy-conf configMap: name: envoy Initializer 更新用户的P大对象的时候， 必须使用PATCH API 来完成， 而这种PATCH API 正式声明式API的主要能力 Istio 将一个编写好的Initializer， 作为一个Pod部署在kubernetes中 apiVersion: v1 kind: Pod metadata: labels: app: envoy-initializer name: envoy-initializer spec: containers: - name: envoy-initializer image: envoy-initializer:0.0.1 imagePullPolicy: Always 不断获取“实际状态”， 然后“期望状态”做对比 initializer的控制器， 不断获取的“实际状态”用户新创建的Pod “期望状态” 就是破的被添加的Envoy容器的定义 有了这个 TwoWayMergePatch 之后，Initializer 的代码就可以使用这个 patch 的数据，调用 Kubernetes 的 Client，发起一个 PATCH 请求 Istio项目的核心， 就是由无数个运行在应用Pod中的Envoy容器组成的服务代理网格。 ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:0:2","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":null,"content":"kubernetes “声明式API”的独到之处 首先， 所谓“声明式”， 我们定义个定义好的API对象来“声明”， 所期望的状态是什么样子的 其次， “声明式API”允许有多个API写端， 以PATCH的方式对API对象进行修改，而无需关心本地原始的YAML文件的内容 最后， Kubernetes项目才可以基于对API对象的增、删、该、查， 在完全无需外界干扰的情况下， 完成对“实际状态”和“期望状态”的协调过程 ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:1:0","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":null,"content":"Kubernetes 编程范式 如何使用控制器模式，同Kubernetes里API对象“增、删、改、查”进行协作，完成用户业务逻辑的编写过程 ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:2:0","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":null,"content":"为 Network 这个自定义 API 对象编写一个自定义控制器（Custom Controller） 你好， 监控看到15、16、17这三台机器资源使用率和负载都太不高。 目前现在我们这边资源需求较多，计划3月8号（下周三）回收这3台GPU机器。请知悉。 ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:3:0","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":null,"content":"自定义控制器的原理 控制器的工作流程 从kubernetes的APISerer里获取它所关心的对象， 就是自定义的Network informer与API 对象时——对应的， 所以我传递给自定义控制器的， 是Network对象informer Network Informer跟APIServer建立连接， 是informer所使用的Reflector包 Reflector 使用的ListAndWatch的方法，来“获取”并“监听”这些Network对象实例的变化 在ListAndWatch机制下， 一旦APIServer端有新的Network实例被创建、删除或者更新Reflector都会收到“事件通知”， 会放进一个Delta FIFO Queue（增量先进先出队列）中 informe会不断从这个Delta FIFO Queue 读取增量， 每拿到一个增量， informer 判断这个增量的事件类型。 然后创建或者更新本地对象的缓存。（这个缓存在kubernetes里叫Store） informer职责 同步本地缓存的工作 根据这些事件的类型，触发事先注册号的ResourceEventHandler informer 是一个钓友本地缓存和索引机制的 可以注册的EventHandler的client ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:4:0","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":null,"content":"Job 与 CronJob ","date":"2023-05-20","objectID":"/job_cronjob/:0:0","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":null,"content":"Job API apiVersion: batch/v1 kind: Job metadata: name: pi spec: template: spec: containers: - name: pi image: resouer/ubuntu-bc command: [\"sh\", \"-c\", \"echo 'scale=10000; 4*a(1)' | bc -l \"] restartPolicy: Never backoffLimit: 4 计算π值的容器。而通过 scale=10000，我指定了输出的小数点后的位数是 10000 ","date":"2023-05-20","objectID":"/job_cronjob/:0:1","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":null,"content":"创建job $ kubectl create -f job.yaml 查看job 对象 $ kubectl describe jobs/pi Name: pi Namespace: default Selector: controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495 Labels: controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495 job-name=pi Annotations: \u003cnone\u003e Parallelism: 1 Completions: 1 .. Pods Statuses: 0 Running / 1 Succeeded / 0 Failed Pod Template: Labels: controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495 job-name=pi Containers: ... Volumes: \u003cnone\u003e Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 1m 1m 1 {job-controller } Normal SuccessfulCreate Created pod: pi-rq5rl pod模板， 会自动加上一个controller-uid= 随机字符串 job对象本身， 也会自动加上label的对应的Selector， 保证job与他所管理的Pod之间的匹配关系 这种自动生成的Label对用户来说并不友好， 所以不太适合推广到Deployment等长作业编排对象上。 事实上， restartPolicy在job对象里只允许被设置为Never 和 OnFailure 而在deployment对象里， restartPolicy则只允许被设置为Always ","date":"2023-05-20","objectID":"/job_cronjob/:0:2","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":null,"content":"查看pod日志 $ kubectl logs pi-rq5rl 3.141592653589793238462643383279... ","date":"2023-05-20","objectID":"/job_cronjob/:0:3","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":null,"content":"离线作业失败了怎么办？ job 定义了 restartPolicy=Never， 那么离线作业失败后 job Controller 就会不断尝试创建一个新的pod 在job对象的spec.backoffLimit字段字段里定义了重试次数为 4（即，backoffLimit=4）， 默认为6 重新创建Pod的间隔是呈指数增加的， 重新创建Pod的发生在 10 s、20 s、40 s 定义 restartPolicy=OnFailure, 那么离线作业失败后， JOb Controller就不会尝试创建新的Pod， 但是会不断尝试重启Pod的容器 在 spec.activeDeadlineSeconds 字段可以设置最长运行时间 spec: backoffLimit: 5 activeDeadlineSeconds: 100 一旦运行了100s, 这个Job的所有Pod都会被终止 你可以在 Pod 的状态里看到终止的原因是 reason: DeadlineExceeded。 ","date":"2023-05-20","objectID":"/job_cronjob/:0:4","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":null,"content":"Job Controller 对并行作业的控制方法 负责控制并行控制的两个参数 spec.parallelism: 定义一个Job在任意时间最多可以启动多少个Pod同事运行 spec.completions: 定义Job至少要完成的Pod数目， 即Job的最小完成数 创建一个参数例子 apiVersion: batch/v1 kind: Job metadata: name: pi spec: parallelism: 2 completions: 4 template: spec: containers: - name: pi image: resouer/ubuntu-bc command: [\"sh\", \"-c\", \"echo 'scale=5000; 4*a(1)' | bc -l \"] restartPolicy: Never backoffLimit: 4 创建job $ kubectl create -f job.yaml 查看job $ kubectl get job NAME DESIRED SUCCESSFUL AGE pi 4 0 3s DESIRED的值 正是completions定义的最小完成数 $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-gmcq5 0/1 Completed 0 40s pi-84ww8 0/1 Pending 0 0s pi-5mt88 0/1 Completed 0 41s pi-62rbt 0/1 Pending 0 0s $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-gmcq5 0/1 Completed 0 40s pi-84ww8 0/1 ContainerCreating 0 0s pi-5mt88 0/1 Completed 0 41s pi-62rbt 0/1 ContainerCreating 0 0s 由于所有Pod均成功退出, job执行完成, 看懂SuCCESSFUL为4 $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-5mt88 0/1 Completed 0 5m pi-62rbt 0/1 Completed 0 4m pi-84ww8 0/1 Completed 0 4m pi-gmcq5 0/1 Completed 0 5m $ kubectl get job NAME DESIRED SUCCESSFUL AGE pi 4 4 5m ","date":"2023-05-20","objectID":"/job_cronjob/:1:0","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":null,"content":"Job Controller的工作原理 首先 job Controller的控制对象直接就是 Pod 其次, job Controller在控制循环中进行的协调操作, 是根据实际的Running状态Pod额数目,已经成功退出的Pod的数目, 以及 parallelism、 conpletions 参数的值共同计算出在这个周期里，应该创建或者删除的Pod数目，然后调用Kubernetes API执行这个操作 ","date":"2023-05-20","objectID":"/job_cronjob/:1:1","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":null,"content":"第一种： 外部管理器+Job模板 apiVersion: batch/v1 kind: Job metadata: name: process-item-$ITEM labels: jobgroup: jobexample spec: template: metadata: name: jobexample labels: jobgroup: jobexample spec: containers: - name: c image: busybox command: [\"sh\", \"-c\", \"echo Processing item $ITEM \u0026\u0026 sleep 5\"] restartPolicy: Never 控制这种 Job 时，我们只要注意如下两个方面即可： 创建 Job 时，替换掉 $ITEM 这样的变量； 所有来自于同一个模板的 Job，都有一个 jobgroup: jobexample 标签，也就是说这一组 Job 使用这样一个相同的标识。 $ mkdir ./jobs $ for i in apple banana cherry do cat job-tmpl.yaml | sed \"s/\\$ITEM/$i/\" \u003e ./jobs/job-$i.yaml done $ kubectl create -f ./jobs $ kubectl get pods -l jobgroup=jobexample NAME READY STATUS RESTARTS AGE process-item-apple-kixwv 0/1 Completed 0 4m process-item-banana-wrsf7 0/1 Completed 0 4m process-item-cherry-dnfu9 0/1 Completed 0 4m ","date":"2023-05-20","objectID":"/job_cronjob/:1:2","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":null,"content":"第二种： 拥有固定任务数目的并行Job apiVersion: batch/v1 kind: Job metadata: name: job-wq-1 spec: completions: 8 parallelism: 2 template: metadata: name: job-wq-1 spec: containers: - name: c image: myrepo/job-wq-1 env: - name: BROKER_URL value: amqp://guest:guest@rabbitmq-service:5672 - name: QUEUE value: job1 restartPolicy: OnFailure ","date":"2023-05-20","objectID":"/job_cronjob/:1:3","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":null,"content":"第三种： 指定并行度（parallelism）， 但不设置固定的completions 这种情况 ， 任务的总数是未知的， 所以不仅需要一个工作队列来负责任务分发， 还需要判断工作列表已经为空。 apiVersion: batch/v1 kind: Job metadata: name: job-wq-2 spec: parallelism: 2 template: metadata: name: job-wq-2 spec: containers: - name: c image: gcr.io/myproject/job-wq-2 env: - name: BROKER_URL value: amqp://guest:guest@rabbitmq-service:5672 - name: QUEUE value: job2 restartPolicy: OnFailure ","date":"2023-05-20","objectID":"/job_cronjob/:1:4","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":null,"content":"CronJob（定时任务） CronJob是一个专门用来管理Job对象的控制器， 它的创建和删除job的依据， 是schedule字段的定义一个标准的Unix Cron格式的表达式。 apiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \"*/1 * * * *\" jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure 由于定时任务的特殊性， 某个job还没有执行完， 另一个新的job就产生了， 通过spec.concurrencyPolicy 字段来定义具体的处理策略 concurrencyPolicy=Allow， 默认情况， 意味着这些job可以同时存在 concurrencyPolicy=Forbid， 意味着不会创建新的pod， 该创建周期被跳过 concurrencyPolicy=Replace， 意味着新产生的job会替换旧的、没有执行完的job 如果某一次 Job 创建失败，这次创建就会被标记为“miss”。当在指定的时间窗口内，miss 的数目达到 100 时，那么 CronJob 会停止再创建这个 Job 这个时间窗口，可以由 spec.startingDeadlineSeconds 字段指定。比如 startingDeadlineSeconds=200，意味着在过去 200 s 里，如果 miss 的数目达到了 100 次，那么这个 Job 就不会被创建执行了。 ","date":"2023-05-20","objectID":"/job_cronjob/:2:0","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":null,"content":"StatefulSet ","date":"2023-05-20","objectID":"/stateful/:0:0","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":null,"content":"概念 StatefulSet 的设计其实非常容器理解， 它把真实世界里的应用状态， 抽象为两个情况 拓扑状态：这种情况意味着， 应用的多个实例之间不是完全对等的关系， 这些应用实例， 不行按照某些顺序启动，比如应用的主节点A要先于节点B启动， 而如果你把A和B两个Pod删除调， 他们再次被创建出来也必须严格安装这个顺序才行， 并且， 新创建出来的Pod，必须和原理的Pod的网络标识一样， 这样原先的访问者才能使用同样的方法， 访问到这个新Pod。 存储状态：这种情况， 应用的多个实例分别绑定了不同的存储数据， 对于这些应用实例来说，Pod A第一次读取到的数据， 和隔了十分钟之后再次读取到的数据， 应该是同一份，哪怕再次期间Pod A被重新创建过， 这个情况 就是一个数据库应用的多个存储实例 StatefulSet的核心功能， 就是通过某种方式记录这些状态，然后再Pod被重新创建时， 能够为新的Pod恢复这些状态 ","date":"2023-05-20","objectID":"/stateful/:0:1","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":null,"content":"Headless Service Service的VIP方式：当我访问 10.0.23.1 这个 Service 的 IP 地址时，10.0.23.1 其实就是一个 VIP，它会把请求转发到该 Service 所代理的某一个 Pod 上 **Service的DNS方式：**这时候，只要我访问“my-svc.my-namespace.svc.cluster.local”这条 DNS 记录，就可以访问到名叫 my-svc 的 Service 所代理的某一个 Pod。 Normal Service： 你访问“my-svc.my-namespace.svc.cluster.local”解析到的，正是 my-svc 这个 Service 的 VIP Headless Service： 你访问“my-svc.my-namespace.svc.cluster.local”解析到的，就是my-svc代理的某个Pod的IP地址， 区别在于Headless Service不需要分配一个VIP， 而是直接以DNS记录方式解析出被代理的Pod的IP地址 ","date":"2023-05-20","objectID":"/stateful/:0:2","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":null,"content":"Headless Service 对应的 YAML 文件： apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx Headless Service \u003cpod-name\u003e.\u003csvc-name\u003e.\u003cnamespace\u003e.svc.cluster.local ","date":"2023-05-20","objectID":"/stateful/:0:3","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":null,"content":"StatefulSet 又是如何使用这个DNS记录来维持Pod的拓扑状态的？ apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \"nginx\" replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.9.1 ports: - containerPort: 80 name: web serviceName=nginx 字段，告诉 StatefulSet控制器， 在执行控制循环（Control Loop）的时候， 请使用nginx 这个 Headless Service 来保证 Pod 的“可解析身份”。 $ kubectl run -i --tty --image busybox dns-test --restart=Never --rm /bin/sh $ nslookup web-0.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-0.nginx Address 1: 10.244.1.8 $ nslookup web-1.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-1.nginx Address 1: 10.244.2.8 Kubernetes 就是成功地将Pod的拓扑状态（比如：哪个节点先启动，哪个节点后启动），按照 Pod 的“名字 + 编号”的方式固定了下来 ","date":"2023-05-20","objectID":"/stateful/:0:4","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":null,"content":"总结 StatefulSet 这个控制器主要作用之一： 就是使用Pod模板创建Pod的时候，对他们进行编号， 并且按照编号顺序逐一完成创建工作， 而当StatefulSet 的”控制循环”发现Pod的“实际状态”与“期望状态”不一致， 需要新建或者删除Pod进行 “调谐”的时候，它会严格按照这些 Pod 编号的顺序，逐一完成这些操作。 深入理解StatefulSet（二）：存储状态 ","date":"2023-05-20","objectID":"/stateful/:0:5","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":null,"content":"工作原理 首信， StatefulSet的控制器直接管理的是Pod， ， 因为StatefulSet里的不同的Pod实例， 不想ReplicaSet中那样都是完全一样的， 而是有了细微区别的， 比如， 每个Pod的hostanme、名字等都是不同的， 携带了标红。 而statefulSet，区分这些实例的方式， 通过在Pod的名字里加上事先约定号的编号。 其次， Kubernetes通过headless ， 为这些有编号的Pod， 在DNS服务器中生成带有同样标红的DNS记录， 只有StatefulSet能够保证这些Pod的名字的编号不变， 那么Service里类似于 web-0.nginx.default.svc.cluster.local 这样的 DNS 记录也就不会变， 而这条记录解析出来的Pod的IP地址， 则会随着后端Pod的删除和再创建而自动更新， 这当然是Service机制本身的能力， 不需要StatefulSet操心 最后， StatefulSet 还为每个Pod分配并创建一个同样编号的PVC，， 这样 kubernetes 就可以通过Persistent Volume 机制为这个PVC绑定上毒药的PV， 从而保证了每个Pode都拥有一个独立的Volume ","date":"2023-05-20","objectID":"/stateful/:0:6","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":null,"content":"总结 StatefulSet 其实就是一种特殊的 Deployment，而其独特之处在于，它的每个 Pod 都被编号了。而且，这个编号会体现在 Pod 的名字和 hostname 等标识信息上，这不仅代表了 Pod 的创建顺序，也是 Pod 的重要网络标识（即：在整个集群里唯一的、可被的访问身份）。 有了这个编号后，StatefulSet 就使用 Kubernetes 里的两个标准功能：Headless Service 和 PV/PVC，实现了对 Pod 的拓扑状态和存储状态的维护。 ","date":"2023-05-20","objectID":"/stateful/:0:7","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":null,"content":"DaemonSet ","date":"2023-05-20","objectID":"/daemonset/:0:0","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":null,"content":"主要作用 在kubernetes集群里， 运行一个Daemon Pod， 所以， 这个Pod有如下三个特征 这个Pod运载kubernetes集群里的每个节点（Node） 上 每个节点上只有一个这样的Pod实例 当有新的节点加入Kubernetes集群后， 该Pod 会自动的再新节点上被创建出来， 而当节点被删除后， 它上面的Pod也相应会被回收调。 ","date":"2023-05-20","objectID":"/daemonset/:0:1","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":null,"content":"Daemon Pod 的例子 各种网络插件的Agent组件， 都必须运行在每个节点上， 用来处理这个节点上的容器网络 各种存储的插件的Agent组件， 也必须运行在每个节点上， 用来在这个节点上挂载远程存储目录，操作容器的Volume目录 各种监控组件和日志组件， 也必须运行在每个节点上，复制这个节点上监控信息和日志搜索 API apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd-elasticsearch image: k8s.gcr.io/fluentd-elasticsearch:1.20 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers ","date":"2023-05-20","objectID":"/daemonset/:0:2","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":null,"content":"Daemonset 如何确保每个Node上有且只有一个被管理的Pod？ Daemonset Controller ， 首先从Etcd 里获取所有的Node列表，然后遍历所有的Node， 这时， 它就可以很容器的去检查， 当前这个Node上是不是携带了name=fluentd-elasticsearch 标签的 Pod 在运行。 检查结果有三种情况 没有每种Pod， 那么就意味着这个Node创建这样一个Pod 有这种Pod， 但是数量大于1， 那就说明 多余的Pod从这个Node删除掉 正好只有一个这种Pod， 说明这个节点是正常的 ","date":"2023-05-20","objectID":"/daemonset/:1:0","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":null,"content":"nodeAffinity apiVersion: v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: metadata.name operator: In values: - node-geektime ","date":"2023-05-20","objectID":"/daemonset/:2:0","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":null,"content":"含义 requiredDuringSchedulingIgnoredDuringExecution：这个nodeAffunuty 必须在每个调度的时候给予考虑， 同时 意味着可以设置在某个情况下不考虑这个nodeAffinity 这个Pod, 将来只允许允许在“metadata.name是“node-geektime”的节点上 Daemonset Controller 会在创建Pod的时候， 自动在这个Pod的API对象里， 加上这样一个nodeAffinity定义 ","date":"2023-05-20","objectID":"/daemonset/:2:1","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":null,"content":"tolerations 这个字段： 意味着这个Pod， 会“容忍”（Toleration）某些Node的污点（Taint） apiVersion: v1 kind: Pod metadata: name: with-toleration spec: tolerations: - key: node.kubernetes.io/unschedulable operator: Exists effect: NoSchedule ","date":"2023-05-20","objectID":"/daemonset/:3:0","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":null,"content":"含义 toleration： “容忍”所有被标记为unschedulable“污点”的Node， “容忍”的效果是允许调度 在 Kubernetes 项目中，当一个节点的网络插件尚未安装时，这个节点就会被自动加上名为node.kubernetes.io/network-unavailable的“污点”。 而通过这样一个 Toleration，调度器在调度这个 Pod 的时候，就会忽略当前节点上的“污点”，从而成功地将网络插件的 Agent 组件调度到这台机器上启动起来。 ","date":"2023-05-20","objectID":"/daemonset/:3:1","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":null,"content":"Deployment ","date":"2023-05-20","objectID":"/deployment/:0:0","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":null,"content":"Deployment Pod 的 “水平扩展、收缩” ","date":"2023-05-20","objectID":"/deployment/:0:1","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":null,"content":"ReplicaSet apiVersion: apps/v1 kind: ReplicaSet metadata: name: nginx-set labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 一个ReplicaSet对象， 其实就是由副本数目的定义和一个Pod模板组合， 更重要的是 Deployment控制器实际操纵的， 正是ReplicasSet对象， 而不是Pod对象 ","date":"2023-05-20","objectID":"/deployment/:0:2","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":null,"content":"Deployment apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 ","date":"2023-05-20","objectID":"/deployment/:0:3","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":null,"content":"Deployment 、 ReplicaSet、 Pod的关系 ","date":"2023-05-20","objectID":"/deployment/:0:4","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":null,"content":"水平扩展、收缩的操作 $ kubectl scale deployment nginx-deployment --replicas=4 deployment.apps/nginx-deployment scaled ","date":"2023-05-20","objectID":"/deployment/:0:5","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":null,"content":"滚动更新 $ kubectl create -f nginx-deployment.yaml --record -record 参数： 记录每次操作的执行命令， 方便后面查看 检查一下 nginx-deployment 创建后的状态信息 $ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 0 0 0 1s 状态含义 DESIERD：用户期望的Pod 副本个数 CURRENT：当前处于Running状态的Pod的个数 UP-TO-DATE： 当前处于最新版的Pod的个数， （Pod的Spec部分与Deployment的Pod模板的定义的一致） AVALABLE： 当前已经可用的Pod的个数， 既是 Running 状态，又是最新版本，并且已经处于 Ready（健康检查正确）状态的 Pod 的个数。 查看Deployment对象的状态变化kubectl rollout status $ kubectl rollout status deployment/nginx-deployment Waiting for rollout to finish: 2 out of 3 new replicas have been updated... deployment.apps/nginx-deployment successfully rolled out ","date":"2023-05-20","objectID":"/deployment/:0:6","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":null,"content":"Deployment对应进行版本控制的具体原理 这个镜像名字修改成为了一个错误的名字，比如：nginx:1.91。这样，这个 Deployment 就会出现一个升级失败的版本。 $ kubectl set image deployment/nginx-deployment nginx=nginx:1.91 deployment.extensions/nginx-deployment image updated 由于这个 nginx:1.91 镜像在 Docker Hub 中并不存在，所以这个 Deployment 的“滚动更新”被触发后，会立刻报错并停止。 这时，我们来检查一下 ReplicaSet 的状态，如下所示： $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-1764197365 2 2 2 24s nginx-deployment-3167673210 0 0 0 35s nginx-deployment-2156724341 2 2 0 7s 回滚到一起的旧版本 $ kubectl rollout undo deployment/nginx-deployment deployment.extensions/nginx-deployment 要使用 kubectl rollout history 命令，查看每次 Deployment 变更对应的版本 $ kubectl rollout history deployment/nginx-deployment deployments \"nginx-deployment\" REVISION CHANGE-CAUSE 1 kubectl create -f nginx-deployment.yaml --record 2 kubectl edit deployment/nginx-deployment 3 kubectl set image deployment/nginx-deployment nginx=nginx:1.91 查看每个版本对应的Deployment的API对象的细节 $ kubectl rollout history deployment/nginx-deployment --revision=2 可以在kubectl roolout undo 命令加上回滚的版本号， 指定版本回滚 $ kubectl rollout undo deployment/nginx-deployment --to-revision=2 deployment.extensions/nginx-deployment 对Deployment的多次更新操作， 最后只生成一个ReplicaSet， $ kubectl rollout pause deployment/nginx-deployment deployment.extensions/nginx-deployment paused 是Deployment进入一个暂定状态， 可以随意使用 kubectl edit 或者 kubectl set image 指令，修改这个 Deployment 的内容了。 操作完成之后将Deployment 恢复回来 $ kubectl rollout resume deploy/nginx-deployment deployment.extensions/nginx-deployment resumed ","date":"2023-05-20","objectID":"/deployment/:0:7","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":null,"content":"如何控制“历史的” ReplicaSet数量 Deployment 对象有一个字段，叫作 spec.revisionHistoryLimit，就是 Kubernetes 为 Deployment 保留的“历史版本”个数。所以，如果把它设置为 0，你就再也不能做回滚操作了 ","date":"2023-05-20","objectID":"/deployment/:0:8","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":null,"content":"pod ","date":"2023-05-17","objectID":"/pod/:0:0","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":null,"content":"pod 的属性 凡是调度、网络、存储、以及安全相关的属性， 都是pod级别的 ","date":"2023-05-17","objectID":"/pod/:0:1","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":null,"content":"NodeSelector 用户将Pod 与Node进行绑定的字段 apiVersion: v1 kind: Pod ... spec: nodeSelector: disktype: ssd ","date":"2023-05-17","objectID":"/pod/:0:2","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":null,"content":"NodeName 一旦pod 这个字段被赋值， Kubernetes项目就会被认为这个POd以及经过调度， 调度的结果就是复制的节点名字 ","date":"2023-05-17","objectID":"/pod/:0:3","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":null,"content":"HostAliases 定义了Pod的Hosts文件**（比如 /etc/hosts）里的内容**，用法如下： apiVersion: v1 kind: Pod ... spec: hostAliases: - ip: \"10.1.2.3\" hostnames: - \"foo.remote\" - \"bar.remote\" ... pod启动之后、 /etc/hosts 文件内容如下 cat /etc/hosts # Kubernetes-managed hosts file. 127.0.0.1 localhost ... 10.244.135.10 hostaliases-pod 10.1.2.3 foo.remote 10.1.2.3 bar.remote ","date":"2023-05-17","objectID":"/pod/:0:4","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":null,"content":"shareProcessNamespace=true： apiVersion: v1 kind: Pod metadata: name: nginx spec: shareProcessNamespace: true containers: - name: nginx image: nginx - name: shell image: busybox stdin: true tty: true pod 的容器共享 PID Namespace ","date":"2023-05-17","objectID":"/pod/:0:5","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":null,"content":"ImagePullPolicy 默认是Always ，每次都出重新拉取 Never： 意味着Pod永远不会主动拉取这个镜像， IfNotPresent： 只在宿主机不存在这个镜像时才拉取 ","date":"2023-05-17","objectID":"/pod/:0:6","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":null,"content":"lifecycle 在容器发生变化的时候触发一系列“钩子” apiVersion: v1 kind: Pod metadata: name: lifecycle-demo spec: containers: - name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: [\"/bin/sh\", \"-c\", \"echo Hello from the postStart handler \u003e /usr/share/message\"] preStop: exec: command: [\"/usr/sbin/nginx\",\"-s\",\"quit\"] ","date":"2023-05-17","objectID":"/pod/:0:7","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":null,"content":"pod对象在kubernetes中的生命周期 Pending：这个状态意味着， Pod的Yaml 文件以及提交给了kubernetes， API对象已经被创建并保存在Etcd 当中， 但是， 这个Pod 里有些容器因为某些原因不能被顺利创建， 比如， 调度不成功 Running： Pod已经调度成功， 跟一个具体的节点绑定，它包含的容器已经被创建，并且至少有一个已经运行成功 Succeeded： Pod的所有容器都运行成功， 并且已经退出， Failed： 这个状态下， Pod 里至少有一个容器以不正常的状态退出， 这个砖头的出现， 意味着你想办法Debug这个容器的应用， unknown： 异常状态， 意味着Pod的状态不能持续的被kubelet汇报给kube-apiserver， 很有可能是主从节点的通信出现了问题 ","date":"2023-05-17","objectID":"/pod/:0:8","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":null,"content":"projected Volume 四种 Secret ConfigMap Downward API ServiceAccountToken ","date":"2023-05-17","objectID":"/pod/:0:9","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":null,"content":"secret 把Pod想要访问的加密数据存在Etcd 中， 可以通过Pod的容器挂载Volume的方式， 访问这些Secret 保存的信息 ","date":"2023-05-17","objectID":"/pod/:0:10","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":null,"content":"configmap 保存的是不需要加密的， 应用所需的配置信息 ","date":"2023-05-17","objectID":"/pod/:0:11","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":null,"content":"Downward API 让Pod里的容器能够直接获取到这个Pod API对象本身的信息 apiVersion: v1 kind: Pod metadata: name: test-downwardapi-volume labels: zone: us-est-coast cluster: test-cluster1 rack: rack-22 spec: containers: - name: client-container image: k8s.gcr.io/busybox command: [\"sh\", \"-c\"] args: - while true; do if [[ -e /etc/podinfo/labels ]]; then echo -en '\\n\\n'; cat /etc/podinfo/labels; fi; sleep 5; done; volumeMounts: - name: podinfo mountPath: /etc/podinfo readOnly: false volumes: - name: podinfo projected: sources: - downwardAPI: items: - path: \"labels\" fieldRef: fieldPath: metadata.labels 支持的字段 spec.nodeName - 宿主机名字 status.hostIP - 宿主机 IP metadata.name - Pod 的名字 metadata.namespace - Pod 的 Namespace status.podIP - Pod 的 IP ","date":"2023-05-17","objectID":"/pod/:0:12","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":null,"content":"Service Account Service Account 对象的作用，就是 Kubernetes 系统内置的一种“服务账户”，它是 Kubernetes 进行权限分配的对象 Service Account 的授权信息和文件，实际上保存在它所绑定的一个特殊的 Secret 对象里的。这个特殊的 Secret 对象，就叫作ServiceAccountToken 这种把 Kubernetes 客户端以容器的方式运行在集群里，然后使用 default Service Account 自动授权的方式，被称作“InClusterConfig”，也是我最推荐的进行 Kubernetes API 编程的授权方式。 ","date":"2023-05-17","objectID":"/pod/:0:13","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":null,"content":"容器的健康检查和恢复机制 容器定义一个监控检查“探针”， kubelet就会根据这个Probe的返回值决定这个容器的状态，而不是直接以容器进行是否运行（来自 Docker 返回的信息）作为依据。这种机制，是生产环境中保证应用健康存活的重要手段。 apiVersion: v1 kind: Pod metadata: labels: test: liveness name: test-liveness-exec spec: containers: - name: liveness image: busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 ## 在容器启动后5s后开始执行 periodSeconds: 5 ## 每5s执行一次 Kubernetes 里的Pod 恢复机制，也叫 restartPolicy ","date":"2023-05-17","objectID":"/pod/:0:14","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":null,"content":"基本原理 只有Pod 的 restartPolicy 指定的策略允许重启的容器， 那么这个Pod就会保持Running状态，并进行容器重启 对于包含多个容器的Pod，只有它里面所有的容器都进入异常状态后，Pod 才会进入 Failed 状态 。在此之前，Pod 都是 Running 状态。此时，Pod 的 READY 字段会显示正常容器的个数 ","date":"2023-05-17","objectID":"/pod/:0:15","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":null,"content":"PodPreset PodPreset里定义的内容， 只会在Pod API对象被创建之前追加这个对象本身上， 而不会影响热河Pod的控制的定义 PodPreset 这样专门用来对 Pod 进行批量化、自动化修改的工具对象 ","date":"2023-05-17","objectID":"/pod/:0:16","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"}]