[{"categories":["文档"],"content":"图解 Kubernetes 核心组件工作原理 ","date":"2024-01-08","objectID":"/kubernetes_%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/:0:0","tags":["k8s"],"title":"Kubernetes_核心组件工作原理","uri":"/kubernetes_%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/"},{"categories":["文档"],"content":"问题1: Master节点和Worker节点如何通信？ 首先， 当Master节点启动时， 会运行一个 Kube-apiserver进程，它提供了集群管理的API接口，是集群中各个功能模块之间进行数据交互和通信的中心枢纽， 同时也提供了一个完善的机器安全机制。 在Node 节点上， 利用Kubernetes中的kubelet组件，每个Node节点上都会运行一个kubelet进程，负责向Master 汇报本节点的运行状态， 如 Node节点注册、终止、定期健康报告等等，并接收来自Master 的命令并创建相应的Pod。 在Kubernetes中， Pod是最基本的运行单元，它与Docker容器略有不同， 因为pod中可能包好一个或者多个容器， 这些容器内部共享网络资源，即可以通过localhost 相互访问 关于如何在Pod中实现网络共享，每个pod启动，内部都会启动一个pause容器，它使用默认的网络模式，其他容器的网络设置完成网络共享问题 k8s01 ","date":"2024-01-08","objectID":"/kubernetes_%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/:1:0","tags":["k8s"],"title":"Kubernetes_核心组件工作原理","uri":"/kubernetes_%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/"},{"categories":["文档"],"content":"问题2: Master 如何将Pod调度到指定的Node上？ 这项工作由 Kube-scheduler完成，整个调度过程通过执行一系列复杂的算法，最终为每个pod计算出一个最优的目标Node， 这是由Kube-scheduler进程自动完成的 最常见的是循环调度（RR）。当然也有可能需要将Pod调度到指定的Node上，我们可以通过将节点的标签（Label）与Pod的节点选择器进行匹配来达到指定的效果 k8s02 ","date":"2024-01-08","objectID":"/kubernetes_%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/:2:0","tags":["k8s"],"title":"Kubernetes_核心组件工作原理","uri":"/kubernetes_%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/"},{"categories":["文档"],"content":"问题3: 各个节点和Pod的信息统一在哪里维护，谁来维护？ 从上面的Pod调度来看， 我们必须有一个存储中心来存储每个节点的每个Pod的资源使用情况、监控状态和基本情况， 这样Pod调度才能正常进行 在Kubernetes中，etcd组件被用作高可用和一致的存储库 该组件可以内置在Kubernets中， 也可以外部构建提供Kubernetes使用。 集群上的所有配置信息都存储在etcd中， 考虑到各个组件的相对应的独立性和整体的可维护性，这些存储的数据的增删改查都是统一由 Kube-apiserver来调用的，并且apiserver还提供了REST支持， 不仅为各个内部组件提供服务单也想集群外部的用户公开服务 外部用户可以通过REST接口或kubectl命令行工具管理集群，该工具内部与apiserver通信 k8s03 ","date":"2024-01-08","objectID":"/kubernetes_%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/:3:0","tags":["k8s"],"title":"Kubernetes_核心组件工作原理","uri":"/kubernetes_%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/"},{"categories":["文档"],"content":"问题4: 外部用户如何访问集群中的运行的pod？ 在分布式集群中， 服务往往由多个应用提供，以分担访问压力，而这些应用可能分布在多个节点上， 这就设计到跨主机通信 这里Kubernetes引入了Service的概念，将多个相同的Pod包装成一个完整的服务，对外提供服务。至于获取这些相同的Pod，每个Pod在启动时都会设置labels为attribute。 在服务中， 我们传递选择器Selectort， 选择与整体服务具有相同Name标签属性的Pod， 将服务信息通过Apiserver存储到etcd中， 由Service Controller完成，同时在每个节点上启动一个kube-proxy进程， 负责从服务地址到Pod地址的代理和负载均衡 k8s04 ","date":"2024-01-08","objectID":"/kubernetes_%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/:4:0","tags":["k8s"],"title":"Kubernetes_核心组件工作原理","uri":"/kubernetes_%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/"},{"categories":["文档"],"content":"问题5: Pod 如何动态扩容和伸缩？ 在Kubernetes 中，使用Replication Controller进行管理，为每个Pod设置一个预期的副本数，当实际副本数量不符合预期时，动态调整数量以达到预期值，所需值可以由我们手动更新，也可以由自动缩放代理更新 k8s05 ","date":"2024-01-08","objectID":"/kubernetes_%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/:5:0","tags":["k8s"],"title":"Kubernetes_核心组件工作原理","uri":"/kubernetes_%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/"},{"categories":["文档"],"content":"问题6:各个组件如何协同工作？ Kube-controller-manager进程的作用。我们知道ectd是作为集群数据的存储中心，而apiserver是用来管理数据中心，充当其他进程与数据中心通信的桥梁。 Service Controller 和 Replication Controller由Kube-controller-manager管理。作为一个守护进程，每个Controller都是一个控制回路， 通过apiserver监控集群的共享状态，并尝试改变 将实际状态中不合符预期的变化 关于 Controller，管理器还包含 Node Controller、 ResourceQuota Controller、Namespace Controller 等 k8s06 ","date":"2024-01-08","objectID":"/kubernetes_%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/:6:0","tags":["k8s"],"title":"Kubernetes_核心组件工作原理","uri":"/kubernetes_%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/"},{"categories":["文档"],"content":"VXLAN介绍 VXLAN（Virtual eXtensible LAN，可拓展虚拟局域网络） VXLAN 可以基于已有的服务提供商或企业IP网络， 为离散的物理站点提供二层互联， 并且可以为不同的租户提供业务隔离 Kubernetes_核心组件工作原理 ","date":"2024-01-08","objectID":"/vxlan/:0:0","tags":["network"],"title":"Vxlan","uri":"/vxlan/"},{"categories":["文档"],"content":"Serverless 介绍 serverless 指的是由开发者实现的服务端逻辑运行在无状态的计算容器中, 它由事件触发，完全由第三方管理，其业务层面的状态则被开发者使用的数据库和存储资源所记录 Baas： 后端及服务， 一般是一个个的API调用后端或者别人已经别人已经实现好的逻辑 Faas： 函数即服务， 本质上是一种事件驱动的由消息触发的服务， Faas供应商一般会集成各种同步和异步的事件源， 通过订阅这些事件源， 可以突发或者定期的触发函数运行 ","date":"2023-12-25","objectID":"/serverless-openfaas/:0:0","tags":["serverless"],"title":"Serverless Openfaas","uri":"/serverless-openfaas/"},{"categories":["文档"],"content":"Faas的优势和不足 优势 降低人力成本 降低风险 减少资源开销 增加伸缩的灵活性 缩短创新周期 不足 不适合长时间运行的应用 状态管理 冷启动时间 缺乏调试和开发工具 构建复杂 ","date":"2023-12-25","objectID":"/serverless-openfaas/:0:1","tags":["serverless"],"title":"Serverless Openfaas","uri":"/serverless-openfaas/"},{"categories":["文档"],"content":"应用场景 发送通知 webhhok 轻量级API 物联网 数据统计分析 Trigger和定时任务 聊天机器人 ","date":"2023-12-25","objectID":"/serverless-openfaas/:0:2","tags":["serverless"],"title":"Serverless Openfaas","uri":"/serverless-openfaas/"},{"categories":["文档"],"content":"Faas 蓝图 OpenFaas OpenFaaS一款高人气的开源的faas框架，可以直接在Kubernetes上运行，也可以基于Swarm或容器运行 openFaas的架构图 用到的镜像如下： functions/faas-netesd:0.3.4 functions/gateway:0.6.14 functions/prometheus:latest-k8s functions/alertmanager:latest-k8s ","date":"2023-12-25","objectID":"/serverless-openfaas/:1:0","tags":["serverless"],"title":"Serverless Openfaas","uri":"/serverless-openfaas/"},{"categories":["文档"],"content":"基础组件 Gateway Provider queue-worker watchdog Gateway： 为函数调用提供一个路由， 起到一个代理转发的作用 内置UI 界面， 可访问函数商店 Prometheus 手机监控指标 接收AlertManager 通知， 自动伸缩 ","date":"2023-12-25","objectID":"/serverless-openfaas/:1:1","tags":["serverless"],"title":"Serverless Openfaas","uri":"/serverless-openfaas/"},{"categories":["文档"],"content":"自动伸缩函数 根据每秒请求数 内存和CPU 使用量 最大和最小副本数 com.openfaas.sacle.min 最小 com.openfaas.sacle.max 最大 com.openfaas.sacle.factor 步长 手动调用 API 伸缩 AlertManaer 触发 获取现在副本数 计算新副本数： min+(max/100)*factor 调用 k8s API 设置新副本数 ","date":"2023-12-25","objectID":"/serverless-openfaas/:1:2","tags":["serverless"],"title":"Serverless Openfaas","uri":"/serverless-openfaas/"},{"categories":["文档"],"content":"watchdog 职责是调用函数， healthcheck 和超时 任何二进制都会被watchdog 变成一个函数 小型的http server ","date":"2023-12-25","objectID":"/serverless-openfaas/:1:3","tags":["serverless"],"title":"Serverless Openfaas","uri":"/serverless-openfaas/"},{"categories":["文档"],"content":"queue-worker 同步函数 路由是/function/:name 等待 结束时得到结果 明确知道是成功还是失败 异步函数 路由是 /async-function/:name http的状态码是202， 即时响应码 从queue-worker中调用函数 默认情况下，函数的执行结果是被丢弃的 ","date":"2023-12-25","objectID":"/serverless-openfaas/:1:4","tags":["serverless"],"title":"Serverless Openfaas","uri":"/serverless-openfaas/"},{"categories":["文档"],"content":"什么是 Shared Memory 共享内存是 Unix 下的多进程之间的通信方式之一，它使得多进程可以访问同一块内存空间， 不通进程可以及时看到对方进程中共享内存中数据的更新 由于多个进程共享一段内存，这种方式需要依靠某种同步操作， 如互斥锁和信号量等来达到进程间的同步及互斥， 可以说 Shared Memory 是最有用的进程间通信方式。 以上对 Shared Memory 的解说摘自 进程间通信 IPC (InterProcess Communication) 这篇文章 ","date":"2023-12-18","objectID":"/kuberenetes-sharedmemory/:0:0","tags":["k8s"],"title":"Kuberenetes SharedMemory","uri":"/kuberenetes-sharedmemory/"},{"categories":["文档"],"content":"Docker 查看docker 容器内存的共享内存 ➜ docker run -it --rm lqshow/busybox-curl:1.28 sh /data # df -h /dev/shm Filesystem Size Used Available Use% Mounted on shm 64.0M 0 64.0M 0% /dev/shm Docker 默认分配的共享内存大小就是 64MB 可以通过设置参数 --shm-size 来调整默认的共享内存大小 docker run -it --rm --shm-size 256M lqshow/busybox-curl:1.28 sh /data # df -h /dev/shm Filesystem Size Used Available Use% Mounted on shm 256.0M 0 256.0M 0% /dev/shm ","date":"2023-12-18","objectID":"/kuberenetes-sharedmemory/:1:0","tags":["k8s"],"title":"Kuberenetes SharedMemory","uri":"/kuberenetes-sharedmemory/"},{"categories":["文档"],"content":"Kubernetes k8s 内 Pod 內的实际运行情况 cat \u003c\u003cEOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: hello-world labels: name: hello-world spec: containers: - image: lqshow/busybox-curl:1.28 name: hello-world command: ['sh', '-c'] args: - while true; do echo hello-world; sleep 1; done EOF 我们在Kubernetes 创建的 Pod内， 发现其共享内存默认也是 64MB， Docker 容器內默认的共享内存大小是一样的。 kubectl exec -it hello-world -- df -h /dev/shm Filesystem Size Used Available Use% Mounted on shm 64.0M 0 64.0M 0% /dev/shm 我们尝试用 dd 命令去写 100M 的内容， 系统无法完整的写入，提示 No space left on device 错误 kubectl exec -it hello-world sh /data # dd if=/dev/zero of=/dev/shm/output bs=1M count=100 dd: writing '/dev/shm/output': No space left on device 65+0 records in 64+0 records out 67108864 bytes (64.0MB) copied, 0.060007 seconds, 1.0GB/s 再次做下确认，发现共享内存其实已经被 dd 写满 64MB 了。 /data # df -h /dev/shm Filesystem Size Used Available Use% Mounted on shm 64.0M 64.0M 0 100% /dev/shm pod 里缺失无法使用超过 64MB 的 Shared Memory， ","date":"2023-12-18","objectID":"/kuberenetes-sharedmemory/:2:0","tags":["k8s"],"title":"Kuberenetes SharedMemory","uri":"/kuberenetes-sharedmemory/"},{"categories":["文档"],"content":"解决方案 Kubernetes 的资源模型中， 可以将 pod 中 Memory 资源在 limits 中做配置（ spec.containers[].resources.limits.memory ），但是它只对Cgroups 中的 memory.limit_in_bytes 起作用，并不会作用到 Shared Memory 中。 官方文档有提到，可以通过将 emptyDir 挂载到 /dev/shm，并将介质类型设置为Memory 来解决这个问题。 ","date":"2023-12-18","objectID":"/kuberenetes-sharedmemory/:3:0","tags":["k8s"],"title":"Kuberenetes SharedMemory","uri":"/kuberenetes-sharedmemory/"},{"categories":["文档"],"content":"1、 只设置介质类型 脚本放在 Kubernetes 集群內执行 cat \u003c\u003cEOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: hello-world labels: name: hello-world spec: volumes: - name: dshm emptyDir: # 只设置介质类型 medium: Memory containers: - image: lqshow/busybox-curl:1.28 name: hello-world command: ['sh', '-c'] args: - while true; do echo hello-world; sleep 1; done volumeMounts: - mountPath: /dev/shm name: dshm EOF 查看 pod 的情况 kubectl exec -it hello-world -c hello-world -- df -h /dev/shm Filesystem Size Used Available Use% Mounted on tmpfs 14.7G 0 14.7G 0% /dev/shm 查看 pod 所在节点的内存情况 ssh 138 'free -h' total used free shared buff/cache available Mem: 29G 23G 2.6G 1.4G 3.6G 20G Swap: 0B 0B 0B 虽然未设置 sizeLimit，但是显示分配了 HOST 主机上将近一半的内存。 HOST 主机上提示 可用的内存其实只有 2.6G 了，但是 Pod 內提示Available 的还有 14.7G， ","date":"2023-12-18","objectID":"/kuberenetes-sharedmemory/:4:0","tags":["k8s"],"title":"Kuberenetes SharedMemory","uri":"/kuberenetes-sharedmemory/"},{"categories":["文档"],"content":"2、 设置介质类型， 同时配置 sizeLimit Deployment 资源， cat \u003c\u003cEOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: hello-world labels: app: hello-world spec: replicas: 1 selector: matchLabels: app: hello-world template: metadata: labels: app: hello-world spec: volumes: - name: dshm emptyDir: # 设置介质类型，且配置 sizeLimit medium: Memory sizeLimit: 256Mi containers: - name: hello-world image: lqshow/busybox-curl:1.28 command: ['sh', '-c'] args: - while true; do echo hello-world; sleep 1; done volumeMounts: - mountPath: /dev/shm name: dshm EOF 我们通过实际运行的情况，发现即便设置了 sizelimt 限制，但是实际显示仍然分配了 HOST 主机上将近一半的内存，和第一个测试结果是一模一样的 kubectl exec -it $(kubectl get pod -l app=hello-world --no-headers|grep -v \"Evicted\"|awk '{print $1}') -c hello-world -- df -h /dev/shm Filesystem Size Used Available Use% Mounted on tmpfs 14.7G 0 14.7G 0% /dev/shm 使用dd 尝试去做验证 kubectl exec -it $(kubectl get pod -l app=hello-world --no-headers|grep -v \"Evicted\"|awk '{print $1}') -c hello-world sh # 1. 先尝试直接写满 256 M /data # dd if=/dev/zero of=/dev/shm/output bs=1M count=256 256+0 records in 256+0 records out 268435456 bytes (256.0MB) copied, 0.520999 seconds, 491.4MB/s /data # /data # df -h /dev/shm Filesystem Size Used Available Use% Mounted on tmpfs 14.7G 256.0M 14.4G 2% /dev/shm /data # # 2. 再尝试写入 100 M，并没有看到出现 `No space left on device` 的提示 /data # dd if=/dev/zero of=/dev/shm/output2 bs=1M count=100 100+0 records in 100+0 records out 104857600 bytes (100.0MB) copied, 0.227946 seconds, 438.7MB/s /data # # 3. 发现成功的写入了 100 M /data # df -h /dev/shm Filesystem Size Used Available Use% Mounted on tmpfs 14.7G 356.0M 14.4G 2% /dev/shm # 4. 在一定时间间隔內，Pod 会因为异常被驱逐 /data # command terminated with exit code 137 sizeLimit 实际上是起了作用的，Pod 最后是以 137 CODE 退出了 被驱逐 Pod 的 Event 信息，只保留了重要的提示 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- ... Warning Evicted 20m kubelet, kind-dev4 Usage of EmptyDir volume \"dshm\" exceeds the limit \"256Mi\". ... 当宿主机资源紧张的情况下，kubelet 会主动地结束 Pod 以回收短缺的资源。具体驱逐 Pod ","date":"2023-12-18","objectID":"/kuberenetes-sharedmemory/:5:0","tags":["k8s"],"title":"Kuberenetes SharedMemory","uri":"/kuberenetes-sharedmemory/"},{"categories":["文档"],"content":"升级 Kubernetes 集群版本 从官方文档给出的提示可以看出，SizeMemoryBackedVolumes 这个特性在我们的集群內并没有生 SizeMemoryBackedVolumes 是从 1.20 版本才开始支持 ","date":"2023-12-18","objectID":"/kuberenetes-sharedmemory/:6:0","tags":["k8s"],"title":"Kuberenetes SharedMemory","uri":"/kuberenetes-sharedmemory/"},{"categories":["文档"],"content":"在Kubernetes下实现网络抓包（sniff） 如果 kubernetes 线上服务出现问题， 通过网络抓包，排查网络来定位问题 常见的抓包有几种方法 ","date":"2023-12-18","objectID":"/kuberenetes-tcpdump/:0:0","tags":["k8s"],"title":"Kuberenetes  网络抓包  sniff","uri":"/kuberenetes-tcpdump/"},{"categories":["文档"],"content":"在宿主机上抓包 首先定位pod内的容器被调度到哪个Node 上， 在相应的Node里， 对容器进行抓包 ","date":"2023-12-18","objectID":"/kuberenetes-tcpdump/:1:0","tags":["k8s"],"title":"Kuberenetes  网络抓包  sniff","uri":"/kuberenetes-tcpdump/"},{"categories":["文档"],"content":"定位Pod 的containerID 以及它所运行的宿主机IP 在 Kubernetes 集群内执行下面这个指令，并从返回的结果中拿到两个信息 宿主机 IP = 172.18.0.xxx container ID = 78e91175699f..... # 注: 需要替换 namespace 和 pod name ➜ kubectl get pod \\ -n ${NAMESPACE} ${POD_NAME} \\ -o json|jq '.status|{hostIP: .hostIP, container: [.containerStatuses[]|{name: .name, containerID: .containerID}]}' { \"hostIP\": \"172.18.0.xxx\", \"container\": [ { \"name\": \"linkerd-proxy\", \"containerID\": \"docker://78e91175699f8cc0a3b0ff87da97407c19c7a86706a5b74e2d86f4428a4de75a\" }, { \"name\": \"nginx\", \"containerID\": \"docker://7a6f7eabc2d5112437d30ee8ec1aa7ef963e97c3d09c3bc63613a70d106d7d01\" } ] } ","date":"2023-12-18","objectID":"/kuberenetes-tcpdump/:1:1","tags":["k8s"],"title":"Kuberenetes  网络抓包  sniff","uri":"/kuberenetes-tcpdump/"},{"categories":["文档"],"content":"查询网络接口索引 通过ssh 登陆到pod所在的宿主机上， 然后在容器内执行 cat /sys/class/net/eth0/iflink ， 查找容器中的网卡与宿主机的veth 网卡直接的对应关系 # 注: 需要替换 container id docker exec -it ${CONTAINER_ID} /bin/bash -c 'cat /sys/class/net/eth0/iflink' 10227 ","date":"2023-12-18","objectID":"/kuberenetes-tcpdump/:1:2","tags":["k8s"],"title":"Kuberenetes  网络抓包  sniff","uri":"/kuberenetes-tcpdump/"},{"categories":["文档"],"content":"查找网络接口信息 在宿主机上做 ip link 操作 ip link |grep 10227 10227: calicf227ed888a@if4: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1440 qdisc noqueue state UP mode DEFAULT group default ","date":"2023-12-18","objectID":"/kuberenetes-tcpdump/:1:3","tags":["k8s"],"title":"Kuberenetes  网络抓包  sniff","uri":"/kuberenetes-tcpdump/"},{"categories":["文档"],"content":"在宿主机上抓包 通过tcpdump指令， 将报文保存到文件中 tcpdump -i calicf227ed888a@if4 -w /root/tcpdump.pcap ","date":"2023-12-18","objectID":"/kuberenetes-tcpdump/:1:4","tags":["k8s"],"title":"Kuberenetes  网络抓包  sniff","uri":"/kuberenetes-tcpdump/"},{"categories":["文档"],"content":"在pod 内抓包 在目标pod 内抓包 # 注: -c ${CONTAINER_NAME} 是可选择的。如果 Pod 中仅包含一个容器，就可以忽略它 kubectl exec ${POD_NAME} -c ${CONTAINER_NAME} -- tcpdump -i eth0 -w - | wireshark -k -i - ","date":"2023-12-18","objectID":"/kuberenetes-tcpdump/:2:0","tags":["k8s"],"title":"Kuberenetes  网络抓包  sniff","uri":"/kuberenetes-tcpdump/"},{"categories":["文档"],"content":"通过 ephemeral containers 抓包 值得一提的是 Kubernetes 在 v1.16 [Alpha] 开始支持 Ephemeral Containers[2]，它正好可以解决上面提的 2 个痛点，临时容器对于排除交互式故障非常有用，Kubernetes 在 v1.23 [beta] 已经默认开启该功能了。 比如我使用 nicolaka/netshoot[3] 镜像用来调试，用法如下 # 注: 需要替换 pod name 和 container name kubectl debug -i ${POD_NAME} \\ --image=nicolaka/netshoot \\ --target=${CONTAINER_NAME} -- tcpdump -i eth0 -w - | wireshark -k -i - ","date":"2023-12-18","objectID":"/kuberenetes-tcpdump/:3:0","tags":["k8s"],"title":"Kuberenetes  网络抓包  sniff","uri":"/kuberenetes-tcpdump/"},{"categories":["文档"],"content":"通过 Ksniff 抓包 ksniff 是一个 kubectl 的插件，它利用 tcpdump 和 Wireshark 对 Kubernetes 集群中的任何 Pod 启动远程抓包 ksniff 一般使用 krew 这个 kubectl 包管理器进行安装 ","date":"2023-12-18","objectID":"/kuberenetes-tcpdump/:4:0","tags":["k8s"],"title":"Kuberenetes  网络抓包  sniff","uri":"/kuberenetes-tcpdump/"},{"categories":["文档"],"content":"安装 Krew ( set -x; cd \"$(mktemp -d)\" \u0026\u0026 curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/download/v0.4.4/krew-linux_amd64.tar.gz\" \u0026\u0026 tar zxvf krew.tar.gz \u0026\u0026 KREW=./krew-\"$(uname | tr '[:upper:]' '[:lower:]')_$(uname -m | sed -e 's/x86_64/amd64/' -e 's/arm.*$/arm/')\" \u0026\u0026 \"$KREW\" install krew ) export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\" ","date":"2023-12-18","objectID":"/kuberenetes-tcpdump/:4:1","tags":["k8s"],"title":"Kuberenetes  网络抓包  sniff","uri":"/kuberenetes-tcpdump/"},{"categories":["文档"],"content":"安装 sniff 插件 kubectl krew install sniff ","date":"2023-12-18","objectID":"/kuberenetes-tcpdump/:4:2","tags":["k8s"],"title":"Kuberenetes  网络抓包  sniff","uri":"/kuberenetes-tcpdump/"},{"categories":["文档"],"content":"执行 kubectl sniff 命令抓包 kubectl sniff ${POD_NAME} -n ${NAMESPACE} 执行 sniff 命令后，本地会自动启动 Wireshark 进行抓包，如下图 sniff02 以下是 sniff 运行的日志，我只提取了日志的关键部分 ➜ kubectl sniff httpbin -n default time=\"2022-02-20T19:56:13+08:00\" level=info msg=\"using tcpdump path at: '/Users/linqiong/.krew/store/sniff/v1.6.1/static-tcpdump'\" time=\"2022-02-20T19:56:13+08:00\" level=info msg=\"selected container: 'httpbin'\" 从运行的日志来看，sniff 是将本地的 static-tcpdump 文件上传到 Pod 的指定容器的 /tmp 目录下，然后在容器内，通过运行以下命令来达到抓包的目的 /tmp/static-tcpdump -i any -U -w - ","date":"2023-12-18","objectID":"/kuberenetes-tcpdump/:4:3","tags":["k8s"],"title":"Kuberenetes  网络抓包  sniff","uri":"/kuberenetes-tcpdump/"},{"categories":["文档"],"content":"保存抓包文件 有时在生产环境我们可能无法直接在本地执行 kubectl，需要经过跳板机，这个时候我们可以将抓到的包保存成文件，然后再拷到本地使用 wireshark 分析。 只需加一个 -o 参数指定下保存的文件路径即可: kubectl -n test sniff website-7d7d96cdbf-6v4p6 -o test.pcap ","date":"2023-12-18","objectID":"/kuberenetes-tcpdump/:4:4","tags":["k8s"],"title":"Kuberenetes  网络抓包  sniff","uri":"/kuberenetes-tcpdump/"},{"categories":["文档"],"content":"SUNK 简介（Slurm on Kubernetes） sunk01 SUNK 是一个开源项目（将于 2024 年初发布），它将 Kubernetes 容器化部署和 GitOps 引入 Slurm，并将 Slurm 调度程序插件集成到 Kubernetes PDF 参考 本质上 SUNK 将 Slurm 集成为Kubernetes 调度程序， 并允许 Slurm 作业 在 Kubernetes 内运行。这创造了更加无缝的体验， 在同一中央平台上支持爆发式和批量工作负载，并允许开发人员利用 Kubernetes 上的 SLURM 资源管理。 单独管理 Slurm 和 Kubernetes 可以降低整体复杂性， 但也大大降低了你选择在所有计算上运行的工作负载的灵活性，也就是说，最大限度地利用 GPU 资源变得更加困难。 通过在k8s 上部署 Slurm集群， 在同一计算池上， 可以灵活从 Kubernetes 或 Slurm 端无缝使用该计算。 两种解决方案。 一个平台。 一个计算池， 一个编排层来统治他们 sunk02 sunk03 ","date":"2023-12-14","objectID":"/kuberenetes-sunk/:0:0","tags":["k8s"],"title":"Kuberenetes SUNK","uri":"/kuberenetes-sunk/"},{"categories":["文档"],"content":"为什么创建SUNK 客户效率，当你运行非常大且昂贵的HPC集群时， ，接近 100% 的利用率非常重要。任何时候，当你可能不使用你所付费的计算时，成本都可能非常高 SUNK 完全构建在 Kubernetes 之上, 每个客户端都对其集群有一个单一的入口和管理点，但是，我们意识到许多喜欢 Slurm 的客户单独管理他， 或者询问我们是否有 Sulrm 集成 SUNK 的核心是效率， 这就是专为 GPU 密集型用例而构建的原因 客户能够利用 Slurm 的优势， 同时保持系统的完整性和易用性（也称为无需管理单独的集群）。由于该解决方案不存在，我们决定构建它。 ","date":"2023-12-14","objectID":"/kuberenetes-sunk/:0:1","tags":["k8s"],"title":"Kuberenetes SUNK","uri":"/kuberenetes-sunk/"},{"categories":["文档"],"content":"SUNK 的特点 sunk04 配置和部署： 通过将 Slurm 集群部署为 各种 Kubernetes 资源，我们能够使用高度可定制的 Helm Chart 来部署它， 解锁了基于 Kubernetes 的 GitOps 工作流程的大型生态系统及其附带的所有功能， 好处包括 轻松追踪和配置prolog 和epilog 脚本 快速部署测试机器 支持s6脚本和服务 可配置的身份验证方案，包括通过配套的 OpenLdap helm chart或第三方解决方案（Authentik、GAuth 等） Kubernetes集成： 部署SUNK 后，你将获得在 Kubernetes 上运行的所有正常好处，例如： 快速调度 集装箱化 控制平面服务的高可用性 动态节点扩展 具有请求和限制的资源管理 通过 PersistentVolumeClaim 资源共享文件系统 它还包括自定义 Slurm Kubernetes 调度程序 （用于通过 Slurm 调度程序本机 Kubernetes 工作负载）， 使你能够在 Slurm 作业（突发工作负载） 和 Kubernetes 工作负载（无服务器工作负载）之间动态转移单个计算池。 状态管理： 通过在 Kubernetes 之上运行，你还可以更好地控制状态管理，包括： 在 k8s 和 Slurm 之间具有双向状态同步的动态节点 自动拓扑生成 支持 Pyxis 容器执行 GRES 支持和自动识别 ","date":"2023-12-14","objectID":"/kuberenetes-sunk/:1:0","tags":["k8s"],"title":"Kuberenetes SUNK","uri":"/kuberenetes-sunk/"},{"categories":["文档"],"content":"SUNK 的工作原理 了解SUNK 如何有效集成 Slurm和Kubernetes， 了解一下底层结构 展示了 SUNK的高层架构图 sunk05 SUNK 的工作原理以及节点集 首先要指出的是， 所有这些服务都是在 Kubernetes 中容器化。 在顶部， 你拥有集群范围内部署的资源， 在底部， 你拥有耽搁 Slurm 集群部署的组件 A部分： 所有典型的Slurm 组件都部署在一个pod内， 每个组件都有自己的可配置资源请求，包括了用户为了与 Slurm集群交互而链接的登陆节点，一旦连接到这些登陆节点， Kubernetes就会被抽象出来， 将获得正常 Slurm 集群的体验 B部分： 其中 许多组件都需要 Slurm 配置 通过将它们部署为k8s ConfigMap 和 Secret ， Slurm 配置文件可以在一个位置进行管理，并安装在 需要它们的任何地方，包括Slurm 配置、拓扑、prolog /epilog 脚本及数据库密码等敏感信息 C部分： HPC 集群最重要的方面是计算节点， 他们在中间显示为 裸机 Kubernetes 节点， slurmd 在上面以 1:1 映射现实的计算 pod中运行， 计算的部署由称为 NodeSet的CRD进行管理 D部分： 我们讨论的许多功能都要求 Slurm 和 Kubernetes 端的计算状态保持同步。 Slurm同步器充当两端的中间人。 通过Slurm 的REST API 发送和拉取信息 E部分：一旦同步器从 Slurm获取状态信息到 Kubernetes， 信息需要在许多不同的地方保持一致， 集群范围的Operator监视不同的资源并在适当的时候进行变更， 无论更改来自 Kubernetes 还是 Slurm。 由于具有同步状态的能力，我们能够使用自定义 Kubernetes 调度程序，该调度程序允许你根据 Slurm 状态将工作负载调度到计算 Kubernetes 节点上。 由于所有这些组件都是在Kuernetes 上运行， 我们可以通过 Prometheus 公开指标，这些指标可通过多种不同的方式使用。 ","date":"2023-12-14","objectID":"/kuberenetes-sunk/:2:0","tags":["k8s"],"title":"Kuberenetes SUNK","uri":"/kuberenetes-sunk/"},{"categories":["文档"],"content":"节点集、 同步器和调度器 为了使这种集成称为可能， 必须定制SUNK 的三个方面： 节点集、 同步器和调度程序 ","date":"2023-12-14","objectID":"/kuberenetes-sunk/:3:0","tags":["k8s"],"title":"Kuberenetes SUNK","uri":"/kuberenetes-sunk/"},{"categories":["文档"],"content":"节点集 sunk06 首先是 Nodeset （节点集）（如上图和第一个图表的 F 部分所示）。 这是我们开发的CRD， 它在Kubernetes 环境上下问中定义了Slurm 节点。它的定义与Statefulset或Deployment类似，但与节点的 一对一映射更类似于 Daemonset Nodeset 维护着一系列 Slurm中状态的状态字段。 着提供了基于 kubernetes 和 Slurm 中的状态来保护 Slurm 节点更新和扩展的机制 Nodeset pod 运行 slurmd 和 munged， 安装在共享配置映射中， 用于Slurm配置、prolog 和epilog，以及作为 PVC 的共享文件系统卷。 下图显示了大部分状态字段。正如你所看到的，Nodeset pod 可以主动了解有多少可能的节点与关联性匹配、其中有多少节点当前被分配为 Slurm 节点、跟踪准备情况、运行和Drain条件等 ","date":"2023-12-14","objectID":"/kuberenetes-sunk/:3:1","tags":["k8s"],"title":"Kuberenetes SUNK","uri":"/kuberenetes-sunk/"},{"categories":["文档"],"content":"同步器 Nodeset 的许多功能都以来与 Kubernetes 端了解 Slurm 的状态，反之亦然。 Syncer 通过两部分完成此任务， Slurm 客户端和pod控制器 Slurm 客户端与 Slurm REST API通信以推送和拉取信息， 随着Slurm 集群的规模增长到数百甚至千个节点。 这种通信将增长到大量流量。 客户端有效地缓存结构以处理大规模集群 Pod 控制器接收来自客户端的事件， 并根据任何差异来协调 pod状态， 如果更改来自 Kubernetes 端。pod 控制器会将事件推送到客户端， 客户端会根据需要将该更改传递给 Slurm。 因此，信息流有两个方向 举例来说， Slurm 中的一个节点进入Drain状态， Syncer 将监测状态更改并在pod 上添加注释，此注释可以在 Kubernetes 端进行操作，例如仅当 Slurm 节点处于Drain状态时才开始更新。 另一方面，改变可以从 Kubernetes 端开始。假设你检测到硬件问题并封锁 Kubernetes 内部的一个节点。Syncer 会将相应的 Slurm 节点放入Drain中，并添加一个很好的理由来解释为什么它被驱逐。 sunk07 ","date":"2023-12-14","objectID":"/kuberenetes-sunk/:3:2","tags":["k8s"],"title":"Kuberenetes SUNK","uri":"/kuberenetes-sunk/"},{"categories":["文档"],"content":"调度程序 调度程序 是在命名空间中运行的另一个服务， 它允许一些非常有趣的功能。 假设你想要在 SUNK上获取一个预留实例池，并确保除了从按需池中使用的实例之外，你还能获得这些实例的最大利用率。又名：同时运行推理和训练。 当在 k8s 中使用此 Scheduler 调度 pod 时，就像由 Knative 等无服务器解决方案驱动的推理 pod 一样，会创建一个 Slurm 作业，该作业能够抢占低优先级任务。当 Slurm 节点最终分配给 Slurm 中的作业时，调度程序会将 k8s pod 放置到该节点上并在那里运行 简而言之，你可以无缝地使用 Kubernetes 中 Slurm 集群的计算，而无需主动维护两个独立的预留计算池（这些池不是动态分配的）。 sunk08 ","date":"2023-12-14","objectID":"/kuberenetes-sunk/:3:3","tags":["k8s"],"title":"Kuberenetes SUNK","uri":"/kuberenetes-sunk/"},{"categories":["文档"],"content":"参考链接 Kubernetes 遇见高性能计算：https://kubernetes.io/blog/2017/08/kubernetes-meets-high-performance/ SUNK 是 Kubernetes 上 Slurm 的实现：https://www.coreweave.com/blog/sunk-slurm-on-kubernetes-implementations? ","date":"2023-12-14","objectID":"/kuberenetes-sunk/:4:0","tags":["k8s"],"title":"Kuberenetes SUNK","uri":"/kuberenetes-sunk/"},{"categories":["文档"],"content":"用Telepresence 本地调试kubernetes中的微服务 github地址：telepresence 功能介绍： Telepresence01 快速的本地开发循环，无需等待容器构建/推送/部署 能够使用他们最喜欢的本地工具（IDE、调试器等） 能够运行无法在本地运行的大型应用程序 telepresence 主要是解决 Kubernetes 集群下开发微服务时遇到的日常问题， 解决本地和集群内服务集成联调的问题。 ","date":"2023-12-14","objectID":"/kuberenetes-telepersence/:0:0","tags":["k8s"],"title":"Kuberenetes Telepersence","uri":"/kuberenetes-telepersence/"},{"categories":["文档"],"content":"微服务开发的痛点 一个平台如果微服务化后，至少也是几十个服务起步了吧，这还是在不包含基础设施服务的前提下。 接到一个需求，在某个服务下增加一个功能或者是要修复一个 bug，那么你接下来准备怎么做呢？ 打算将几十个服务都在本机跑起来吗？且不说你不知道平台到底有多少个服务，光是想想配置这一整套环境都头大，再说你的机器是否有足够的资源能撑的起平台的运行，所以说 pure offline 的方式肯定不是一个可行的方向。 ","date":"2023-12-14","objectID":"/kuberenetes-telepersence/:1:0","tags":["k8s"],"title":"Kuberenetes Telepersence","uri":"/kuberenetes-telepersence/"},{"categories":["文档"],"content":"方法一： 利用端口转发 利用 Kubernetes 原生的特性 port-forward，将远端集群的两个服务，转发到本地，通过 localhost 的方式来访问，完成了服务的验证 Port forwarding k8s.Service.1 ➜ kubectl port-forward svc/k8s-service-1 22222 Forwarding from 127.0.0.1:22222 -\u003e 22222 Forwarding from [::1]:22222 -\u003e 22222 Port forwarding k8s.Service.2 kubectl port-forward svc/k8s-service-2 22223 Forwarding from 127.0.0.1:22223 -\u003e 22223 Forwarding from [::1]:22223 -\u003e 22223 这个方法存在问题： 开发者需要明确清楚依赖的目标服务 开发者需要清楚目标服务在 Kubernetes 集群內的 Pod Name 或者 Service Name 以及端口号 如果依赖的服务增多，每个服务都要手动或写脚本做端口转发，非常的复杂且不方便 本地配置和部署配置不一致（本地使用 localhost，线上使用 svc） 端口转发因为网络问题容易中断 ","date":"2023-12-14","objectID":"/kuberenetes-telepersence/:2:0","tags":["k8s"],"title":"Kuberenetes Telepersence","uri":"/kuberenetes-telepersence/"},{"categories":["文档"],"content":"方法二： 利用 kubefwd 工具 Kubefwd，它是一个用于端口转发 Kubernetes 中指定 namespace 下全部或部分服务的命令行工具。 使用 Kubefwd 后，本地环境也能使用 Kubernetes 集群內 svc + port 方式直接访问 一个端口转发的工具kubewd Telepresence02 Examples: kubefwd services --help kubefwd svc -n the-project kubefwd svc -n the-project -l env=dev,component=api kubefwd svc -n the-project -f metadata.name=service-name kubefwd svc -n default -l \"app in (ws, api)\" kubefwd svc -n default -n the-project kubefwd svc -n the-project -m 80:8080 -m 443:1443 kubefwd svc -n the-project -z path/to/conf.yml kubefwd svc -n the-project -r svc.ns:127.3.3.1 kubefwd svc --all-namespaces Kubefwd将转发所有headlesss Service的Pod sudo kubefwd svc -n the-project 转发namespace the-project下所有的带有label为system: wx的service： sudo kubefwd svc -l system=wx -n the-project 这种方法虽然方便，但是存在以下问题 跨多个 namespace 调试不方便 单向调用（只能本地服务访问远程集群中的其他服务） ","date":"2023-12-14","objectID":"/kuberenetes-telepersence/:3:0","tags":["k8s"],"title":"Kuberenetes Telepersence","uri":"/kuberenetes-telepersence/"},{"categories":["文档"],"content":"开发现状 ","date":"2023-12-14","objectID":"/kuberenetes-telepersence/:4:0","tags":["k8s"],"title":"Kuberenetes Telepersence","uri":"/kuberenetes-telepersence/"},{"categories":["文档"],"content":"Local + Remote 利用 Kubernetes 原生的特性port-forward 命令或者 kubefwd 工具，将 Kubernetes 集群中的服务转发到本地，然后通过 local 的方式访问 Kubernetes 集群中的服务，这种方式可以解决开发过程中的一些联调问题。 需要转发的服务配置不可控，且取决于团队成员是否都有 Kubernetes 基础，是否对平台的微服务架构都比较了解。 不稳定，时不时需要重新做服务映射，特别是在远程用 VPN 连入公司内网情况下 单向调用，只能从本地向集群发请求，集群内的流量请求无法劫持转发到本地 ","date":"2023-12-14","objectID":"/kuberenetes-telepersence/:4:1","tags":["k8s"],"title":"Kuberenetes Telepersence","uri":"/kuberenetes-telepersence/"},{"categories":["文档"],"content":"Remote 利用内网自动化流水线的的方式，将代码发布到内网测试环境，即将服务持续的部署到内网 Kubernetes 测试环境中进行联调测试，以便提前发现问题。 虽然该过程可以自动完成，但是每次调试至少需要经历以下步骤（Git commit -\u003e docker build -\u003e deploy to k8s cluster），耗时比较长，效率低下。 一般每个团队都是共享内网开发测试环境的，这种 Remote Debug 方式会影响到团队内的其他成员的开发进度 ","date":"2023-12-14","objectID":"/kuberenetes-telepersence/:4:2","tags":["k8s"],"title":"Kuberenetes Telepersence","uri":"/kuberenetes-telepersence/"},{"categories":["文档"],"content":"Telepresence Telepresence 对基于 Kubernetes 的开发者来说是种非常强大的调试工具 Telepresence 是一个面向Kubernetes用户的开发测试环境治理的辅助工具，用于本地轻松开发和调试服务， 同时将服务代理到远程 Kubernetes 集群，无需等待容器做 build/push/deploy 使用 Telepresence 开发者可以使用本地熟悉的 IDE 和调试工具运行一个服务，并提供对 Configmap、Secrets 和远程集群上运行的服务的完全访问，无缝与 Kubernetes 集群中的其他服务进行联调，让微服务本地开发不再难。 可以在不用修改代码的情况下，让本地应用程序无感知接入到 Kubernetes 集群中，简单来说就是可直接使用集群内的 PodIP， ClusterIP 以及 DNS 域名来访问集群中的其他服务。 因为 Telepresence 在 kubernetes 集群中运行的Pod 中部署了 双向网络代理，所以不再是单向代理，本地服务可以完全访问远程集群中的其他服务，同时远程集群中运行的服务也可以完全访问本地服务 ","date":"2023-12-14","objectID":"/kuberenetes-telepersence/:5:0","tags":["k8s"],"title":"Kuberenetes Telepersence","uri":"/kuberenetes-telepersence/"},{"categories":["文档"],"content":"安装方法 参考链接telepresence # Intel Macs # 1. Download the latest binary (~105 MB): sudo curl -fL https://app.getambassador.io/download/tel2oss/releases/download/v2.16.1/telepresence-darwin-amd64 -o /usr/local/bin/telepresence # 2. Make the binary executable: sudo chmod a+x /usr/local/bin/telepresence # Apple silicon Macs # 1. Download the latest binary (~101 MB): sudo curl -fL https://app.getambassador.io/download/tel2oss/releases/download/v2.16.1/telepresence-darwin-arm64 -o /usr/local/bin/telepresence # 2. Make the binary executable: sudo chmod a+x /usr/local/bin/telepresence ","date":"2023-12-14","objectID":"/kuberenetes-telepersence/:5:1","tags":["k8s"],"title":"Kuberenetes Telepersence","uri":"/kuberenetes-telepersence/"},{"categories":["文档"],"content":"本地环境 当执行完 telepresence connect ， 可以想象成你的本地环境就是 Kubernetes 集群中的一个 pod， 能够无感知接入到 Kubernetes 集群， Telepresence 让你的本地环境成为 集群中的一部分 telepresence connect 开发者每次修改代码后，重新编译，推送镜像，部署到 Kubernetes 集群，然后调试，如果出现错误，再重复这样的步骤，这个是以往大部分开发者的日常行为。 大家也知道，下图中右侧虚框中的部分因为各种外部原因，常常是不可控的 Telepresence03 Telepresence 是复杂的微服务架构原本只能从这种Remote 的验证方式得到解放 调整为开发和联调测试行为都发生在本地，这样对开发者的效率明显提升很多。 Telepresence04 安装 Telepresence(macOS) brew install datawire/blackbird/telepresence ","date":"2023-12-14","objectID":"/kuberenetes-telepersence/:5:2","tags":["k8s"],"title":"Kuberenetes Telepersence","uri":"/kuberenetes-telepersence/"},{"categories":["文档"],"content":"案例 telepresence 默认会使用当前的 kubectl 的 current context 来进行请求。 一、在 Kubernetes 集群运行一个 hello-world 服务 kubectl run hello-world \\ --image=datawire/hello-world \\ --port=8000 \\ --expose 二、 在本地启动 Telepresence，建立到集群的连接 # 将本地环境连接到远程 kubernetes 集群 ➜ telepresence connect Launching Telepresence Root Daemon Launching Telepresence User Daemon Connected to context development-private@xdp-bee (https://\u003cclusterip\u003e:6443) 三、查看连接状态 telepresence status User Daemon: Running Version : v2.17.1 Executable : /opt/homebrew/bin/telepresence Install ID : 834d6f44-05a1-4b63-82ff-8de25208e3ce Status : Connected Kubernetes server : https://192.168.0.65:6443 Kubernetes context: k8s-65 Connection name : k8s-65-default Namespace : default Manager namespace : ambassador Intercepts : 0 total Root Daemon: Running Version : v2.17.1 Version : v2.17.1 DNS : Remote IP : 127.0.0.1 Exclude suffixes: [.com .io .net .org .ru] Include suffixes: [] Timeout : 8s Also Proxy : (0 subnets) Never Proxy: (1 subnets) - 192.168.0.65/32 Ambassador Cloud: Running Status : Logged in User ID : 8b474552-01e1-4c7c-8163-6c48141ffee1 Account ID : 6987f3d9-9f4e-4569-8045-1d563ef73f57 User Name : hongfeng liu Email : fxqi1221@gmail.com Account Name: hongfeng Plan : Free Trial Traffic Manager: Connected Version : v2.17.1 Intercept Spec: 四、验证直接连接 hello-world 服务 curl http://hello-world.default:8000 Hello, world! 用浏览器访问该域名 Telepresence05 Telepresence v2 通过 Kubernetes service 的 服务名称 + 命名空间 + 端口 访问服务 五、 退出守护进程 telepresence quit Telepresence Root Daemon quitting... done Telepresence User Daemon is already stopped 参考 https://mp.weixin.qq.com/s/583aqGHcjYkBLStWrxiknA https://github.com/lqshow/telepresence-labs https://imti.co/kubernetes-port-forwarding/ https://hackernoon.com/locally-developing-kubernetes-services-without-waiting-for-a-deploy-f63995de7b99 ","date":"2023-12-14","objectID":"/kuberenetes-telepersence/:6:0","tags":["k8s"],"title":"Kuberenetes Telepersence","uri":"/kuberenetes-telepersence/"},{"categories":["文档"],"content":"Kubernetes 容器设计模式之边车模式 github地址：sidecar loki01 sidecar 边车模式 简单来说就是加装在摩托车旁边，用来拓展现有功能的能力，可以坐上更多的人或者物， 边车模式 像软件工程的代理模式，对服务进行包装， 使其不改变原来的功能，拓展原来的服务 sidecar 模式 是用来解决微服务的服务治理问题 ","date":"2023-12-14","objectID":"/kuberenetes-sidecar/:0:0","tags":["k8s"],"title":"Kuberenetes Sidecar模式","uri":"/kuberenetes-sidecar/"},{"categories":["文档"],"content":"介绍 日志收集 和 服务健康检查 这两个附加能力，来举例说明在不同时期 Sidecar 模式的实现方式 ","date":"2023-12-14","objectID":"/kuberenetes-sidecar/:1:0","tags":["k8s"],"title":"Kuberenetes Sidecar模式","uri":"/kuberenetes-sidecar/"},{"categories":["文档"],"content":"Docker loki01 这种容器的设计模式是不正确的， 它将所有的Sidecar 的能力和主应用程序打包在了一起， 变成了一个富容器，容器应该是解决某个问题的功能单元，让容器保持单一用途才是正确的方式 第一阶段 所有微服务治理的相关功能比如 限流熔断、流量控制 、 服务限流 等等，和应用程序紧耦合在一起， 业务逻辑混合了各式各样非业务功能的代码 第二阶段 一个是为了简化开发将这些非业务功能性代码做了剥离，将提供的能力代码拆分为独立的类库，有部分采用了开源的成熟框架，使其和业务代码做集成。 另外一件事是重构，将部分服务统一往 Golang 上做迁移 ","date":"2023-12-14","objectID":"/kuberenetes-sidecar/:1:1","tags":["k8s"],"title":"Kuberenetes Sidecar模式","uri":"/kuberenetes-sidecar/"},{"categories":["文档"],"content":"Kubernetes loki01 kubernetes的pod在容器的基础上，做了更高一层抽象，一个pod内可以有多个容器存在， 他们共享了同一个 Network Namespace， 并且可以声明共享一个Volume 左边是主应用程序所在的容器，右边两个是 服务治理 相关的辅助容器，也就是我们本篇文章的主题，可以称之为 Sidecar 容器 可以在 pod 中启动一个或多个辅助容器， 来完成一些独立于主容器之外的工作，这种模式的好处是显而易见的，每个容器都有它自己单一的用途，真正做到了职责分离，可以说是基本上解决了上述 容器时代架构 遗留下的大部分问题 ","date":"2023-12-14","objectID":"/kuberenetes-sidecar/:1:2","tags":["k8s"],"title":"Kuberenetes Sidecar模式","uri":"/kuberenetes-sidecar/"},{"categories":["文档"],"content":"Sidecar Pattern 本质上 就是 主应用和附加能力的应用他妈拥有共同的生命周期 Sidecar Pattern 可以说是现代云计算非常重要的设计模式，通过指责分离与容器的隔离特性， 能够降低容器的复杂度，同时能扩展并增强已有容器的功能。 ","date":"2023-12-14","objectID":"/kuberenetes-sidecar/:2:0","tags":["k8s"],"title":"Kuberenetes Sidecar模式","uri":"/kuberenetes-sidecar/"},{"categories":["文档"],"content":"使用方式 loki04 Pod 的 YAML 结构 cat \u003c\u003cEOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: nginx-example spec: containers: - name: application-container image: nginx:1.15.2 imagePullPolicy: Always ports: - containerPort: 80 volumeMounts: - name: shared-files mountPath: /usr/share/nginx/html - name: sidecar-container image: lqshow/busybox-curl:1.28 # 你可以想象成你的前端应用的静态文件全部打包在 /project/dist 目录下 command: ['/bin/sh', '-c', \"echo 'Hello, World!' \u003e /project/dist/index.html \u0026\u0026 sleep 3600\"] volumeMounts: - name: shared-files mountPath: /project/dist workingDir: /project/dist volumes: # 通过同一个卷来共享数据，用来共享 Sidecar 静态文件 - name: shared-files emptyDir: {} EOF 将脚本放在 Kubernetes 集群內执行，并通过 port-forward 来验证结果。 kubectl port-forward pod/nginx-example 3000:80 Forwarding from 127.0.0.1:3000 -\u003e 80 Forwarding from [::1]:3000 -\u003e 80 curl localhost:3000 Hello, World! ","date":"2023-12-14","objectID":"/kuberenetes-sidecar/:3:0","tags":["k8s"],"title":"Kuberenetes Sidecar模式","uri":"/kuberenetes-sidecar/"},{"categories":["文档"],"content":"Service Mesh Sidecar Pattern 的使用越来越普遍，尤其是在 Service Mesh 领域， 它可以说是将 Sidecar Pattern 玩出了极致，且非常符合云原生的理念 Service Mesh 重新定义了微服务治理。 loki05 ","date":"2023-12-14","objectID":"/kuberenetes-sidecar/:4:0","tags":["k8s"],"title":"Kuberenetes Sidecar模式","uri":"/kuberenetes-sidecar/"},{"categories":["文档"],"content":"kubernetes 常用运维 kubectl命令 ","date":"2023-12-12","objectID":"/kuberenetes-kubectl/:0:0","tags":["k8s"],"title":"Kuberenetes Kubectl","uri":"/kuberenetes-kubectl/"},{"categories":["文档"],"content":"环境上下文切换 ","date":"2023-12-12","objectID":"/kuberenetes-kubectl/:1:0","tags":["k8s"],"title":"Kuberenetes Kubectl","uri":"/kuberenetes-kubectl/"},{"categories":["文档"],"content":"切换集群 # 获取所有的 contexts kubectl config get-contexts # 切换到指定的 context kubectl config use-context \u003cCONTEXT-NAME\u003e ","date":"2023-12-12","objectID":"/kuberenetes-kubectl/:1:1","tags":["k8s"],"title":"Kuberenetes Kubectl","uri":"/kuberenetes-kubectl/"},{"categories":["文档"],"content":"设置默认的namespace # 将当前上下文设置为指定的 namespace kubectl config set-context --current --namespace=\u003cMY_NAMESPACE\u003e # 验证当前上下文是否是正确设置的 namespace kubectl config view --minify | grep namespace 平时用的最顺手的，还数 kubectx + kubens 的组合，但是大部分客户的环境并没有预装该工具，还是需要记录下原生命令 kubectx ","date":"2023-12-12","objectID":"/kuberenetes-kubectl/:1:2","tags":["k8s"],"title":"Kuberenetes Kubectl","uri":"/kuberenetes-kubectl/"},{"categories":["文档"],"content":"列出 non-running Pods 当前集群内有哪些pod处于异常状态， 帮助运维工程师快速定位系统是否有异常 kubectl get pods -A --no-headers | grep -v Running | grep -v Completed ","date":"2023-12-12","objectID":"/kuberenetes-kubectl/:2:0","tags":["k8s"],"title":"Kuberenetes Kubectl","uri":"/kuberenetes-kubectl/"},{"categories":["文档"],"content":"资源排序 以 pod资源作为例子， 提供常用的排序方式 # 根据启动时间降序（descending order） kubectl get pods --sort-by=.metadata.creationTimestamp # 根据启动时间升序（ascending order） kubectl get pods --sort-by=.metadata.creationTimestamp | awk 'NR == 1; NR \u003e 1 {print $0 | \"tac\"}' kubectl get pods --sort-by=.metadata.creationTimestamp | tail -n +2 | tac kubectl get pods --sort-by={metadata.creationTimestamp} --no-headers | tac kubectl get pods --sort-by=.metadata.creationTimestamp | tail -n +2 | tail -r # 根据重启次数排序 kubectl get pods -A --sort-by='.status.containerStatuses[0].restartCount' ","date":"2023-12-12","objectID":"/kuberenetes-kubectl/:3:0","tags":["k8s"],"title":"Kuberenetes Kubectl","uri":"/kuberenetes-kubectl/"},{"categories":["文档"],"content":"导出干净的 YAML kubectl get cm nginx-config -o yaml | kubectl neat -o yaml ","date":"2023-12-12","objectID":"/kuberenetes-kubectl/:4:0","tags":["k8s"],"title":"Kuberenetes Kubectl","uri":"/kuberenetes-kubectl/"},{"categories":["文档"],"content":"释放集群内资源 临时将某个namespace下的Pod关闭 如果你需要释放某个 namespace 下的资源，但是又不想删除 Kubernetes 集群內的信息，可通过调整 replicas 来解决 # 方法一：通过 patch 模式 kubectl get deploy -o name -n \u003cNAMESPACE\u003e|xargs -I{} kubectl patch {} -p '{\"spec\":{\"replicas\":0}}' # 方法二：通过资源伸缩副本数 kubectl get deploy -o name |xargs -I{} kubectl scale --replicas=0 {} ","date":"2023-12-12","objectID":"/kuberenetes-kubectl/:5:0","tags":["k8s"],"title":"Kuberenetes Kubectl","uri":"/kuberenetes-kubectl/"},{"categories":["文档"],"content":"临时关闭 Daemonsets 如果需要临时将 Daemonsets 关闭，只需要将其调度到一个不存在的 node 上即可，调整下 nodeSelector kubectl patch daemonsets nginx-ingress-controller -p '{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":{\"project/xdp\":\"none\"}}}}}' ","date":"2023-12-12","objectID":"/kuberenetes-kubectl/:5:1","tags":["k8s"],"title":"Kuberenetes Kubectl","uri":"/kuberenetes-kubectl/"},{"categories":["文档"],"content":"批量删除某种状态下资源 批量强制删除 当前 namespace 下所有处于 Terminating 状态的 Pod 资源 kubectl get pod |grep Terminating|awk '{print $1}'|xargs kubectl delete pod --grace-period=0 --force 批量强制删除 集群內所有 namespace 下所有处于 Terminating 状态的 Pod 资源 for ns in $(kubectl get ns --no-headers | cut -d ' ' -f1); do \\ for po in $(kubectl -n $ns get po --no-headers --ignore-not-found | grep Terminating | cut -d ' ' -f1); do \\ kubectl -n $ns delete po $po --force --grace-period 0; \\ done; \\ done; ","date":"2023-12-12","objectID":"/kuberenetes-kubectl/:6:0","tags":["k8s"],"title":"Kuberenetes Kubectl","uri":"/kuberenetes-kubectl/"},{"categories":["文档"],"content":"资源复制 如果当前 Namespace 下的某个资源，其他 Namespace 下也需要，可参考以下方法，以复制 secret 举例。 kubectl get secret \u003cSECRET-NAME\u003e -n \u003cSOURCE-NAMESPACE\u003e -oyaml | sed \"/namespace:/d\" | kubectl apply --namespace=\u003cTARGET-NAMESPACE\u003e -f - ","date":"2023-12-12","objectID":"/kuberenetes-kubectl/:7:0","tags":["k8s"],"title":"Kuberenetes Kubectl","uri":"/kuberenetes-kubectl/"},{"categories":["文档"],"content":"使用交互 shell 访问匹配到 标签的 Pod # example 1 kubectl exec -i -t $(kubectl get pod -l \u003cKEY\u003e=\u003cVALUE\u003e -o name |sed 's/pods\\///') -- bash # example 2 kubectl exec -i -t $(kubectl get pod -l \u003cKEY\u003e=\u003cVALUE\u003e -o jsonpath='{.items[0].metadata.name}') -- bash ","date":"2023-12-12","objectID":"/kuberenetes-kubectl/:8:0","tags":["k8s"],"title":"Kuberenetes Kubectl","uri":"/kuberenetes-kubectl/"},{"categories":["文档"],"content":"在多个 Pod 中运行命令 kubectl get pods -o name | xargs -I{} kubectl exec {} -- \u003ccommand goes here\u003e ","date":"2023-12-12","objectID":"/kuberenetes-kubectl/:9:0","tags":["k8s"],"title":"Kuberenetes Kubectl","uri":"/kuberenetes-kubectl/"},{"categories":["文档"],"content":"查看所有镜像 kubectl get pods -o custom-columns='NAME:metadata.name,IMAGES:spec.containers[*].image' ","date":"2023-12-12","objectID":"/kuberenetes-kubectl/:10:0","tags":["k8s"],"title":"Kuberenetes Kubectl","uri":"/kuberenetes-kubectl/"},{"categories":["文档"],"content":"设置环境变量 kubectl set env deploy \u003cDEPLOYMENT_NAME\u003e OC_XXX_HOST=bbb ","date":"2023-12-12","objectID":"/kuberenetes-kubectl/:11:0","tags":["k8s"],"title":"Kuberenetes Kubectl","uri":"/kuberenetes-kubectl/"},{"categories":["文档"],"content":"充重置集群节点 # 1. 将节点标记为不可调度，确保新的容器不会调度到该节点 kubectl cordon \u003cNODE-NAME\u003e # 2. Master 节点上将需要重置的节点驱逐, 除了 deemonset kubectl drain \u003cNODE-NAME\u003e --delete-local-data --force --ignore-daemonsets # 3. 删除节点 kubectl delete node \u003cNODE-NAME\u003e # 4. 在需要重置节点上执⾏重置脚本，注意，如果在 Master 主节点执⾏ kubeadm reset，则需要重新初始化集群 kubeadm reset ","date":"2023-12-12","objectID":"/kuberenetes-kubectl/:12:0","tags":["k8s"],"title":"Kuberenetes Kubectl","uri":"/kuberenetes-kubectl/"},{"categories":["文档"],"content":"Forward local port to a pod or service 为 pod 创建本地端口映射 # 将 localhost:3000 的请求转发到 nginx-pod Pod 的 80 端口 kubectl port-forward nginx-po 3000:80 为 service 创建本地端口映射 # 将 localhost:3201 的请求转发到 nginx-web service 的 3201 端口 kubectl port-forward svc/nginx-web 3201 一个端口转发的工具kubewd ","date":"2023-12-12","objectID":"/kuberenetes-kubectl/:13:0","tags":["k8s"],"title":"Kuberenetes Kubectl","uri":"/kuberenetes-kubectl/"},{"categories":["文档"],"content":"创建一个临时可调试的Pod 退出 shell，可自动删除 Pod kubectl run ephemeral-busybox \\ --rm \\ --stdin \\ --tty \\ --restart=Never \\ --image=lqshow/busybox-curl:1.28 \\ -- sh ","date":"2023-12-12","objectID":"/kuberenetes-kubectl/:14:0","tags":["k8s"],"title":"Kuberenetes Kubectl","uri":"/kuberenetes-kubectl/"},{"categories":["文档"],"content":"故障排除 # 1. 查看资源详情 kubectl describe # 2. 查看 Pod 内容器的日志 kubectl logs # 3. 在指定容器内执行命令 kubectl exec # 4. 查看节点列表明细 kubectl get nodes --show-labels # 5. 查看所有事件 kubectl get events 参考 https://kubernetes.io/zh/docs/reference/kubectl/cheatsheet/ https://ahmet.im/blog/mastering-kubeconfig/ ","date":"2023-12-12","objectID":"/kuberenetes-kubectl/:15:0","tags":["k8s"],"title":"Kuberenetes Kubectl","uri":"/kuberenetes-kubectl/"},{"categories":["文档"],"content":"日志聚合分析系统-Loki loki 是Grafana的开源项目，是一个水平可扩展，高可用性，多租户的日志聚合系统， 为每个日志流编制一组标签，专门为 Prometheus 和 Kubernetes 用户做了相关优化 ","date":"2023-12-09","objectID":"/kuberenetes-loki/:0:0","tags":["k8s"],"title":"Kubernetes Loki","uri":"/kuberenetes-loki/"},{"categories":["文档"],"content":"Loki的优势 不对日志进行全文索引，通过压缩非结构化日志和仅索引元数据 通过与Prometheus 相同的标签记录对日志进行索引和分组， 适合存储 Kubernetes Pod 日志，比如pod 标签之类的元数据会被自动删除和编入索引 受 Grafana 原生支持，与 Prometheus 配合更加方便 ","date":"2023-12-09","objectID":"/kuberenetes-loki/:1:0","tags":["k8s"],"title":"Kubernetes Loki","uri":"/kuberenetes-loki/"},{"categories":["文档"],"content":"整体架构 loki01 Loki 的组成部分 Loki 为主服务器， 负责存储日志和处理查询 Prometail 是代理， 负责手机日志并将其发送给Loki Grafana 用来UI展示。 Loki使用与Prometheus 相同的服务发现和标签重新标记库。编写了 Promtail。在 Kubernetes 中 Promtail 以 DaemonSet 方式运行在每个节点中，通过 Kubernetes API 得到日志的正确元数据，并将它们发送到Loki，如下图： loki02 Loki种主要组件有 DIstributor， Ingester和Querier 负责写入的组件有Distributor和Ingester两个 loki03 ","date":"2023-12-09","objectID":"/kuberenetes-loki/:2:0","tags":["k8s"],"title":"Kubernetes Loki","uri":"/kuberenetes-loki/"},{"categories":["文档"],"content":"Distributor Promtaif 一旦将日志发给Loki， DIstributor就是第一个接收日志的组件。 由于 日志的写入量可能很大，所以不能再它传入时并行写入数据库， 要先进行批处理和压缩数据。 Distributor 接收到HTTP请求，用于存储流数据 通过 hash 环对数据流进行hash 通过 hash 算法计算出应该发送到哪个Ingester后， 发送数据流 Ingester新建Chunks或将数据追加到已有的Chunk上 loki04 ","date":"2023-12-09","objectID":"/kuberenetes-loki/:2:1","tags":["k8s"],"title":"Kubernetes Loki","uri":"/kuberenetes-loki/"},{"categories":["文档"],"content":"Ingester ingester 接收到日志开始构建 Chunk loki05 ingester 是一个有状态组件， 负责构建和刷新Chunk，当 Chunk达到一定数量或者时间后，刷新到存储中去， 每个流日志对应一个Ingester。index 和Chunk 各自使用单独的数据库， 因为他们存储额数据类型不同。 ","date":"2023-12-09","objectID":"/kuberenetes-loki/:2:2","tags":["k8s"],"title":"Kubernetes Loki","uri":"/kuberenetes-loki/"},{"categories":["文档"],"content":"负责读的组件则是Querier： loki06 读取就 比较简单， 由Querier 负责 给定一个时间和标签选择器，也就是收到读请求： Querier 收到HTTP读请求 Querier 将请求发送至Ingester读取还未写入Chunks的内存数据 随后再去index+chunks中查找数据 Querier 遍历所有数据并进行去重处理，再返回最终结果 ","date":"2023-12-09","objectID":"/kuberenetes-loki/:2:3","tags":["k8s"],"title":"Kubernetes Loki","uri":"/kuberenetes-loki/"},{"categories":["文档"],"content":"搭建使用 上边主要介绍的Loki的工作流程及组件，下面我们实际搭建操作下： Loki项目地址：https://github.com/grafana/loki/ 官网：https://grafana.com/oss/loki/ ","date":"2023-12-09","objectID":"/kuberenetes-loki/:3:0","tags":["k8s"],"title":"Kubernetes Loki","uri":"/kuberenetes-loki/"},{"categories":["文档"],"content":"一、通过Helm部署： ## 添加chart helm repo add loki https://grafana.github.io/loki/charts ## 更新chart helm repo update ## 将loki template下载到本地 helm fetch loki/loki-stack ## 解压并自定义修改参数 tar zxvf loki-stack-2.0.2.tgz cd loki-stack/ $$ ls charts Chart.yaml README.md requirements.lock requirements.yaml templates values.yaml cd charts/ $$ ls filebeat fluent-bit grafana logstash loki prometheus promtail ","date":"2023-12-09","objectID":"/kuberenetes-loki/:3:1","tags":["k8s"],"title":"Kubernetes Loki","uri":"/kuberenetes-loki/"},{"categories":["文档"],"content":"开始helm安装前需要注意几个点 1、可以修改values.yaml文件，指定是否开启Grafana、Prometheus等服务，默认不开启的： loki: enabled: true promtail: enabled: true fluent-bit: enabled: false grafana: enabled: true sidecar: datasources: enabled: true image: tag: 6.7.0 prometheus: enabled: false 修改Grafana的values.yaml，使其Service暴露方式为NodePort(默认为ClusterIp)： vim charts/grafana/values.yaml service: type: NodePort port: 80 nodePort: 30002 # 端口范围：30000-32767 targetPort: 3000 # targetPort: 4181 To be used with a proxy extraContainer annotations: {} labels: {} portName: service 还有一处账号密码可以自定义修改下： # Administrator credentials when not using an existing secret (see below) adminUser: admin adminPassword: admin ","date":"2023-12-09","objectID":"/kuberenetes-loki/:4:0","tags":["k8s"],"title":"Kubernetes Loki","uri":"/kuberenetes-loki/"},{"categories":["文档"],"content":"promtail服务在构建时会自动挂载 宿主机docker 主目录下的 container目录，一般默认都为/var/lib/docker/containers pod的日志目录， 一般默认在/var/log/pods 这就需要特别注意一下，如果是修改过docker默认的存储路径的，需要将mount的路径进行修改，promtail找不到对应的容器日志 具体docker 存储路径，可以使用docker info 命令查询 vim charts/promtail/values.yaml # @default -- See `values.yaml` defaultVolumes: - name: run hostPath: path: /run/promtail - name: containers hostPath: path: /var/lib/containers - name: pods hostPath: path: /var/log/pods # -- Default volume mounts. Corresponds to `volumes`. # @default -- See `values.yaml` defaultVolumeMounts: - name: run mountPath: /run/promtail - name: containers mountPath: /var/lib/containers readOnly: true - name: pods mountPath: /var/log/pods readOnly: true ","date":"2023-12-09","objectID":"/kuberenetes-loki/:5:0","tags":["k8s"],"title":"Kubernetes Loki","uri":"/kuberenetes-loki/"},{"categories":["文档"],"content":"开始安装 volumes: - name: docker hostPath: path: /data/lib/docker/containers ## 我的是放在了data下 - name: pods hostPath: path: /var/log/pods volumeMounts: - name: docker mountPath: /data/lib/docker/containers ## 挂载点也要进行修改 readOnly: true - name: pods mountPath: /var/log/pods readOnly: true 开始安装： helm install -n loki --namespace loki -f values.yaml ../loki-stack 2020/11/11 17:18:54 Warning: Merging destination map for chart 'logstash'. The destination item 'filters' is a table and ignoring the source 'filters' as it has a non-table value of: \u003cnil\u003e NAME: loki LAST DEPLOYED: Wed Nov 11 17:18:53 2020 NAMESPACE: loki STATUS: DEPLOYED RESOURCES: ==\u003e v1/ClusterRole NAME AGE loki-promtail-clusterrole 1s loki-grafana-clusterrole 1s ==\u003e v1/ClusterRoleBinding NAME AGE loki-promtail-clusterrolebinding 1s loki-grafana-clusterrolebinding 1s ==\u003e v1/ConfigMap NAME DATA AGE loki-grafana 1 1s loki-grafana-test 1 1s loki-loki-stack 1 1s loki-loki-stack-test 1 1s loki-promtail 1 1s ==\u003e v1/DaemonSet NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE loki-promtail 2 2 0 2 0 \u003cnone\u003e 1s ==\u003e v1/Deployment NAME READY UP-TO-DATE AVAILABLE AGE loki-grafana 0/1 1 0 1s ==\u003e v1/Pod(related) NAME READY STATUS RESTARTS AGE loki-0 0/1 ContainerCreating 0 2s loki-grafana-56bf5d8d-8zcgp 0/1 Init:0/1 0 2s loki-promtail-6r24r 0/1 ContainerCreating 0 2s loki-promtail-fvnfc 0/1 ContainerCreating 0 2s ==\u003e v1/Role NAME AGE loki-promtail 1s loki-grafana-test 1s loki 1s ==\u003e v1/RoleBinding NAME AGE loki-promtail 1s loki-grafana-test 1s loki 1s ==\u003e v1/Secret NAME TYPE DATA AGE loki Opaque 1 1s loki-grafana Opaque 3 1s ==\u003e v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE loki ClusterIP 10.109.216.219 \u003cnone\u003e 3100/TCP 1s loki-grafana NodePort 10.100.203.138 \u003cnone\u003e 80:30002/TCP 1s loki-headless ClusterIP None \u003cnone\u003e 3100/TCP 1s ==\u003e v1/ServiceAccount NAME SECRETS AGE loki 1 1s loki-grafana 1 1s loki-grafana-test 1 1s loki-promtail 1 1s ==\u003e v1/StatefulSet NAME READY AGE loki 0/1 1s ==\u003e v1beta1/PodSecurityPolicy NAME PRIV CAPS SELINUX RUNASUSER FSGROUP SUPGROUP READONLYROOTFS VOLUMES loki false RunAsAny MustRunAsNonRoot MustRunAs MustRunAs true configMap,emptyDir,persistentVolumeClaim,secret,projected,downwardAPI loki-grafana false RunAsAny RunAsAny RunAsAny RunAsAny false configMap,emptyDir,projected,secret,downwardAPI,persistentVolumeClaim loki-grafana-test false RunAsAny RunAsAny RunAsAny RunAsAny false configMap,downwardAPI,emptyDir,projected,secret loki-promtail false RunAsAny RunAsAny RunAsAny RunAsAny true secret,configMap,hostPath,projected,downwardAPI,emptyDir ==\u003e v1beta1/Role NAME AGE loki-grafana 1s ==\u003e v1beta1/RoleBinding NAME AGE loki-grafana 1s 创建完成后，通过暴露的svc访问Grafana： [root@Centos8 loki-stack]# kubectl get svc -n loki NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE loki ClusterIP 10.109.216.219 \u003cnone\u003e 3100/TCP 113s loki-grafana NodePort 10.100.203.138 \u003cnone\u003e 80:30002/TCP 113s loki-headless ClusterIP None \u003cnone\u003e 3100/TCP 113s ","date":"2023-12-09","objectID":"/kuberenetes-loki/:6:0","tags":["k8s"],"title":"Kubernetes Loki","uri":"/kuberenetes-loki/"},{"categories":["文档"],"content":"开始使用Loki 通过服务器ip+30002 访问， 登陆成功， 如果是安装Loki时采用的以上方法，开启了Grafana，那系统会自动配置好Data sources，应该不会有什么问题。 但是，如果是手动搭建的Grafana，需要手动添加Data Sources时，一定注意： 数据源名称中的Loki，L一定要是大写！！！ 如果不是大写，会导致连接不到Loki源，一般回报错：Error connecting to datasource: Loki: Bad Gateway. 502 如果是Loki，L大写 loki07 数据源添加完毕后，开始查看日志 点击Explore，可以看到选择labels的地方 loki08 以下是labels的展现形式 loki09 选择一个app:grafana的标签查看一下 loki10 默认 Loki会将stdout（正常输出）类型和stderr（错误输出）类型全部展示出来 除了这种办法，还可以直接通过上边的搜索栏，进行自定义的筛选，具体的语法问题，可以再自行查询学习。 还可以查看 Prometheus 的 metrics 信息 loki11 ","date":"2023-12-09","objectID":"/kuberenetes-loki/:7:0","tags":["k8s"],"title":"Kubernetes Loki","uri":"/kuberenetes-loki/"},{"categories":["文档"],"content":"kubernetes 日志查看工具 ","date":"2023-12-07","objectID":"/k8s-logs/:0:0","tags":["k8s"],"title":"K8s Logs","uri":"/k8s-logs/"},{"categories":["文档"],"content":"kubetail kubetail kubetail是一个bash脚本， 可以将多个pod的日志聚合到一个输出流中，并且支持彩色输出和条件过滤  liuhongfeng  kubetail -h kubetail \u003csearch term\u003e [-h] [-c] [-n] [-t] [-l] [-f] [-d] [-P] [-p] [-s] [-b] [-e] [-j] [-k] [-z] [-v] [-r] [-i] -- tail multiple Kubernetes pod logs at the same time where: -h, --help Show this help text. -c, --container The name of the container to tail in the pod (if multiple containers are defined in the pod). Defaults to all containers in the pod. Can be used multiple times. -t, --context The k8s context. ex. int1-context. Relies on ~/.kube/config for the contexts. -l, --selector Label selector. If used the pod name is ignored. -n, --namespace The Kubernetes namespace where the pods are located. Defaults to \"go-zero-baremetal\". -f, --follow Specify if the logs should be streamed. (true|false) Defaults to true. -d, --dry-run Print the names of the matched pods and containers, then exit. -P, --prefix Specify if add the pod name prefix before each line. (true|false) Defaults to true. -p, --previous Return logs for the previous instances of the pods, if available. (true|false) Defaults to false. -s, --since Only return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to 10s. -b, --line-buffered This flags indicates to use line-buffered. (true|false) Defaults to false. -e, --regex The type of name matching to use (regex|substring). Defaults to substring. -j, --jq If your output is json - use this jq-selector to parse it. Defaults to \"\". example: --jq \".logger + \\\" \\\" + .message\" -k, --colored-output Use colored output (pod|line|false). pod = only color pod name, line = color entire line, false = don't use any colors. Defaults to line. -z, --skip-colors Comma-separated list of colors to not use in output. If you have green foreground on black, this will skip dark grey and some greens: -z 2,8,10 Defaults to: 7,8. --timestamps Show timestamps for each log line. (true|false) Defaults to false. --tail Lines of recent log file to display. Defaults to -1, showing all log lines. -v, --version Prints the kubetail version. -r, --cluster The name of the kubeconfig cluster to use. -i, --show-color-index Show the color index before the pod name prefix that is shown before each log line. Normally only the pod name is added as a prefix before each line, for example \"[app-5b7ff6cbcd-bjv8n]\", but if \"show-color-index\" is true then color index is added as well: \"[1:app-5b7ff6cbcd-bjv8n]\". This is useful if you have color blindness or if you want to know which colors to exclude (see \"--skip-colors\"). Defaults to false. examples: kubetail my-pod-v1 kubetail my-pod-v1 -c my-container kubetail my-pod-v1 -t int1-context -c my-container kubetail '(service|consumer|thing)' -e regex kubetail -l service=my-service kubetail --selector service=my-service --since 10m kubetail --tail 1 ","date":"2023-12-07","objectID":"/k8s-logs/:1:0","tags":["k8s"],"title":"K8s Logs","uri":"/k8s-logs/"},{"categories":["文档"],"content":"安装 Just download the kubetail file (or any of the releases) and you’re good to go. ","date":"2023-12-07","objectID":"/k8s-logs/:2:0","tags":["k8s"],"title":"K8s Logs","uri":"/k8s-logs/"},{"categories":["文档"],"content":"Homebrew $ brew tap johanhaleby/kubetail \u0026\u0026 brew install kubetail ","date":"2023-12-07","objectID":"/k8s-logs/:2:1","tags":["k8s"],"title":"K8s Logs","uri":"/k8s-logs/"},{"categories":["文档"],"content":"Linux # download and to go # https://github.com/johanhaleby/kubetail/releases $ wget https://raw.githubusercontent.com/johanhaleby/kubetail/master/kubetail $ chmod +x kubetail $ cp kubetail /usr/local/bi ","date":"2023-12-07","objectID":"/k8s-logs/:2:2","tags":["k8s"],"title":"K8s Logs","uri":"/k8s-logs/"},{"categories":["文档"],"content":"zsh plugin # oh-my-zsh $ cd ~/.oh-my-zsh/custom/plugins/ $ git clone https://github.com/johanhaleby/kubetail.git kubetail $ vim ~/.zshrc plugins=( ... kubetail ) $ source ~/.zshrc ","date":"2023-12-07","objectID":"/k8s-logs/:2:3","tags":["k8s"],"title":"K8s Logs","uri":"/k8s-logs/"},{"categories":["文档"],"content":"使用 展示命名空间下的pod # show all your pods $ kubectl get pods -n test NAME READY STATUS RESTARTS AGE app1-v1-aba8y 1/1 Running 0 1d app1-v1-gc4st 1/1 Running 0 1d app1-v1-m8acl 1/1 Running 0 6d app1-v1-s20d0 1/1 Running 0 1d app2-v31-9pbpn 1/1 Running 0 1d app2-v31-q74wg 1/1 Running 0 1d my-demo-v5-0fa8o 1/1 Running 0 3h my-demo-v5-yhren 1/1 Running 0 2 一次追踪两个“app2”的日志 $ kubetail app2 仅追踪多个pod的特定容器的，指定容器 $ kubetail app2 -c container1 使用-c 追踪多个特定容器 $ kubetail app2 -c container1 -c container2 同时追随多个pod，使用逗号分隔 $ kubetail app1,app2 对于高级匹配，您可以使用正则表达式： $ kubetail \"^app1|.*my-demo.*\" --regex 要尾随特定命名空间内的日志，请确保在为容器和应用程序提供值后附加命名空间标志： $ kubetail app2 -c container1 -n namespace1 使用-k参数，指定kubetail如何使用颜色 $ kubetail app2 -k pod # pod： 只有pod名称着色且其他输出均使用终端默认颜色 $ kubetail app2 -k line # line： 整行是彩色的(默认) $ kubetail app2 -k false # false: 所有输出都不着色 starn 工具 kubernetes 多pod和container日志追踪 Stern 是使用 Go 语言开发的一款开箱即用的简单工具，它可以将多个 Pod 中的日志信息聚合到一起进行展示，并支持彩色输出和条件过滤 ","date":"2023-12-07","objectID":"/k8s-logs/:3:0","tags":["k8s"],"title":"K8s Logs","uri":"/k8s-logs/"},{"categories":["文档"],"content":"工具安装 Homebrew $ brew install stern Linux # download and to go # https://github.com/wercker/stern/tags $ wget https://github.com/wercker/stern/releases/download/1.11.0/stern_linux_amd64 $ chmod +x stern_linux_amd64 $ mv stern_linux_amd64 /usr/local/bin zsh plugin # bash-completion $ brew install bash-completion $ source \u003c(brew --prefix)/etc/bash-completion $ source \u003c(stern --completion=bash) # .zshrc $ source \u003c(stern --completion=zsh) ","date":"2023-12-07","objectID":"/k8s-logs/:4:0","tags":["k8s"],"title":"K8s Logs","uri":"/k8s-logs/"},{"categories":["文档"],"content":"介绍了工具的使用方式 # 查看默认名称空间下的所有Pod日志 $ stern . # 查看 Pod 中指定容器的日志 $ stern app2 --container container1 # 查看指定命名空间中容器的日志 $ stern app2 --namespace namespace1 # 查看指定命名空间中除指定容器外的所有容器的日志 $ stern --namespace namespace1 --exclude-container container1 . # 查看指定时间范围内容器的日志(15分钟内) $ stern app2 -t --since 15m # 查看所有命名空间中符合指定标签容器的日志 $ stern --all-namespaces -l run=nginx # 查找前端Pod中版本为canary的日志 $ stern frontend --selector release=canary # 将日志消息通过管道传输到jq命令 $ stern backend -o json | jq . # 仅输出日志消息本身 $ stern backend -o raw # 使用自定义模板输出 $ stern --template '{{.Message}} ({{.Namespace}}/{{.PodName}}/{{.ContainerName}})' backend # 使用stern提供的颜色的自定义模板输出 $ stern --template '{{.Message}} ({{.Namespace}}/{{color .PodColor .PodName}}/{{color .ContainerColor .ContainerName}})' backend ","date":"2023-12-07","objectID":"/k8s-logs/:5:0","tags":["k8s"],"title":"K8s Logs","uri":"/k8s-logs/"},{"categories":["文档"],"content":" Kubernetes 的 Limits 和 Requests 在k8s适应容器时， 要知道所涉及的资源时什么以及如何需要他们，有些进程比其他进程需要更多的CPU和内存 知道这些之后， 才能正确的配置我们的容器和pod Kubernetes 的Limits和Requests介绍 实践案例 Kubernetes Requests Kubernetes Limits CPU的特殊性 内存的特殊性 Namespace ResourceQuta Namespace LimitRange 总结 ","date":"2023-10-24","objectID":"/k8s_limit_quests/:0:0","tags":["k8s"],"title":"K8s_limit_quests","uri":"/k8s_limit_quests/"},{"categories":["文档"],"content":"Kubernetes 的Limits和Requests介绍 在适应Kubernetes 时，Limits和Requests 时重要的配置，主要包含了CPU和内存的配置 kubernetes将Limits定义为一个容器使用的最大资源量，意味着容器的消耗量永远不能超过显示的内存或CPU量。 Requests时指容器保留的最小保证量 ","date":"2023-10-24","objectID":"/k8s_limit_quests/:1:0","tags":["k8s"],"title":"K8s_limit_quests","uri":"/k8s_limit_quests/"},{"categories":["文档"],"content":"实践案例 deployment - demo kind: Deployment apiVersion: extensions/v1beta1 … template: spec: containers: - name: redis image: redis:5.0.3-alpine resources: limits: memory: 600Mi cpu: 1 requests: memory: 300Mi cpu: 500m - name: busybox image: busybox:1.28 resources: limits: memory: 200Mi cpu: 300m requests: memory: 100Mi cpu: 100m 将deployment 部署到4C16 G配置的节点上， 可以得到信息 Pod的有效请求时400MiB的内存和600 millicores的CPU， 需要一个有足够资源的可分配空间的节点来安排pod Redis容器的CPU份额将是512， 而busybox容器时102，Kubernetes总是为每个核心分配的1024个份额，因此redis：1024 * 0.5 cores ≅ 512和busybox：1024 * 0.1核 ≅ 102 如果Redis容器试图分配超过600MB的RAM，它将被OOM杀死，很可能是pod失败 如果Redis试图在每100ms内使用超过100ms的CPU， （因为有4个CPU， 可用时间为每个100ms 400ms），它将遭受CPU节流，导致性能下降 如果Busybox容器试图分配超过200MB的RAM，它将被OOM杀死，导致一个失败的Pod 如果Busybox试图每100ms使用超过30ms的CPU，它将遭受CPU节流，导致性能下降。 ","date":"2023-10-24","objectID":"/k8s_limit_quests/:2:0","tags":["k8s"],"title":"K8s_limit_quests","uri":"/k8s_limit_quests/"},{"categories":["文档"],"content":"Kubernetes Requests Kubernetes 将请求定义为容器使用的资源最低保证量 基本上，他将设定容器所需消耗的资源的最小数量 当一个pod被调度时，kube-scheduler将检查kebernetes请求，以便将其分配给一个特定的节点，该节点至少可以满足pod中所有容器的这个数量，如果请求的数量高于可用的资源，pod将不会被安排，并保持在Pending状态 在这个例子中，在容器定义中，我们设置了一个请求，要求100m核心的CPU和4Mi的内存 resources: requests: cpu: 0.1 memory: 4Mi Requests通常被使用在一下场景： 当把Pod分配给一个节点时， 所以Pod中的容器的指定请求被满足 在运行时， 指定的请求量被保证在该Pod 中的容器的最小值。 ","date":"2023-10-24","objectID":"/k8s_limit_quests/:3:0","tags":["k8s"],"title":"K8s_limit_quests","uri":"/k8s_limit_quests/"},{"categories":["文档"],"content":"Kubernetes Limits Kubernetes 将Limits定义为一个容器使用的最大资源量 这意味着容器的消耗量永远不能超过指定的内存量或CPU量 resources: limits: cpu: 0.5 memory: 100Mi Limits通常用于一下场景： 当把Pod分配给一个节点时，如果没有设置请求，默认情况下，Kubernetes将分配请求=限制 在运行时， Kubernetes 将检查Pod中的容器所消耗的资源量是否过于限制所显示的数量 ","date":"2023-10-24","objectID":"/k8s_limit_quests/:4:0","tags":["k8s"],"title":"K8s_limit_quests","uri":"/k8s_limit_quests/"},{"categories":["文档"],"content":"CPU的特殊性 CPU是一种可压缩的资源，这意味他可以被拉伸，一满足所有的需求，如果进程要求的太多的CPU，其中一些将被节制 CPU代表计算处理时间，以核为单位 你可以用毫微米（m）来表示比一个核心更小的数量（例如，500米是半个核心） 最小的数量时1m 一个节点可能有一个以上的核心可用， 所以请求CPU\u003e1时kennel的 ","date":"2023-10-24","objectID":"/k8s_limit_quests/:5:0","tags":["k8s"],"title":"K8s_limit_quests","uri":"/k8s_limit_quests/"},{"categories":["文档"],"content":"内存的特殊性 内存时一种不可压缩的资源，意味着它不能像CPU那样被拉伸，如果一个进程没有得到足够的内存来工作， 这个进程就会被杀死 在Kubernetes中，内存的单位是字节。 你可以用，E，P，T，G，M，k来代表Exabyte，Petabyte，Terabyte，Gigabyte，Megabyte和kilobyte，尽管只有最后四个是常用的。(例如，500M, 4G) 不要用小写的m表示内存（这代表Millibytes，低得离谱） 你可以用Mi来定义Mebibytes，其余的也可以用Ei、Pi、Ti来定义（例如，500Mi） ","date":"2023-10-24","objectID":"/k8s_limit_quests/:6:0","tags":["k8s"],"title":"K8s_limit_quests","uri":"/k8s_limit_quests/"},{"categories":["文档"],"content":"最佳实践 在Kubernetes中， 你应该很少使用限制来控制你的资源的使用，这是因为如果你想避免使用饥饿（确保每个重要的进程都能得到它的份额）， 应该首先使用请求 通过设置限制，你只是防止进程在特殊情况下检索额外的资源，在内存方面造成OOM杀戮，在CPU方面造成Throttling（进程将需要等待CPU可以再次使用） 如果一个Pod的所有请求中设置一个等于限制的请求值，该Pod将获得保证的服务质量 还需要注意的是， 资源使用量高于请求的Pod更有可能被驱逐，所以设置非常低的请求会造成弊大于利。 ","date":"2023-10-24","objectID":"/k8s_limit_quests/:7:0","tags":["k8s"],"title":"K8s_limit_quests","uri":"/k8s_limit_quests/"},{"categories":["文档"],"content":"Namespace ResourceQuta 由于命名空间的存在，我们可以将Kubernetes资源隔离到不同的组，也称为租户 通过ResourceQuota， 可以为整个命令空间设置一个内存或CPU限制，确保其中的实体不能消耗超过这个数量 apiVersion: v1 kind: ResourceQuota metadata: name: mem-cpu-demo spec: hard: requests.cpu: 2 requests.memory: 1Gi limits.cpu: 3 limits.memory: 2Gi requests.cpu：这个命名空间中所有请求的最大CPU数量。 requests.memory：这个命名空间中所有请求的最大内存量。 limits.cpu：这个命名空间中所有限制的最大CPU数量。 limits.memory：这个命名空间中所有限制的总和的最大内存量。 ","date":"2023-10-24","objectID":"/k8s_limit_quests/:8:0","tags":["k8s"],"title":"K8s_limit_quests","uri":"/k8s_limit_quests/"},{"categories":["文档"],"content":"Namespace LimitRange 如果我们想限制一个命名空间可分配的资源总量，ResourceQuotas很有用。但如果我们想给里面的元素提供默认值，会发生什么？ LimitRanges是一种Kubernetes策略，它限制了命名空间中每个实体的资源设置。 apiVersion: v1 kind: LimitRange metadata: name: cpu-resource-constraint spec: limits: - default: cpu: 500m defaultRequest: cpu: 500m min: cpu: 100m max: cpu: \"1\" type: Container default。如果没有指定，创建的容器将有这个值。 min: 创建的容器不能有比这更小的限制或请求。 max: 创建的容器不能有大于此值的限制或请求。 以后， 如果创建一个没有设置请求或限制的新的Pod， LimitRange会自动为其所有的容器设置这些值。 Limits: cpu: 500m Requests: cpu: 100m ","date":"2023-10-24","objectID":"/k8s_limit_quests/:9:0","tags":["k8s"],"title":"K8s_limit_quests","uri":"/k8s_limit_quests/"},{"categories":["文档"],"content":"总结 kubernetes集群选择最佳的限制是关键，以便获得最佳的能源消耗和成本 为我们的Pod分配过多的资源可能会导致成本激增。 规模过小或专用于极少的CPU或内存将导致应用程序不能正常运行，甚至Pod被驱逐。 ","date":"2023-10-24","objectID":"/k8s_limit_quests/:10:0","tags":["k8s"],"title":"K8s_limit_quests","uri":"/k8s_limit_quests/"},{"categories":["文档"],"content":"参考文档 【1】https://sysdig.com/blog/kubernetes-pod-pending-problems/ 【2】https://sysdig.com/blog/troubleshoot-kubernetes-oom/ 【3】https://sysdig.com/blog/kubernetes-pod-evicted/ 原文：https://sysdig.com/blog/kubernetes-limits-requests/ ","date":"2023-10-24","objectID":"/k8s_limit_quests/:11:0","tags":["k8s"],"title":"K8s_limit_quests","uri":"/k8s_limit_quests/"},{"categories":["文档"],"content":"Jsonparser 开源的JSON包 号称比JSON包性能高10倍，内存分配优化到0，提高JSON操作的性能 jsonparser 的性能在很大程度上取决于使用情况，当不需要处理完成记录而只需要处理某几个字段时（尤其是访问第三方接口时，大多数情况下只需要几个字段）性能表现的非常好 ","date":"2023-08-11","objectID":"/golang-jsonparser/:0:0","tags":["golang","go第三方库"],"title":"Golang Jsonparser","uri":"/golang-jsonparser/"},{"categories":["文档"],"content":"方法对应关系 jsonparser标准库JSON工作方式不同，不会 编码/解码 整个数据结构， 而是按需操作 标准库 jsonparser 编码 Marshal Set 解码 Unmarshal Get jsonparser 在Get 的基础上封装了很多针对单个字段的使用方法，例如 package main import ( \"fmt\" \"log\" \"github.com/buger/jsonparser\" ) var ( // JSON 字符串 dataJson = []byte(` { \"person\": { \"name\": { \"first\": \"Leonid\", \"last\": \"Bugaev\", \"fullName\": \"Leonid Bugaev\" }, \"github\": { \"handle\": \"buger\", \"followers\": 109 }, \"avatars\": [ { \"url\": \"https://avatars1.githubusercontent.com/u/14009?v=3\u0026s=460\", \"type\": \"thumbnail\" } ] }, \"company\": { \"name\": \"Acme\" } } `) ) func main() { // 解析对象 github := struct { Handle string `json:\"handle\"` Followers int `json:\"followers\"` }{} err := jsonparser.ObjectEach(dataJson, func(key []byte, value []byte, dataType jsonparser.ValueType, offset int) error { switch string(key) { case \"handle\": github.Handle = string(value) case \"followers\": followers, _ := jsonparser.ParseInt(value) github.Followers = int(followers) } return nil }, \"person\", \"github\") if err != nil { log.Fatal(err) } fmt.Printf(\"github = %+v\\n\\n\", github) // 编码结构体 githubJson, err := jsonparser.Set([]byte(`{}`), []byte(fmt.Sprintf(`{\"handle: %s\", \"followers\": \"%d\"}`, github.Handle, github.Followers)), \"github\") if err != nil { log.Fatal(err) } fmt.Printf(\"github json = %s\\n\\n\", githubJson) // 解析多个 key paths := [][]string{ {\"person\", \"name\", \"fullName\"}, {\"person\", \"avatars\", \"[0]\", \"url\"}, {\"company\", \"name\"}, } jsonparser.EachKey(dataJson, func(i int, bytes []byte, valueType jsonparser.ValueType, err error) { switch i { case 0: fmt.Printf(\"fullName = %s\\n\", bytes) case 1: fmt.Printf(\"avatars[0].url = %s\\n\", bytes) case 2: fmt.Printf(\"company.name = %s\\n\\n\", bytes) } }, paths...) // 解析整数 n, err := jsonparser.GetInt(dataJson, \"person\", \"github\", \"followers\") if err != nil { log.Fatal(err) } fmt.Printf(\"n = %d\\n\\n\", n) // 解析字符串 name, err := jsonparser.GetString(dataJson, \"company\", \"name\") if err != nil { log.Fatal(err) } fmt.Printf(\"name = %s\\n\", name) } ","date":"2023-08-11","objectID":"/golang-jsonparser/:1:0","tags":["golang","go第三方库"],"title":"Golang Jsonparser","uri":"/golang-jsonparser/"},{"categories":["文档"],"content":"代码实现 jsonparser 的核心代码全部放在了一个文件中 parser.go, 我们主要关注两种操作的实现: 解码GET 和 编码Set ","date":"2023-08-11","objectID":"/golang-jsonparser/:2:0","tags":["golang","go第三方库"],"title":"Golang Jsonparser","uri":"/golang-jsonparser/"},{"categories":["文档"],"content":"数据类型 jsonparser 将合法的JSON数据类型简单进行常量映射 // JSON 数据类型常量 type ValueType int const ( NotExist = ValueType(iota) String Number Object Array Boolean Null Unknown ) 为了规避 GC, 用于 JSON 字符串转义分配的 []byte 切片容量上限，超过这个上限值后，转义结果会被分配到 堆上 引发 GC， 也就是 []byte 切片的长度超过 64 之后，会被分配到 堆上 // 规避 GC 的切片容量上限常量 const unescapeStackBufSize = 64 ","date":"2023-08-11","objectID":"/golang-jsonparser/:2:1","tags":["golang","go第三方库"],"title":"Golang Jsonparser","uri":"/golang-jsonparser/"},{"categories":["文档"],"content":"辅助方法 stringEnd 尝试寻找当前字符串的结尾，也就是 “, 支持转义的情况，例如 \" func stringEnd(data []byte) (int, bool) {} blockEnd 尝试寻找当前数组或对象的结尾，数组的开始和结尾表示为 [ 和 ], 对象的开始和结尾表示为 { 和 }。 func blockEnd(data []byte, openSym byte, closeSym byte) int {} ","date":"2023-08-11","objectID":"/golang-jsonparser/:3:0","tags":["golang","go第三方库"],"title":"Golang Jsonparser","uri":"/golang-jsonparser/"},{"categories":["文档"],"content":"searchKeys 状态机 searchKeys 方法是整个 解析操作 的核心方法，内部实现类似于 有限状态机 机制，通过将 JSON 字符串作为输入参数，并根据定义的状态转换规则逐个解析字符， 结果返回解析到的索引，或者 -1 (解析失败)。 ","date":"2023-08-11","objectID":"/golang-jsonparser/:4:0","tags":["golang","go第三方库"],"title":"Golang Jsonparser","uri":"/golang-jsonparser/"},{"categories":["文档"],"content":"状态机组成部分 输入参数： 存储解析的JSON字符串 解析器： 解析 JSON 字符串并返回相应的数据结构 (searchKeys 方法返回的是 []byte) 状态转移表： 定义状态转换规则，包括当前状态、下一个字符以及下一个状态 (searchKeys 主要是通过上面提到的辅助方法来完成的) 状态栈 : 记录状态转换过程中的状态 (searchKeys 用了几个变量来记录状态，例如 keyLevel, level, ln, lk 等) ","date":"2023-08-11","objectID":"/golang-jsonparser/:4:1","tags":["golang","go第三方库"],"title":"Golang Jsonparser","uri":"/golang-jsonparser/"},{"categories":["文档"],"content":"单例模式 单例模式宏观介绍 饿汉式单例模式实现思路 懒汉式单例模式实现推演 Golang 单例工具 sync.Once 源码解析 单例模式背景 在单例模式下， 声明一个类并保证这个类只存在全局唯一的实例供外部反复使用 单例模式的适用场景 一些只允许存在一个实例的类，比如全局统一的监控统计模块。 一些实例化很耗费资源的类，比如协程池、连接池、和第三方交互的客户端等 一些入参复杂的系统模块组件，比如 controller、service、dao等 ","date":"2023-08-05","objectID":"/golang-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/:0:0","tags":["golang"],"title":"Golang 设计模式 单例模式","uri":"/golang-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"categories":["文档"],"content":"实现模式 在单例模式的实现上， 可以分为饿汉式和懒汉式两种类型 饿汉式： 从一开始就完成单例的初始化工作， 以备不时只需 懒汉式： 贯彻佛系思想，不到逼不得已（需要被使用了），不执行单例的初始化工作 ","date":"2023-08-05","objectID":"/golang-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/:1:0","tags":["golang"],"title":"Golang 设计模式 单例模式","uri":"/golang-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"categories":["文档"],"content":"饿汉式单例模式 “饿” 指的是，对于单例对象而言，不论其后续有没有被使用到以及何时才会被使用到，都会在程序启动之初完成其初始化工作. 饿汉式单例模式的执行步骤 单例类和构建方法声明为不可导出类型，避免被外部直接获取到 在代码启动之初，酒初始化一个全局单一的实例， 作为后续所谓的“单例” 暴露一个可倒出的单例获取方法，GetXXX()，用于返回这个单例对象 ","date":"2023-08-05","objectID":"/golang-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/:2:0","tags":["golang"],"title":"Golang 设计模式 单例模式","uri":"/golang-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"categories":["文档"],"content":"Iota 介绍 在常量声明中，预先声明的标识符iota代表连续的无类型的整数常量， 他的值是该常量声明中对应ConstSpec的索引，从零开始计数。 iota： 可以在常量声明中自动创建一系列连续的整数值，值从零开始， 不需要手动指定每个常量的值 ","date":"2023-08-05","objectID":"/golang-iota/:0:0","tags":["golang"],"title":"Golang Iota","uri":"/golang-iota/"},{"categories":["文档"],"content":"iota的应用场景 ","date":"2023-08-05","objectID":"/golang-iota/:1:0","tags":["golang"],"title":"Golang Iota","uri":"/golang-iota/"},{"categories":["文档"],"content":"自动生成递增的常量值 iota 可以方便生成递增的常量值， 在常量声明中的第一个使用iota的常量初始化未0， 而后的常量都会自动递增，这是得在定义一组递增常量时无需手动指定每个常量的值， 提高了代码的 可读性和维护性 const ( Apple = iota // 0 Banana // 1 Cherry // 2 ) ","date":"2023-08-05","objectID":"/golang-iota/:1:1","tags":["golang"],"title":"Golang Iota","uri":"/golang-iota/"},{"categories":["文档"],"content":"构建枚举类型常量 使用iota 可以轻松定义一系列相关的枚举值， 而无需为每个值手动指定具体的数字，这样的枚举类型定义更加简洁，易于扩展和修改 type WeekDay int const ( Sunday WeekDay = iota // 0 Tuesday // 1 Wednesday // 2 Thursday // 3 Friday // 4 Saturday // 5 Monday // 6 ) ","date":"2023-08-05","objectID":"/golang-iota/:1:2","tags":["golang"],"title":"Golang Iota","uri":"/golang-iota/"},{"categories":["文档"],"content":"表达式计算 通过在常量声明中使用iota， 可以创建复杂的表达式， 并在每个常量表达式中， 根据需要调整iota的值， 可以轻松生成一组具有特定规律的常量 const ( _ = iota KB = 1 \u003c\u003c (10 * iota) // 1 \u003c\u003c (10 * 1) = 1024B = 1KB MB = 1 \u003c\u003c (10 * iota) // 1 \u003c\u003c (10 * 2) = 1048576B = 1MB GB = 1 \u003c\u003c (10 * iota) // 1 \u003c\u003c (10 * 3) = 1073741824B = 1GB TB = 1 \u003c\u003c (10 * iota) // 1 \u003c\u003c (10 * 4) = 1099511627776B = 1TB ) ","date":"2023-08-05","objectID":"/golang-iota/:1:3","tags":["golang"],"title":"Golang Iota","uri":"/golang-iota/"},{"categories":["文档"],"content":"位运算 通过左移运算符 （«）与iota配合使用，方便地生成一组按位运算的常量 const ( FlagNone = 0 // 0 FlagRead = 1 \u003c\u003c iota // 1 FlagWrite // 2 FlagExec // 4 ) ","date":"2023-08-05","objectID":"/golang-iota/:1:4","tags":["golang"],"title":"Golang Iota","uri":"/golang-iota/"},{"categories":["文档"],"content":"Iota 的使用技巧和注意事项 ","date":"2023-08-05","objectID":"/golang-iota/:2:0","tags":["golang"],"title":"Golang Iota","uri":"/golang-iota/"},{"categories":["文档"],"content":"跳值使用 使用_下划线来忽略某些值， const ( Apple = iota // 0 _ Banana // 2 ) ","date":"2023-08-05","objectID":"/golang-iota/:2:1","tags":["golang"],"title":"Golang Iota","uri":"/golang-iota/"},{"categories":["文档"],"content":"不同常量块，iota是独立的 iota 的作用范围是整个常量块， 不同常量块的iota是独立的， 每个常量块中第一个iota的值都是0 const ( A = iota // 0 B // 1 ) const ( C = iota // 0 D // 1 ) ","date":"2023-08-05","objectID":"/golang-iota/:2:2","tags":["golang"],"title":"Golang Iota","uri":"/golang-iota/"},{"categories":["文档"],"content":"Redfish Api Redfish 利用常见的互联网和web 服务标准将信息直接提供给相关的工具链 IPMI 是一种较早的带外管理管理工具，仅限最小公共集命令集（开机/关机/重启/温度值/文本控制台）还是需要带内管理软件 当管控设备多的时候， 需要统一管理， 需要对接不同供应商的API， 基础的IMPI功能不好满足横向扩展环境，需要更便捷的方式调用服务器高级管理功能新的需求 Redfish可扩展平台管理API是一种新的规范，使用RESTful语句来访问定义在模型格式的数据，用于执行带外系统管理 ","date":"2023-07-08","objectID":"/redfish-api/:0:0","tags":["redfish-api"],"title":"Redfish Api","uri":"/redfish-api/"},{"categories":["文档"],"content":"Redfish应用场景 用户期望能够批量管理服务器，比如笔者想一次性给100个服务器安装系统，并且这100个服务器并不都是同一厂商，不同厂商的IPMI操作都不一样，比如Dell是iDRAC，你还需要专门学习iDRAC使用和各种对接，这会带来很多困扰。而Redfish标准的出现彻底改变这种情况，它是凌驾于所有服务器之上的一个标准，对服务器的基本操作都是统一的，并且是基于Restful API的方式实现 ","date":"2023-07-08","objectID":"/redfish-api/:1:0","tags":["redfish-api"],"title":"Redfish Api","uri":"/redfish-api/"},{"categories":["文档"],"content":"选择REST、HTTP以及JSON 除了REST、HTTP和JSON之外，Redfish还采用常见的OData v4约定来描述模式、URL约定和命名，以及JSON有效负载中常见属性的结构。越来越多的通用客户端库、应用程序和工具生态系统使用Redfish。 它有多简单?下面显示了使用Redfish从服务器检索序列号的示例Python代码：此示例中的输出如下所示 awData= urllib.urlopen(‘http://192.168.1.135/redfish/v1/Systems/1’) jsonData=json.loads(rawData) print(jsonData[‘SerialNumber’]) 1A87CA442K ","date":"2023-07-08","objectID":"/redfish-api/:2:0","tags":["redfish-api"],"title":"Redfish Api","uri":"/redfish-api/"},{"categories":["文档"],"content":"golang 的SDK https://github.com/Nordix/go-redfish 封装客户端 cfg := \u0026redfish.Configuration{ BasePath: \"http://localhost:8000\", DefaultHeader: make(map[string]string), UserAgent: \"go-redfish/client\", } redfishApi := redfish.NewAPIClient(cfg).DefaultApi 列出可用的计算机系统 sl, _, _ := redfishApi.ListSystems(context.Background()) 重置计算机系统 system_id := \"dd9fd064-263b-469c-91d4-d45f341fe2c5\" systemReq := redfish.ComputerSystem{ResetType: \"ForceRestart\"} reset_resp, _, _ = redfishApi.ResetSystem(context.Background(), system_id, reset_type) 给计算器插入虚拟媒体 manager_id := \"58893887-8974-2487-2389-841168418919\" insertReq := redfish.InsertMediaRequestBody{} insertReq.Image = \"http://releases.ubuntu.com/19.04/ubuntu-19.04-live-server-amd64.iso\" insertReq.Inserted = true redfishApi.InsertVirtualMedia(context.Background(), manager_id, \"Cd\", insertReq) 为下次启动设置启动设备 system_id := \"dd9fd064-263b-469c-91d4-d45f341fe2c5\" systemReq := redfish.ComputerSystem{} systemReq.Boot.BootSourceOverrideTarget = \"Cd\" redfishApi.SetSystem(context.Background(), system_id, systemReq) ","date":"2023-07-08","objectID":"/redfish-api/:3:0","tags":["redfish-api"],"title":"Redfish Api","uri":"/redfish-api/"},{"categories":["文档"],"content":"Redfish实践 The python-redfish project Python环境redfish接口获取泰山服务器和鲲鹏CPU信息 redfish是当前主流的服务器监控协议，通过redfish协议可以通过带外管理通道获取服务器状态和详细硬件信息 pip install python-redfish import redfish login_host=\"https://10.93.20.10\" login_account=\"ADMIN\" login_password=\"ADMIN\" REDFISH_OBJ = redfish.redfish_client(base_url=login_host, username=login_account, password=login_password, default_prefix='/redfish/v1') REDFISH_OBJ.login(auth=\"session\") response = REDFISH_OBJ.get(\"/redfish/v1/Systems/1\", None) print(response) REDFISH_OBJ.logout() 参考链接 ","date":"2023-07-08","objectID":"/redfish-api/:4:0","tags":["redfish-api"],"title":"Redfish Api","uri":"/redfish-api/"},{"categories":["文档"],"content":"Supermicro Redfish 特点 获取系统/机箱信息 管理用户账号和权限 BMC配置（AD、LDAP、SNMP、SMTP、RADIUS、Fan mode、Mouse mode、NTP、Snooping 等等） BIOS 配置 开机顺序变更 RAID 储存装置管理（For Broadcom 3108, 3008, 3216, 3616 \u0026 Marvel SE9230） 磁盘管理 获取 NIC MAC 信息（NIC 资讯） BMC／BIOS／3108韧体更新 获取温度／电源／传感器讯息 ","date":"2023-07-08","objectID":"/redfish-api/:5:0","tags":["redfish-api"],"title":"Redfish Api","uri":"/redfish-api/"},{"categories":["文档"],"content":" 🔥 gRpc 教程- protobuf 通信模式 四种通信模式 Simple RPC Server-Streaming RPC Client-Streaming RPC Bidirectionnal-Streaming RPC ","date":"2023-07-01","objectID":"/golang-protobuf-messagemode/:0:0","tags":["golang","rpc","protobuf"],"title":"Golang Protobuf MessageMode","uri":"/golang-protobuf-messagemode/"},{"categories":["文档"],"content":"Simple RPC syntax = \"proto3\"; package ecommerce; import \"google/protobuf/wrappers.proto\"; option go_package = \"ecommerce/\"; message Order { string id = 1; repeated string items = 2; string description = 3; float price = 4; string destination = 5; } service OrderManagement { rpc getOrder(google.protobuf.StringValue) returns (Order); } 注意事项 使用protobuf最新版本syntax = “proto3”; protoc-gen-go要求 pb 文件必须指定 go 包的路径。即option go_package = “ecommerce/”; 定义的method仅能有一个入参和出参数。如果需要传递多个参数需要定义成message 使用import引用另外一个文件的 pb。google/protobuf/wrappers.proto是 google 内置的类型 生成 go 和 grpc 的代码 $ protoc -I ./pb \\ --go_out ./ecommerce --go_opt paths=source_relative \\ --go-grpc_out ./ecommerce --go-grpc_opt paths=source_relative \\ ./pb/product.proto ","date":"2023-07-01","objectID":"/golang-protobuf-messagemode/:1:0","tags":["golang","rpc","protobuf"],"title":"Golang Protobuf MessageMode","uri":"/golang-protobuf-messagemode/"},{"categories":["文档"],"content":"server实现","date":"2023-07-01","objectID":"/golang-protobuf-messagemode/:1:1","tags":["golang","rpc","protobuf"],"title":"Golang Protobuf MessageMode","uri":"/golang-protobuf-messagemode/"},{"categories":["文档"],"content":" 🔥 gRpc 教程- protobuf 基础 序列化协议： grpc 使用 protobuf, 首先使用protobuf 定义服务，然后使用这个文件来生成客户端和服务端的代码， 因为pb是跨语言的， 因此使用服务端和客户端语言并不一致也可以相互序列化和反序列化 网络传输层： grpc使用的是http2.0协议， ","date":"2023-07-01","objectID":"/golang-protobuf-base/:0:0","tags":["golang","rpc","protobuf"],"title":"Golang Protobuf Base","uri":"/golang-protobuf-base/"},{"categories":["文档"],"content":"Protobuf IDL 序列化通俗来说就是把内存的一段数据转化为二进制并存储或者通过网络传输， 读取磁盘或另一端收到后可以在内存中重建这段数据 protobuf 协议是跨语言跨平台的序列化协议 protobuf 本身并不是和gPRC绑定的， 它可以被用于非RPC场景， 如内存等 json、xml 都是一种序列化方式， 只是他们不需要提前预定义 idl， 且具备可读性， https://protobuf.dev/programming-guides/proto3/ ","date":"2023-07-01","objectID":"/golang-protobuf-base/:1:0","tags":["golang","rpc","protobuf"],"title":"Golang Protobuf Base","uri":"/golang-protobuf-base/"},{"categories":["文档"],"content":"定义消息类型 protobuf里最基础的类型就是 message, 每一个message都会有一个或者多个字段（field）， 其中字段包含如下元素 类型： 类型不仅可以是标量类型（int、string等），也可以是复合类型（enum等），也可以是其他message 字段名： 字段名比较推荐的是使用下划线/分隔名称 字段编号： 一个message内每一个字段编号都必须唯一的，在编码后其实传递的是这个编号而不是字段名 字段规则：消息字段可以是以下字段之一 singular：格式正确的消息可以有零个或一个字段（但不能超过一个）。使用 proto3 语法时，如果未为给定字段指定其他字段规则，则这是默认字段规则 optional：与 singular 相同，不过您可以检查该值是否明确设置 repeated：在格式正确的消息中，此字段类型可以重复零次或多次。系统会保留重复值的顺序 map：这是一个成对的键值对字段 保留字段：为了避免再次使用到已移除的字段可以设定保留字段。如果任何未来用户尝试使用这些字段标识符，协议缓冲区编译器就会报错 ","date":"2023-07-01","objectID":"/golang-protobuf-base/:2:0","tags":["golang","rpc","protobuf"],"title":"Golang Protobuf Base","uri":"/golang-protobuf-base/"},{"categories":["文档"],"content":"复合类型 ","date":"2023-07-01","objectID":"/golang-protobuf-base/:3:0","tags":["golang","rpc","protobuf"],"title":"Golang Protobuf Base","uri":"/golang-protobuf-base/"},{"categories":["文档"],"content":"数组 message SearchResponse { repeated Result results = 1; } message Result { string url = 1; string title = 2; repeated string snippets = 3; } ","date":"2023-07-01","objectID":"/golang-protobuf-base/:3:1","tags":["golang","rpc","protobuf"],"title":"Golang Protobuf Base","uri":"/golang-protobuf-base/"},{"categories":["文档"],"content":"枚举 message SearchRequest { string query = 1; int32 page_number = 2; int32 result_per_page = 3; enum Corpus { UNIVERSAL = 0; WEB = 1; IMAGES = 2; LOCAL = 3; NEWS = 4; PRODUCTS = 5; VIDEO = 6; } Corpus corpus = 4; } ","date":"2023-07-01","objectID":"/golang-protobuf-base/:3:2","tags":["golang","rpc","protobuf"],"title":"Golang Protobuf Base","uri":"/golang-protobuf-base/"},{"categories":["文档"],"content":"服务 定义的method仅能有一个入参和出参数。如果需要传递多个参数需要定义成message service SearchService { rpc Search(SearchRequest) returns (SearchResponse); } ","date":"2023-07-01","objectID":"/golang-protobuf-base/:3:3","tags":["golang","rpc","protobuf"],"title":"Golang Protobuf Base","uri":"/golang-protobuf-base/"},{"categories":["文档"],"content":"使用其他消息类型 使用import引用另外一个文件的pb syntax = \"proto3\"; import \"google/protobuf/wrappers.proto\"; package ecommerce; message Order { string id = 1; repeated string items = 2; string description = 3; float price = 4; google.protobuf.StringValue destination = 5; } ","date":"2023-07-01","objectID":"/golang-protobuf-base/:3:4","tags":["golang","rpc","protobuf"],"title":"Golang Protobuf Base","uri":"/golang-protobuf-base/"},{"categories":["文档"],"content":"protoc使用 protoc就是protobuf的编译器，它把proto文件编译成不同的语言 ","date":"2023-07-01","objectID":"/golang-protobuf-base/:4:0","tags":["golang","rpc","protobuf"],"title":"Golang Protobuf Base","uri":"/golang-protobuf-base/"},{"categories":["文档"],"content":"安装 Linux, using apt or apt-get, for example: $ apt install -y protobuf-compiler $ protoc --version # Ensure compiler version is 3+ MacOS, using Homebrew[1]: $ brew install protobuf $ protoc --version # Ensure compiler version is 3+ ","date":"2023-07-01","objectID":"/golang-protobuf-base/:5:0","tags":["golang","rpc","protobuf"],"title":"Golang Protobuf Base","uri":"/golang-protobuf-base/"},{"categories":["文档"],"content":"使用 $ protoc --help Usage: protoc [OPTION] PROTO_FILES -IPATH, --proto_path=PATH 指定搜索路径 --plugin=EXECUTABLE: .... --cpp_out=OUT_DIR Generate C++ header and source. --csharp_out=OUT_DIR Generate C# source file. --java_out=OUT_DIR Generate Java source file. --js_out=OUT_DIR Generate JavaScript source. --objc_out=OUT_DIR Generate Objective C header and source. --php_out=OUT_DIR Generate PHP source file. --python_out=OUT_DIR Generate Python source file. --ruby_out=OUT_DIR Generate Ruby source file @\u003cfilename\u003e proto文件的具体位置 ","date":"2023-07-01","objectID":"/golang-protobuf-base/:5:1","tags":["golang","rpc","protobuf"],"title":"Golang Protobuf Base","uri":"/golang-protobuf-base/"},{"categories":["文档"],"content":"搜索路径参数 第一个比较重要的参数就是搜索路径参数，即上述展示的-IPATH, –proto_path=PATH。它表示的是我们要在哪个路径下搜索.proto文件，这个参数既可以用-I指定，也可以使用–proto_path=指定 ","date":"2023-07-01","objectID":"/golang-protobuf-base/:5:2","tags":["golang","rpc","protobuf"],"title":"Golang Protobuf Base","uri":"/golang-protobuf-base/"},{"categories":["文档"],"content":"语言插件参数 Language C++ (include C++ runtime and protoc) Java Python Objective-C C# Ruby PHP ","date":"2023-07-01","objectID":"/golang-protobuf-base/:5:3","tags":["golang","rpc","protobuf"],"title":"Golang Protobuf Base","uri":"/golang-protobuf-base/"},{"categories":["文档"],"content":"proto文件位置参数 proto文件位置参数即上述的@参数，指定了我们proto文件的具体位置，如proto1/greeter/greeter.proto ","date":"2023-07-01","objectID":"/golang-protobuf-base/:5:4","tags":["golang","rpc","protobuf"],"title":"Golang Protobuf Base","uri":"/golang-protobuf-base/"},{"categories":["文档"],"content":"golang插件 安装 非内置的语言支持就得自己单独安装语言插件，比如–go_out=对应的是protoc-gen-go，安装命令如下 # 最新版 $ go install google.golang.org/protobuf/cmd/protoc-gen-go@latest # 指定版本 $ go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.3.0 生成代码 $ protoc --proto_path=src --go_out=out --go_opt=paths=source_relative foo.proto bar/baz.proto ","date":"2023-07-01","objectID":"/golang-protobuf-base/:6:0","tags":["golang","rpc","protobuf"],"title":"Golang Protobuf Base","uri":"/golang-protobuf-base/"},{"categories":["文档"],"content":" 🔥Golang算法 - 快速排序（Quicksort） 广度优先算法的核心思想是：从初始节点开始，应用算符生成第一层节点，检查目标节点是否在这些后继节点中，若没有，再用产生式规则将所有第一层的节点逐一扩展，得到第二层节点，并逐一检查第二层节点中是否包含目标节点。若没有，再用算符逐一扩展第二层的所有节点……，如此依次扩展，检查下去，直到发现目标节点为止。即 ","date":"2023-06-10","objectID":"/golang-beradth-firstsearch/:0:0","tags":["golang","算法"],"title":"🔥Golang Beradth FirstSearch","uri":"/golang-beradth-firstsearch/"},{"categories":["文档"],"content":"图（Graph）是什么？ 图是用来对不同事物间如何管理进行建模的一种方式 ","date":"2023-06-10","objectID":"/golang-beradth-firstsearch/:1:0","tags":["golang","算法"],"title":"🔥Golang Beradth FirstSearch","uri":"/golang-beradth-firstsearch/"},{"categories":["文档"],"content":"数据结构 Queue 现进来的数据先处理（FIFO） 无法随机的访问Queue 里面的元素 相关操作 enqueue: 添加元素 dequeue: 移除元素 代码示例 package main import \"fmt\" type GraphMap map[string][]string func main() { var graphMap GraphMap = make(GraphMap, 0) graphMap[\"you\"] = []string{\"alice\", \"bob\", \"claire\"} graphMap[\"bob\"] = []string{\"anuj\", \"peggy\"} graphMap[\"alice\"] = []string{\"peggy\"} graphMap[\"claire\"] = []string{\"tom\", \"johnny\"} graphMap[\"anuj\"] = []string{} graphMap[\"peggy\"] = []string{} graphMap[\"tom\"] = []string{} graphMap[\"johnny\"] = []string{} searchQueue := graphMap[\"you\"] for { if len(searchQueue) \u003e 0 { var person string person, searchQueue = searchQueue[0], searchQueue[1:] if personIsTom(person) { fmt.Printf(\"%s is the man\\n\", person) break } else { searchQueue = append(searchQueue, graphMap[person]...) } } else { fmt.Println(\"Not Found\") break } } } func personIsTom(p string) bool { return p == \"tom\" } 参考视频 ","date":"2023-06-10","objectID":"/golang-beradth-firstsearch/:2:0","tags":["golang","算法"],"title":"🔥Golang Beradth FirstSearch","uri":"/golang-beradth-firstsearch/"},{"categories":["文档"],"content":" golang-viper 介绍 viper 是一个配置解决方案，用于丰富的特性 支持 JSON/TOML/YAML/HCL/envfile/Java properties 等多种格式的配置文件 可以设置监听配置文件的修改，修改时自动加载新的配置； 从环境变量、命令行选项和io.Reader中读取配置 从远程配置系统中读取和监听修改，如 etcd/Consul； 代码逻辑中显示设置健值 ","date":"2023-06-09","objectID":"/golang-viper/:0:0","tags":["golang","go第三方库"],"title":"Golang Viper","uri":"/golang-viper/"},{"categories":["文档"],"content":"使用 安装 go get github.com/spf13/viper 使用 package main import ( \"fmt\" \"log\" \"github.com/spf13/viper\" ) func main() { viper.SetConfigName(\"config\") viper.SetConfigType(\"toml\") viper.AddConfigPath(\".\") viper.SetDefault(\"redis.port\", 6381) err := viper.ReadInConfig() if err != nil { log.Fatal(\"read config failed: %v\", err) } fmt.Println(viper.Get(\"app_name\")) fmt.Println(viper.Get(\"log_level\")) fmt.Println(\"mysql ip: \", viper.Get(\"mysql.ip\")) fmt.Println(\"mysql port: \", viper.Get(\"mysql.port\")) fmt.Println(\"mysql user: \", viper.Get(\"mysql.user\")) fmt.Println(\"mysql password: \", viper.Get(\"mysql.password\")) fmt.Println(\"mysql database: \", viper.Get(\"mysql.database\")) fmt.Println(\"redis ip: \", viper.Get(\"redis.ip\")) fmt.Println(\"redis port: \", viper.Get(\"redis.port\")) } viper的配置 SetConfigName： 设置文件名 SetConfigType： 配置类型 AddConfigPath： 搜索路径 ReadInConfig： 读取配置 viper.Get ： 获取键值 ⚠️注意 设置文件名时不要带后缀 搜索路径可以设置多个，viper会根据设置顺序依次查找 viper 获取值使用section.key的形式，即传入嵌套的键名 默认值可以调用 viper.SetDefault设置 ","date":"2023-06-09","objectID":"/golang-viper/:1:0","tags":["golang","go第三方库"],"title":"Golang Viper","uri":"/golang-viper/"},{"categories":["文档"],"content":"读取键 GetType 系列方法可以返回指定类型的值， type可以时Bool/Float64/Int/String/Time/Duration/IntSlice/StringSlice 如果指定的键不存在或者类型不正确，GetType方法返回对应类型时零值 判断某个键是否存在， 使用IsSet GetStringMap： 直接以 map 返回某个键下面所有的键值对 GetStringMapString： 返回map[string]string package main import ( \"fmt\" \"log\" \"github.com/spf13/viper\" ) func main() { viper.SetConfigName(\"config\") viper.SetConfigType(\"toml\") viper.AddConfigPath(\".\") err := viper.ReadInConfig() if err != nil { log.Fatalf(\"read config failed: %v\", err) } fmt.Println(\"protocols: \", viper.GetStringSlice(\"server.protocols\")) fmt.Println(\"ports: \", viper.GetIntSlice(\"server.ports\")) fmt.Println(\"timeout: \", viper.GetDuration(\"server.timeout\")) fmt.Println(\"mysql ip: \", viper.GetString(\"mysql.ip\")) fmt.Println(\"mysql port: \", viper.GetInt(\"mysql.port\")) if viper.IsSet(\"redis.port\") { fmt.Println(\"redis.port is set\") } else { fmt.Println(\"redis.port is not set\") } fmt.Println(\"mysql settings: \", viper.GetStringMap(\"mysql\")) fmt.Println(\"redis settings: \", viper.GetStringMap(\"redis\")) fmt.Println(\"all settings: \", viper.AllSettings()) } ","date":"2023-06-09","objectID":"/golang-viper/:2:0","tags":["golang","go第三方库"],"title":"Golang Viper","uri":"/golang-viper/"},{"categories":["文档"],"content":"设置键值 viper 支持在多个地方设置，使用下面的顺序依次读取 调用 Set 显示设置的 命令行选项； 环境变量 默认值 viper.Set 如果某个键通过 viper.set 设置了值， 这个值优先级最高 viper.Set(\"redis.port\", 5381) ","date":"2023-06-09","objectID":"/golang-viper/:3:0","tags":["golang","go第三方库"],"title":"Golang Viper","uri":"/golang-viper/"},{"categories":["文档"],"content":"命令行选项 viper使用pflag库来解析选项，首先在 init 方法中定义选项， 并且在viper.BindPFlags 绑定选项来配置中 func init() { pflag.Int(\"redis.port\", 8381, \"Redis port to connect\") // 绑定命令行 viper.BindPFlags(pflag.CommandLine) } ","date":"2023-06-09","objectID":"/golang-viper/:4:0","tags":["golang","go第三方库"],"title":"Golang Viper","uri":"/golang-viper/"},{"categories":["文档"],"content":"环境变量 在init方法中调用AutomaticEnv方法绑定全部环境变量 func init() { // 绑定环境变量 viper.AutomaticEnv() } 单独绑定环境变量 func init() { // 绑定环境变量 viper.BindEnv(\"redis.port\") viper.BindEnv(\"go.path\", \"GOPATH\") } func main() { // 省略部分代码 fmt.Println(\"go path: \", viper.Get(\"go.path\")) } 调用 BindEnv 方法 只传一个参数， 即表示键名，又表示环境变量名 传入两个参数， 第一个参数表示键名，第二个表示环境变量名 ","date":"2023-06-09","objectID":"/golang-viper/:5:0","tags":["golang","go第三方库"],"title":"Golang Viper","uri":"/golang-viper/"},{"categories":["文档"],"content":"读取配置 ","date":"2023-06-09","objectID":"/golang-viper/:6:0","tags":["golang","go第三方库"],"title":"Golang Viper","uri":"/golang-viper/"},{"categories":["文档"],"content":"从io.Reader 中读取 viper 支持从 io.Reader 中读取配置， 配置灵活 文件 程序中生成的字符串 从网络中链接中读取的字节流 package main import ( \"bytes\" \"fmt\" \"log\" \"github.com/spf13/viper\" ) func main() { viper.SetConfigType(\"toml\") tomlConfig := []byte(` app_name = \"awesome web\" # possible values: DEBUG, INFO, WARNING, ERROR, FATAL log_level = \"DEBUG\" [mysql] ip = \"127.0.0.1\" port = 3306 user = \"dj\" password = 123456 database = \"awesome\" [redis] ip = \"127.0.0.1\" port = 7381 `) err := viper.ReadConfig(bytes.NewBuffer(tomlConfig)) if err != nil { log.Fatal(\"read config failed: %v\", err) } fmt.Println(\"redis port: \", viper.GetInt(\"redis.port\")) } ","date":"2023-06-09","objectID":"/golang-viper/:6:1","tags":["golang","go第三方库"],"title":"Golang Viper","uri":"/golang-viper/"},{"categories":["文档"],"content":"Unmarshal viper 支持将配置Unmarshal到一个结构体中，为结构体中的对应字段赋值 package main import ( \"fmt\" \"log\" \"github.com/spf13/viper\" ) type Config struct { AppName string LogLevel string MySQL MySQLConfig Redis RedisConfig } type MySQLConfig struct { IP string Port int User string Password string Database string } type RedisConfig struct { IP string Port int } func main() { viper.SetConfigName(\"config\") viper.SetConfigType(\"toml\") viper.AddConfigPath(\".\") err := viper.ReadInConfig() if err != nil { log.Fatal(\"read config failed: %v\", err) } var c Config viper.Unmarshal(\u0026c) fmt.Println(c.MySQL) } ","date":"2023-06-09","objectID":"/golang-viper/:6:2","tags":["golang","go第三方库"],"title":"Golang Viper","uri":"/golang-viper/"},{"categories":["文档"],"content":"保存配置 将程序中生成的配置，或者所做的修改保存下来 WriteConfig： 将当前的viper配置写到预定义路径， 如果没有预定义路径，返回错误，将会覆盖当前配置 SafeWriteConfig： 与上面功能一样，如果配置文件存在，则不覆盖 WriteConfigAs： 保存配置到指定路径，如果文件存在，则覆盖 SafeWriteConfig： 与上面功能一样，但是入股配置文件存在，则不覆盖 package main import ( \"log\" \"github.com/spf13/viper\" ) func main() { viper.SetConfigName(\"config\") viper.SetConfigType(\"toml\") viper.AddConfigPath(\".\") viper.Set(\"app_name\", \"awesome web\") viper.Set(\"log_level\", \"DEBUG\") viper.Set(\"mysql.ip\", \"127.0.0.1\") viper.Set(\"mysql.port\", 3306) viper.Set(\"mysql.user\", \"root\") viper.Set(\"mysql.password\", \"123456\") viper.Set(\"mysql.database\", \"awesome\") viper.Set(\"redis.ip\", \"127.0.0.1\") viper.Set(\"redis.port\", 6381) err := viper.SafeWriteConfig() if err != nil { log.Fatal(\"write config failed: \", err) } } ","date":"2023-06-09","objectID":"/golang-viper/:7:0","tags":["golang","go第三方库"],"title":"Golang Viper","uri":"/golang-viper/"},{"categories":["文档"],"content":"监听文件修改 viper 可以监听文件修改， 热加载配置，不需要重启服务器， 让配置生效 package main import ( \"fmt\" \"log\" \"time\" \"github.com/spf13/viper\" ) func main() { viper.SetConfigName(\"config\") viper.SetConfigType(\"toml\") viper.AddConfigPath(\".\") err := viper.ReadInConfig() if err != nil { log.Fatal(\"read config failed: %v\", err) } viper.WatchConfig() fmt.Println(\"redis port before sleep: \", viper.Get(\"redis.port\")) time.Sleep(time.Second * 10) fmt.Println(\"redis port after sleep: \", viper.Get(\"redis.port\")) } viper.WatchConfig： viper 会自动监听配置修改，如果有修改，重新加载的配置 为配置修改增加一个回调 viper.OnConfigChange(func(e fsnotify.Event) { fmt.Printf(\"Config file:%s Op:%s\\n\", e.Name, e.Op) }) 参考链接 https://darjun.github.io/2020/01/18/godailylib/viper/ ","date":"2023-06-09","objectID":"/golang-viper/:8:0","tags":["golang","go第三方库"],"title":"Golang Viper","uri":"/golang-viper/"},{"categories":["文档"],"content":"# golang-cobra 介绍 cobra 是一个命令行程序库 提供了脚手架，用于生成基于cobra的应用程序框架 ","date":"2023-06-08","objectID":"/golang-cobra/:0:0","tags":["golang","go第三方库"],"title":"Golang Cobra","uri":"/golang-cobra/"},{"categories":["文档"],"content":"安装\u0026使用 安装 cobra go get github.com/spf13/cobra/cobra 实现一个git 模拟命令, 通过os/exec 库调用外部程序执行真实的git命令 目录结构如下 tree . ├── cmd │ ├── helper.go │ ├── root.go │ └── version.go └── main.go root.go package cmd import ( \"errors\" \"github.com/spf13/cobra\" ) var rootCmd = \u0026cobra.Command{ Use: \"git\", Short: \"Git is a distributed version control system.\", Long: `Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency.`, Run: func(cmd *cobra.Command, args []string) { Error(cmd, args, errors.New(\"unrecognized command\")) }, } func Execute() { rootCmd.Execute() } version.go package cmd import ( \"fmt\" \"os\" \"github.com/spf13/cobra\" ) var versionCmd = \u0026cobra.Command{ Use: \"version\", Short: \"version subcommand show git version info.\", Run: func(cmd *cobra.Command, args []string) { output, err := ExecuteCommand(\"git\", \"version\", args...) if err != nil { Error(cmd, args, err) } fmt.Fprint(os.Stdout, output) }, } func init() { rootCmd.AddCommand(versionCmd) } main.go package main import ( \"demo/dajun/cobra/cmd\" ) func main() { cmd.Execute() } helper.go 封装了调用外部程序和错误处理函数 package cmd import ( \"fmt\" \"os\" \"os/exec\" \"github.com/spf13/cobra\" ) func ExecuteCommand(name string, subname string, args ...string) (string, error) { args = append([]string{subname}, args...) cmd := exec.Command(name, args...) bytes, err := cmd.CombinedOutput() return string(bytes), err } func Error(cmd *cobra.Command, args []string, err error) { fmt.Fprintf(os.Stderr, \"execute %s args:%v error:%v\\n\", cmd.Name(), args, err) os.Exit(1) } 每个cobra 程序都有一个根命令，可以给他添加任意多个子命令， 在version.go init函数将子命令添加到跟命令中 不能直接go run main.go，这已经不是单文件程序了。如果强行要用，请使用go run . go build -o main cobra 自动生成的帮助信息 ./main -h Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. Usage: git [flags] git [command] Available Commands: completion Generate the autocompletion script for the specified shell help Help about any command version version subcommand show git version info. Flags: -h, --help help for git Use \"git [command] --help\" for more information about a command 单个子命令的帮助信息 ./main version -h version subcommand show git version info. Usage: git version [flags] Flags: -h, --help help for version 调用子命令 ./main version git version 2.39.2 (Apple Git-143) ","date":"2023-06-08","objectID":"/golang-cobra/:1:0","tags":["golang","go第三方库"],"title":"Golang Cobra","uri":"/golang-cobra/"},{"categories":["文档"],"content":"特性 cobra 提供非常丰富的功能 轻松支持子命令如 app server, app fetch等 完全兼容POSIX选项（包括短，长选项）； 嵌套子命令 全局/本地层级选项，可以在多处设置选项， 按照一定的顺序调用 使用脚手架轻松胜出程序框架和命令 ","date":"2023-06-08","objectID":"/golang-cobra/:2:0","tags":["golang","go第三方库"],"title":"Golang Cobra","uri":"/golang-cobra/"},{"categories":["文档"],"content":"基本概念 命令(Command): 需要执行的的操作。 参数(Arg) ： 命令的参数， 即要操作的对象。 选项(Flag)： 命令选项可以调整命令的行为。 ","date":"2023-06-08","objectID":"/golang-cobra/:2:1","tags":["golang","go第三方库"],"title":"Golang Cobra","uri":"/golang-cobra/"},{"categories":["文档"],"content":"命令 在cobra中， 命令和子命令都是用 Command 结构表示的，Command有非常多的字段，用来制定命令的行为， Use: 使用信息，即命令怎么被调用，Example: add [-F file | -D dir]... [-f format] profile Short\\Long：命令的帮助信息，前者是简短的，后者是详细的 Run： 实际执行的操作函数 定义新的子命令很简单， 创建一个cobra.Command变量， 设置一些字段，添加到根命令 添加一个clone package cmd import ( \"fmt\" \"os\" \"github.com/spf13/cobra\" ) var cloneCmd = \u0026cobra.Command { Use: \"clone url [destination]\", Short: \"Clone a repository into a new directory\", Run: func(cmd *cobra.Command, args []string) { output, err := ExecuteCommand(\"git\", \"clone\", args...) if err != nil { Error(cmd, args, err) } fmt.Fprintf(os.Stdout, output) }, } func init() { rootCmd.AddCommand(cloneCmd) } Use： clone url [destination] clone: 子命令 url： 参数 destination : 目标路径 ","date":"2023-06-08","objectID":"/golang-cobra/:3:0","tags":["golang","go第三方库"],"title":"Golang Cobra","uri":"/golang-cobra/"},{"categories":["文档"],"content":"选项 cobra 的选项分为两种 永久选项：定义的命令和其子命令都可以使用，通过给跟命令添加一个选项定义全局选项 本地选项：定义它 的命令中使用 cobra 使用pflag 解析命令行选项， 存储选项的变量需要提前定义好 var Verbose bool var Source string 设置永久选项 rootCmd.PersistentFlags().BoolVarP(\u0026Verbose, \"verbose\", \"v\", false, \"verbose output\") 设置本地选项 localCmd.Flags().StringVarP(\u0026Source, \"source\", \"s\", \"\", \"Source directory to read from\") 座一个计算机功能， 支持加、减、乘、除操作。并且可以通过选项设置是否忽略非数字参数，设置除 0 是否报错。 显然，前一个选项应该放在全局选项中，后一个应该放在除法命令中 程序结构 ├── cmd │ ├── add.go │ ├── divide.go │ ├── helper.go │ ├── minus.go │ ├── multiply.go │ └── root.go └── main.go devide.go package cmd import ( \"fmt\" \"strings\" \"github.com/spf13/cobra\" ) var ( dividedByZeroHanding int // 除 0 如何处理 ) var divideCmd = \u0026cobra.Command { Use: \"divide\", Short: \"Divide subcommand divide all passed args.\", Run: func(cmd *cobra.Command, args []string) { values := ConvertArgsToFloat64Slice(args, ErrorHandling(parseHandling)) result := calc(values, DIVIDE) fmt.Printf(\"%s = %.2f\\n\", strings.Join(args, \"/\"), result) }, } func init() { divideCmd.Flags().IntVarP(\u0026dividedByZeroHanding, \"divide_by_zero\", \"d\", int(PanicOnDividedByZero), \"do what when divided by zero\") rootCmd.AddCommand(divideCmd) } root.go var ( parseHandling int ) var rootCmd = \u0026cobra.Command { Use: \"math\", Short: \"Math calc the accumulative result.\", Run: func(cmd *cobra.Command, args []string) { Error(cmd, args, errors.New(\"unrecognized subcommand\")) }, } func init() { rootCmd.PersistentFlags().IntVarP(\u0026parseHandling, \"parse_error\", \"p\", int(ContinueOnParseError), \"do what when parse arg error\") } func Execute() { rootCmd.Execute() } 在divide.go中定义了如何处理除 0 错误的选项，在root.go中定义了如何处理解析错误的选项。选项枚举如下： const ( ContinueOnParseError ErrorHandling = 1 // 解析错误尝试继续处理 ExitOnParseError ErrorHandling = 2 // 解析错误程序停止 PanicOnParseError ErrorHandling = 3 // 解析错误 panic ReturnOnDividedByZero ErrorHandling = 4 // 除0返回 PanicOnDividedByZero ErrorHandling = 5 // 除0 painc ) 测试程序 $ go build -o math $ ./math add 1 2 3 4 1+2+3+4 = 10.00 $ ./math minus 1 2 3 4 1-2-3-4 = -8.00 $ ./math multiply 1 2 3 4 1*2*3*4 = 24.00 $ ./math divide 1 2 3 4 1/2/3/4 = 0.04 默认情况下， 解析错误被忽略， $ ./math add 1 2a 3b 4 1+2a+3b+4 = 5.00 $ ./math divide 1 2a 3b 4 1/2a/3b/4 = 0.25 设置解析失败的处理，2 表示退出程序，3 表示 panic ./math add 1 2a 3b 4 -p 2 invalid number: 2a $ ./math add 1 2a 3b 4 -p 3 panic: strconv.ParseFloat: parsing \"2a\": invalid syntax goroutine 1 [running]: ","date":"2023-06-08","objectID":"/golang-cobra/:4:0","tags":["golang","go第三方库"],"title":"Golang Cobra","uri":"/golang-cobra/"},{"categories":["文档"],"content":"脚手架 始化一个新的Go模块： 创建一个新目录 cd进入那个目录 run go mod init cd $HOME/code mkdir myapp cd myapp go mod init github.com/spf13/myapp cobra-cli init也可以从子目录运行 –author标志的作者姓名。例如cobra-cli init –author “Steve Francia spf@spf13.com” –license一起使用的许可证，例如cobra-cli init –license apache ","date":"2023-06-08","objectID":"/golang-cobra/:5:0","tags":["golang","go第三方库"],"title":"Golang Cobra","uri":"/golang-cobra/"},{"categories":["文档"],"content":"给项目添加命令 cobra-cli add serve cobra-cli add config cobra-cli add create -p 'configCmd' 运行了这三个命令，你就会有一个类似于以下内容的应用程序结构： ▾ app/ ▾ cmd/ config.go create.go serve.go root.go main.go 参考链接 https://github.com/spf13/cobra-cli https://darjun.github.io/2020/01/17/godailylib/cobra/ ","date":"2023-06-08","objectID":"/golang-cobra/:5:1","tags":["golang","go第三方库"],"title":"Golang Cobra","uri":"/golang-cobra/"},{"categories":["文档"],"content":" 🔥Golang算法 - 快速排序（Quicksort） 不需要排序的数组（Base case 基线条件） [],空数组 [s], 单元素数组 容易排序的数组 [a,b],两个元素的数组，只需坚持它们之间的大小即可，调整位置 ","date":"2023-06-08","objectID":"/golang-quicksort/:0:0","tags":["golang","算法"],"title":"🔥Golang Quicksort","uri":"/golang-quicksort/"},{"categories":["文档"],"content":"使用Quicksort 排序数组 3个元素的数组（例如[23, 19, 35]） 使用D \u0026 C策略， 简化为基线条件（Base case） 从数组中随便找一个元素， 例如35， 这个元素叫做pivot(基准元素) 找到比pivot小的元素，找到比pivot大的元素， 这叫做分区：[23, 19],(35),[] 如果左右两个子数组已排好序（达到基线条件）， 结果： 左边 + [pivot] + 右边 如果左右两个子数组没有已排好序（没有达到基线条件）， 那么： quiksort左边 + [pivot] + quicksort右边 ","date":"2023-06-08","objectID":"/golang-quicksort/:1:0","tags":["golang","算法"],"title":"🔥Golang Quicksort","uri":"/golang-quicksort/"},{"categories":["文档"],"content":"使用Quicksourt 排序数组的步骤 选择一个pivot 将数组分为两个子数组 左侧数组的元素都比pivot小 右侧数组的元素都比pivot大 在两个子数组上递归的调用quicksort 代码示例01 package main import \"fmt\" func main() { arr := []int{12, 87, 62, 66, 30, 12, 328, 121, 67, 98, 3, 256, 81} result := quicksort(arr) fmt.Println(result) } func quicksort(arr []int) []int { if len(arr) \u003c 2 { return arr } pivot := arr[0] var left, right []int for _, ele := range arr[1:] { if ele \u003c= pivot { left = append(left, ele) } else { right = append(right, ele) } } return append(quicksort(left), append([]int{pivot}, quicksort(right)...)...) } 参考视频 ","date":"2023-06-08","objectID":"/golang-quicksort/:2:0","tags":["golang","算法"],"title":"🔥Golang Quicksort","uri":"/golang-quicksort/"},{"categories":["文档"],"content":" 🔥Golang算法 - D\u0026C 算法 Devide \u0026 Conquer D \u0026 C 的步骤 找到一个简单的基线条件（Base Case） 把问题分开处理。直到它变为极限条件 ","date":"2023-06-07","objectID":"/golang-divide_conquer/:0:0","tags":["golang","算法"],"title":"🔥Golang Divide_Conquer","uri":"/golang-divide_conquer/"},{"categories":["文档"],"content":"例子 需求： 将数组[1,3,5,7,9]求和 思路1： 使用循环（例如for循环） 思路2: D \u0026 C策略 基线条件： 空数组[], 其和为0， 递归：[1,3,5,7,9] 1 + SUM([3,5,7,9]) 3 + SUM([5,7,9]) 5 + SUM([7,9]) 7 + SUM([9]) 9 + SUM([]) ","date":"2023-06-07","objectID":"/golang-divide_conquer/:1:0","tags":["golang","算法"],"title":"🔥Golang Divide_Conquer","uri":"/golang-divide_conquer/"},{"categories":["文档"],"content":"代码示例 package main import \"fmt\" func main() { totals := sum([]int{1, 3, 5, 7, 9}) fmt.Println(totals) } func sum(arr []int) int { if len(arr) == 0 { return 0 } return arr[0] + sum(arr[1:]) } 参考视频 ","date":"2023-06-07","objectID":"/golang-divide_conquer/:2:0","tags":["golang","算法"],"title":"🔥Golang Divide_Conquer","uri":"/golang-divide_conquer/"},{"categories":["文档"],"content":" git tag (figure) 🚀git tag 标签管理 可以在版本库中打一个标签，确定打标签的版本 标签就是版本库的快照 ","date":"2023-06-06","objectID":"/git-tag/:0:0","tags":["git"],"title":"Git Tag","uri":"/git-tag/"},{"categories":["文档"],"content":"创建标签 切换到需要打标签的分支上 $ git branch * dev master $ git checkout master Switched to branch 'master' 打一个新标签 git tag v1.0 查看所有标签 git tag v1.0 默认标签是打在最新提交的commit上 找到历史提交的commit id，然后打 git log --pretty=oneline --abbrev-commit 12a631b (HEAD -\u003e master, tag: v1.0, origin/master) merged bug fix 101 4c805e2 fix bug 101 e1e9c68 merge with no-ff f52c633 add merge cf810e4 conflict fixed 要对add merge这次提交打标签 git tag v0.9 f52c633 再次查看标签 git tag v0.9 v1.0 标签不是按时间顺序列出，而是按字母排序的。可以用git show 查看标签信息 git show v0.9 commit f52c63349bc3c1593499807e5c8e972b82c8f286 (tag: v0.9) Author: Michael Liao \u003caskxuefeng@gmail.com\u003e Date: Fri May 18 21:56:54 2018 +0800 add merge diff --git a/readme.txt b/readme.txt 可以创建带有说明的标签，用-a指定标签名，-m指定说明文字 git tag -a v0.1 -m \"version 0.1 released\" 1094adb ","date":"2023-06-06","objectID":"/git-tag/:1:0","tags":["git"],"title":"Git Tag","uri":"/git-tag/"},{"categories":["文档"],"content":"操作标签 删除标签 git tag -d v0.1 推送某个标签到远程 git push origin v1.0 推送所有标签 git push origin --tags 删除远程标签 ## 本地删除标签 git tag -d v0.9 ## 远程删除 git push origin :refs/tags/v0.9 ","date":"2023-06-06","objectID":"/git-tag/:2:0","tags":["git"],"title":"Git Tag","uri":"/git-tag/"},{"categories":["文档"],"content":"命令总结 git tag 用于新建一个标签，默认为HEAD，也可以指定一个commit id； git tag -a -m “blablabla…“可以指定标签信息； git tag可以查看所有标签 git push origin 可以推送一个本地标签； git push origin –tags可以推送全部未推送过的本地标签； git tag -d 可以删除一个本地标签； git push origin :refs/tags/可以删除一个远程标签 参考链接 https://www.liaoxuefeng.com/wiki/896043488029600/900788941487552 ","date":"2023-06-06","objectID":"/git-tag/:3:0","tags":["git"],"title":"Git Tag","uri":"/git-tag/"},{"categories":["文档"],"content":" git (figure) 🚀git 常用命令和技巧 列举一些常用的git命令和一些小技巧 统一概念 工作区： 该动（增删文件和内容） 暂存区： 输入命令，git add 改动的文件名，此次改动就放到了 ‘暂存区’ 本地仓库： 输入命令：git commit 此次修改的描述，此次改动就放到了本地仓库，每个 commit，我叫它为一个版本 远程仓库： 输入命令：git push 远程仓库，此次改动就放到了远程仓库（GitHub 等) commit-id：输出命令：git log，最上面那行 commit xxxxxx，后面的字符串就是 commit-id ","date":"2023-06-03","objectID":"/git-skill/:0:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"git 基础介绍 创建新仓库 # 创建新文件夹， 打开， 然后执行 git init 检出仓库 # 执行命令以创建一个本地仓库的克隆版本 git clone /path/to/repository # 远端服务器的仓库 git clone username@host:/path/to/repository 工作流 本地仓库有git维护三颗树组成 工作目录： 持有实际的文件 暂存区 ： 缓存区域，临时保存你的改动 HEAD： 指向你最后一次提交的结果 添加和提交 ## 提出更改（把它们添加到暂存区） git add \u003cfilename\u003e git add * ## 改动已经提交到了 HEAD git commit -m \"代码提交信息\" 推送改动 ## 将改动提交到 远端仓库 git push origin master ## 将你的仓库连接到某个远程服务器，你可以使用如下命令添加 git remote add origin \u003cserver\u003e 分支 ## 创建一个分支 git checkout -b feature_x ## 切换回主分支 git checkout master ## 把新建的分支删除 git branch -d feature_x ## 除非你将分支推送到远端仓库，不然该分支就是 不为他人所见的 git push origin \u003cbranch\u003e 更新与合并 ## 更新你的本地仓库至最新改动 git pull ## 合并其他分支到你的当前分支 git merge \u003cbranch\u003e ## 在这两种情况下，git 都会尝试去自动合并改动。遗憾的是，这可能并非每次都成功，并可能出现冲突（conflicts）。 这时候就需要你修改这些文件来手动合并这些冲突（conflicts）。改完之后，你需要执行如下命令以将它们标记为合并成功 git add \u003cfilename\u003e ## 在合并改动之前，你可以使用如下命令预览差异 git diff \u003csource_branch\u003e \u003ctarget_branch\u003e 标签 ## 创建一个 1.00 的标签 git tag 1.0.0 1b2e1d63ff ## 1b2e1d63ff 是你想要标记的提交 ID 的前 10 位字符。可以使用下列命令获取提交 ID git log log ## 本地仓库的历史记录 git log ## 只看某人的提交记录 git log --author=bob ## 一个压缩后的每一条提交记录只占一行输出 git log --pretty=oneline ## 通过 ASCII 艺术的树形结构来展示所有的分支 git log --graph --oneline --decorate --all ## 查看那些文件改变了 git log --name-status 替换本地改动 ## 替换掉本地的改动 git checkout -- \u003cfilename\u003e ## 丢弃本地所有的改动和提交，可以到服务器上获取最新的版本历史，并将你本地主分支指向它 git fetch origin git reset --hard origin/master ","date":"2023-06-03","objectID":"/git-skill/:1:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"回到远程仓库的状态 抛弃本地所有的修改，回到远程仓库的状态的 git fetch --all \u0026\u0026 git reset --hard origin/master ","date":"2023-06-03","objectID":"/git-skill/:2:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"重设第一个 commit 把所有的修改都重新放回工作区， 并清空所有的 commit， 这样可以重新提交第一个commit git update-ref -d HEAD ","date":"2023-06-03","objectID":"/git-skill/:3:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"查看冲突文件列表 展示工作区的冲突文件列表 git diff --name-only --diff-filter=U ","date":"2023-06-03","objectID":"/git-skill/:4:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"展示工作区和暂存区的不同 ## 输出**工作区**和**暂存区**的 different git diif ## 还可以展示本地仓库中任意两个 commit 之间的文件变动 git diff \u003ccommit-id\u003e \u003ccommit-id\u003e ","date":"2023-06-03","objectID":"/git-skill/:5:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"展示暂存区和最近版本的不同 ## 输出暂存区和本地最近的版本 (commit) 的 different (不同) git diff --cached ","date":"2023-06-03","objectID":"/git-skill/:6:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"快速切换到上一个分支 git checkout - ","date":"2023-06-03","objectID":"/git-skill/:7:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"删除已经合并到 master 的分支 git branch --merged master | grep -v '^\\*\\| master' | xargs -n 1 git branch -d ","date":"2023-06-03","objectID":"/git-skill/:8:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"展示本地分支关联远程仓库的情况 git branch -vv ","date":"2023-06-03","objectID":"/git-skill/:9:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"关联远程分支 ## 关联之后，git branch -vv 就可以展示关联的远程分支名了，同时推送到远程仓库直接：git push，不需要指定远程仓库 git branch -u origin/mybranch ## 或者在 push 时加上 -u 参数 git push origin/mybranch -u ","date":"2023-06-03","objectID":"/git-skill/:10:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"列出所有的远程分支 ## -r 参数相当于：remote git branch -r ","date":"2023-06-03","objectID":"/git-skill/:11:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"列出本地和远程分支 ## -a 参数相当于：all git branch -a ","date":"2023-06-03","objectID":"/git-skill/:12:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"查看远程分支和本地分支的对应关系 git remote show origin ","date":"2023-06-03","objectID":"/git-skill/:13:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"远程删除了分支本地也想删除 git remote prune origin ","date":"2023-06-03","objectID":"/git-skill/:14:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"创建并切换到本地分支 git checkout -b \u003cbranch-name\u003e ","date":"2023-06-03","objectID":"/git-skill/:15:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"从远程分支中创建并切换到本地分支 git checkout -b \u003cbranch-name\u003e origin/\u003cbranch-name\u003e ","date":"2023-06-03","objectID":"/git-skill/:16:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"删除本地分支 git branch -d \u003clocal-branchname\u003e ","date":"2023-06-03","objectID":"/git-skill/:17:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"删除远程分支 git push origin --delete \u003cremote-branchname\u003e ## 或者 git push origin :\u003cremote-branchname\u003e ","date":"2023-06-03","objectID":"/git-skill/:18:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"重命名本地分支 git branch -m \u003cnew-branch-name\u003e ","date":"2023-06-03","objectID":"/git-skill/:19:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"查看标签 git tag ## 展示当前分支的最近的 tag git describe --tags --abbrev=0 ","date":"2023-06-03","objectID":"/git-skill/:20:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"查看标签的详细信息 git tag -ln ","date":"2023-06-03","objectID":"/git-skill/:21:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"本地创建标签 git tag \u003cversion-number\u003e ## 默认 tag 是打在最近的一次 commit 上，如果需要指定 commit 打 tag： $ git tag -a \u003cversion-number\u003e -m \"v1.0 发布(描述)\" \u003ccommit-id\u003e ","date":"2023-06-03","objectID":"/git-skill/:22:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"推送标签到远程仓库 ## 首先要保证本地创建好了标签才可以推送标签到远程仓库： git push origin \u003clocal-version-number\u003e ## 一次性推送所有标签，同步到远程仓库： git push origin --tags ","date":"2023-06-03","objectID":"/git-skill/:23:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"删除本地标签 git tag -d \u003ctag-name\u003e ","date":"2023-06-03","objectID":"/git-skill/:24:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"删除远程标签 git push origin --delete tag \u003ctagname\u003e ","date":"2023-06-03","objectID":"/git-skill/:25:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"切回到某个标签 ## 一般上线之前都会打 tag，就是为了防止上线后出现问题，方便快速回退到上一版本。下面的命令是回到某一标签下的状态 git checkout -b branch_name tag_name ","date":"2023-06-03","objectID":"/git-skill/:26:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"放弃工作区的修改 git checkout \u003cfile-name\u003e ## 或者放弃所有修改 git checkout . ","date":"2023-06-03","objectID":"/git-skill/:27:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"恢复删除的文件 #得到 deleting_commit git rev-list -n 1 HEAD -- \u003cfile_path\u003e #回到删除文件 deleting_commit 之前的状态 git checkout \u003cdeleting_commit\u003e^ -- \u003cfile_path\u003e ","date":"2023-06-03","objectID":"/git-skill/:28:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"以新增一个 commit 的方式还原某一个 commit 的修改 git revert \u003ccommit-id\u003e ","date":"2023-06-03","objectID":"/git-skill/:29:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"回到某个 commit 的状态，并删除后面的 commit ## 和 revert 的区别：reset 命令会抹去某个 commit id 之后的所有 commit git reset \u003ccommit-id\u003e #默认就是-mixed参数。 git reset --mixed HEAD^ #回退至上个版本，它将重置HEAD到另外一个commit,并且重置暂存区以便和HEAD相匹配，但是也到此为止。工作区不会被更改。 git reset --soft HEAD~3 #回退至三个版本之前，只回退了commit的信息，暂存区和工作区与回退之前保持一致。如果还要提交，直接commit即可 git reset --hard \u003ccommit-id\u003e #彻底回退到指定commit-id的状态，暂存区和工作区也会变为指定commit-id版本的内容 ","date":"2023-06-03","objectID":"/git-skill/:30:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"修改上一个 commit 的描述 ## 如果暂存区有改动，同时也会将暂存区的改动提交到上一个 commit git commit --amend ","date":"2023-06-03","objectID":"/git-skill/:31:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"查看 commit 历史 git log ","date":"2023-06-03","objectID":"/git-skill/:32:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"查看某段代码是谁写的 ## blame 的意思为‘责怪’，你懂的。 git blame \u003cfile-name\u003e ","date":"2023-06-03","objectID":"/git-skill/:33:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"显示本地更新过 HEAD 的 git 命令记录 ## 每次更新了 HEAD 的 git 命令比如 commit、amend、cherry-pick、reset、revert 等都会被记录下来（不限分支），就像 shell 的 history 一样。 这样你可以 reset 到任何一次更新了 HEAD 的操作之后，而不仅仅是回到当前分支下的某个 commit 之后的状态 git reflog ","date":"2023-06-03","objectID":"/git-skill/:34:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"修改作者名 git commit --amend --author='Author Name \u003cemail@address.com\u003e' ","date":"2023-06-03","objectID":"/git-skill/:35:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"修改远程仓库的 url git remote set-url origin \u003cURL\u003e ","date":"2023-06-03","objectID":"/git-skill/:36:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"增加远程仓库 git remote add origin \u003cremote-url\u003e ","date":"2023-06-03","objectID":"/git-skill/:37:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"列出所有远程仓库 git remote ","date":"2023-06-03","objectID":"/git-skill/:38:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"查看两个星期内的改动 git whatchanged --since='2 weeks ago' ","date":"2023-06-03","objectID":"/git-skill/:39:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"把 A 分支的某一个 commit，放到 B 分支上 ## 这个过程需要 cherry-pick 命令 git checkout \u003cbranch-name\u003e \u0026\u0026 git cherry-pick \u003ccommit-id\u003e ","date":"2023-06-03","objectID":"/git-skill/:40:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"给 git 命令起别名 git config --global alias.\u003chandle\u003e \u003ccommand\u003e 比如：git status 改成 git st，这样可以简化命令 git config --global alias.st status ","date":"2023-06-03","objectID":"/git-skill/:41:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"存储当前的修改，但不用提交 commit 详解可以参考廖雪峰老师的 git 教程 https://www.liaoxuefeng.com/wiki/896043488029600/900388704535136 git stash ","date":"2023-06-03","objectID":"/git-skill/:42:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"保存当前状态，包括 untracked 的文件 ## untracked 文件：新建的文件 git stash -u ## 展示所有 stashes git stash list ## 回到某个 stash 的状态 git stash apply \u003cstash@{n}\u003e ## 回到最后一个 stash 的状态，并删除这个 stash git stash pop ## 删除所有的 stash git stash clear ## 从 stash 中拿出某个文件的修改 git checkout \u003cstash@{n}\u003e -- \u003cfile-path\u003e ## 展示所有 tracked 的文件 git ls-files -t ## 展示所有 untracked 的文件 git ls-files --others ","date":"2023-06-03","objectID":"/git-skill/:43:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"展示所有忽略的文件 git ls-files --others -i --exclude-standard ","date":"2023-06-03","objectID":"/git-skill/:44:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"展示简化的 commit 历史 git log --pretty=oneline --graph --decorate --all ","date":"2023-06-03","objectID":"/git-skill/:45:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"把某一个分支导出成一个文件 git bundle create \u003cfile\u003e \u003cbranch-name\u003e ","date":"2023-06-03","objectID":"/git-skill/:46:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"从包中导入分支 ## 新建一个分支，分支内容就是上面 git bundle create 命令导出的内容 git clone repo.bundle \u003crepo-dir\u003e -b \u003cbranch-name\u003e ","date":"2023-06-03","objectID":"/git-skill/:47:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"从远程仓库根据 ID，拉下某一状态，到本地分支 git fetch origin pull/\u003cid\u003e/head:\u003cbranch-name\u003e ","date":"2023-06-03","objectID":"/git-skill/:48:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"清除 gitignore 文件中记录的文件 git clean -X -f ","date":"2023-06-03","objectID":"/git-skill/:49:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"展示所有 alias 和 configs ## 注意： config 分为：当前目录（local）和全局（golbal）的 config，默认为当前目录的 config git config --local --list (当前目录) git config --global --list (全局) ","date":"2023-06-03","objectID":"/git-skill/:50:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"展示忽略的文件 git status --ignored ","date":"2023-06-03","objectID":"/git-skill/:51:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"删除全局设置 git config --global --unset \u003centry-name\u003e ","date":"2023-06-03","objectID":"/git-skill/:52:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"新建并切换到新分支上，同时这个分支没有任何 commit ## 相当于保存修改，但是重写 commit 历史 git checkout --orphan \u003cbranch-name\u003e ","date":"2023-06-03","objectID":"/git-skill/:53:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"git checkout –orphan git show \u003cbranch-name\u003e:\u003cfile-name\u003e ","date":"2023-06-03","objectID":"/git-skill/:54:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"clone 下来指定的单一分支 git clone -b \u003cbranch-name\u003e --single-branch https://github.com/user/repo.git ","date":"2023-06-03","objectID":"/git-skill/:55:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"忽略文件的权限变化 ## 不再将文件的权限变化视作改动 git config core.fileMode false ","date":"2023-06-03","objectID":"/git-skill/:56:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"在 commit log 中查找相关内容 ## 通过 grep 查找，given-text：所需要查找的字段 git log --all --grep='\u003cgiven-text\u003e' ","date":"2023-06-03","objectID":"/git-skill/:57:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"把暂存区的指定 file 放到工作区中 ## 不添加参数，默认是 -mixed git reset \u003cfile-name\u003e ","date":"2023-06-03","objectID":"/git-skill/:58:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":"强制推送 git push -f \u003cremote-name\u003e \u003cbranch-name\u003e ","date":"2023-06-03","objectID":"/git-skill/:59:0","tags":["git"],"title":"Git Skill","uri":"/git-skill/"},{"categories":["文档"],"content":" go-work (figure) 🔥Golang算法 - 递归 递归： 在运行的过程中调用自己 ","date":"2023-05-30","objectID":"/golang-recursion01/:0:0","tags":["golang","算法"],"title":"🔥Golang Recursion","uri":"/golang-recursion01/"},{"categories":["文档"],"content":"递归函数 关键点 在同一函数调用的函数 在递归函数中指定一个退出条件总是好的 它可以检查不必要的函数调用和代码行 递归的缺点 代码逻辑性强， 难以调试和代码检查 go-work (figure) 示例 package main import \"fmt\" func main() { doll := Item{ ID: 1, Type: \"doll\", Child: \u0026Item{ ID: 2, Type: \"doll\", Child: \u0026Item{ ID: 3, Type: \"doll\", Child: \u0026Item{ ID: 4, Type: \"diamond\", Child: nil, }, }, }, } diamond := findDiamond(doll) fmt.Printf(\"Item %d is diamond\\n\", diamond.ID) } func findDiamond(item Item) Item { if item.IsDoll() { return findDiamond(*item.Child) } else { return item } } type Item struct { ID int Type string Child *Item } type ItemClassifier interface { IsDoll() bool } func (it *Item) IsDoll() bool { if it.Type == \"doll\" { return true } return false } 输出结果 go run main.go Item 4 is diamond 示例二 （递归函数实现斐波那契数） package main import \"fmt\" func fibonacci(n int) int { if n \u003c 2 { return n } return fibonacci(n-2) + fibonacci(n-1) } func main() { var i int for i = 0; i \u003c 10; i++ { fmt.Printf(\"%d\\t\", fibonacci(i)) } } ","date":"2023-05-30","objectID":"/golang-recursion01/:1:0","tags":["golang","算法"],"title":"🔥Golang Recursion","uri":"/golang-recursion01/"},{"categories":["文档"],"content":" go-work (figure) Golang context 实现原理 context的主要功能 异步场景中用于实现并发协调 对goroutine 的生命周期控制 有一定的数据存储能力 context 的数据结构 type Context interface { Deadline() (deadline time.Time, ok bool) Done() \u003c-chan struct{} Err() error Value(key any) any } Context 为interface， 定义了四个核心api Deadline： 返回context的过期时间 Done： 返回context中的channel Err： 返回错误 value： 返回context中的对应的key值 ","date":"2023-05-29","objectID":"/golang-context/:0:0","tags":["golang"],"title":"Golang Context 底层原理","uri":"/golang-context/"},{"categories":["文档"],"content":"标准 error var Canceled = errors.New(\"context canceled\") var DeadlineExceeded error = deadlineExceededError{} type deadlineExceededError struct{} func (deadlineExceededError) Error() string { return \"context deadline exceeded\" } func (deadlineExceededError) Timeout() bool { return true } func (deadlineExceededError) Temporary() bool { return true canceled： context 被canel时会报此错误 DeadlineExceeded: context 超时时会报此错误 ","date":"2023-05-29","objectID":"/golang-context/:0:1","tags":["golang"],"title":"Golang Context 底层原理","uri":"/golang-context/"},{"categories":["文档"],"content":"emptyCtx 类的实现 type emptyCtx int func (*emptyCtx) Deadline() (deadline time.Time, ok bool) { return } func (*emptyCtx) Done() \u003c-chan struct{} { return nil } func (*emptyCtx) Err() error { return nil } func (*emptyCtx) Value(key any) any { return } emptyCtx时一个空的context， 本质上类型为一个整型 Deadline 方法会返回一个公元元年时间以及 false 的 flag，标识当前 context 不存在过期时间； Done： 方法返回一个nil 值，用户无论在nil中写入或者读取数据，都会陷入阻塞 Err： 方法返回的错误永远为nil Value： 方法返回的value同样永远为nil ","date":"2023-05-29","objectID":"/golang-context/:0:2","tags":["golang"],"title":"Golang Context 底层原理","uri":"/golang-context/"},{"categories":["文档"],"content":"context.Background() \u0026 context.TODO() var ( background = new(emptyCtx) todo = new(emptyCtx) ) func Background() Context { return background } func TODO() Context { return todo } 常用的 context.Background() 和 context.TODO() 方法返回的均是 emptyCtx 类型的一个实例. ","date":"2023-05-29","objectID":"/golang-context/:0:3","tags":["golang"],"title":"Golang Context 底层原理","uri":"/golang-context/"},{"categories":["文档"],"content":"cancelCtx 数据结构 go-work (figure) type cancelCtx struct { Context mu sync.Mutex // protects following fields done atomic.Value // of chan struct{}, created lazily, closed by first cancel call children map[canceler]struct{} // set to nil by the first cancel call err error // set to non-nil by the first cancel call } type canceler interface { cancel(removeFromParent bool, err error) Done() \u003c-chan struct{} } embed 了context作为其父context， cancelCtx必然为某个context的子context 内置了一把锁， 用以协调并发场景下的资源获取 done： 实际类型为chan struct{}, 即用以反映 cancelCtx生命周期的通道 children： 一个set 指cancelCtx 的所有子context err： 记录了当前cancelCtx的错误， 必然为某一个context的子context ","date":"2023-05-29","objectID":"/golang-context/:1:0","tags":["golang"],"title":"Golang Context 底层原理","uri":"/golang-context/"},{"categories":["文档"],"content":"Deadline 方法 cancelCtx 未实现该方法， 仅时embed了一个带有 Deadline 方法的Context interface 因此直接 调用会报错 ","date":"2023-05-29","objectID":"/golang-context/:1:1","tags":["golang"],"title":"Golang Context 底层原理","uri":"/golang-context/"},{"categories":["文档"],"content":"Done 方法 go-work (figure) func (c *cancelCtx) Done() \u003c-chan struct{} { d := c.done.Load() if d != nil { return d.(chan struct{}) } c.mu.Lock() defer c.mu.Unlock() d = c.done.Load() if d == nil { d = make(chan struct{}) c.done.Store(d) } return d.(chan struct{}) } 基于atomic 包， 读取 cancelCtx中的chan； 倘若已存在，则直接返回 加锁后， 在此检查chan是否存在， 若存在则返回， 初始化 chan 存储到 aotmic.Value 当中，并返回.（懒加载机制） ","date":"2023-05-29","objectID":"/golang-context/:1:2","tags":["golang"],"title":"Golang Context 底层原理","uri":"/golang-context/"},{"categories":["文档"],"content":"Err 方法 func (c *cancelCtx) Err() error { c.mu.Lock() err := c.err c.mu.Unlock() return err } 加锁； 读取 cancelCtx.err； 解锁 返回结果 ","date":"2023-05-29","objectID":"/golang-context/:1:3","tags":["golang"],"title":"Golang Context 底层原理","uri":"/golang-context/"},{"categories":["文档"],"content":"Value方法 func (c *cancelCtx) Value(key any) any { if key == \u0026cancelCtxKey { return c } return value(c.Context, key) } 倘若 key 特定值 \u0026cancelCtxKey，则返回 cancelCtx 自身的指针； 否则遵循 valueCtx 的思路取值返回 ","date":"2023-05-29","objectID":"/golang-context/:1:4","tags":["golang"],"title":"Golang Context 底层原理","uri":"/golang-context/"},{"categories":["文档"],"content":"context.WithCancel() func WithCancel(parent Context) (ctx Context, cancel CancelFunc) { if parent == nil { panic(\"cannot create context from nil parent\") } c := newCancelCtx(parent) propagateCancel(parent, \u0026c) return \u0026c, func() { c.cancel(true, Canceled) } } 校验父 context 非空； 注入父 context 构造好一个新的 cancelCtx； 在 propagateCancel 方法内启动一个守护协程，以保证父 context 终止时，该 cancelCtx 也会被终止； 将 cancelCtx 返回，连带返回一个用以终止该 cancelCtx 的闭包函数. ","date":"2023-05-29","objectID":"/golang-context/:2:0","tags":["golang"],"title":"Golang Context 底层原理","uri":"/golang-context/"},{"categories":["文档"],"content":"newCancelCtx func newCancelCtx(parent Context) cancelCtx { return cancelCtx{Context: parent} } ","date":"2023-05-29","objectID":"/golang-context/:2:1","tags":["golang"],"title":"Golang Context 底层原理","uri":"/golang-context/"},{"categories":["文档"],"content":"propagateCancel go-work (figure) func propagateCancel(parent Context, child canceler) { done := parent.Done() if done == nil { return // parent is never canceled } select { case \u003c-done: // parent is already canceled child.cancel(false, parent.Err()) return default: } if p, ok := parentCancelCtx(parent); ok { p.mu.Lock() if p.err != nil { // parent has already been canceled child.cancel(false, p.err) } else { if p.children == nil { p.children = make(map[canceler]struct{}) } p.children[child] = struct{}{} } p.mu.Unlock() } else { atomic.AddInt32(\u0026goroutines, +1) go func() { select { case \u003c-parent.Done(): child.cancel(false, parent.Err()) case \u003c-child.Done(): } }() } } propagateCancel 用以传递父子 context 之间的 cancel 事件： • 倘若 parent 是不会被 cancel 的类型（如 emptyCtx），则直接返回； • 倘若 parent 已经被 cancel，则直接终止子 context，并以 parent 的 err 作为子 context 的 err； • 假如 parent 是 cancelCtx 的类型，则加锁，并将子 context 添加到 parent 的 children map 当中； • 假如 parent 不是 cancelCtx 类型，但又存在 cancel 的能力（比如用户自定义实现的 context），则启动一个协程，通过多路复用的方式监控 parent 状态，倘若其终止，则同时终止子 context，并透传 parent 的 err. ","date":"2023-05-29","objectID":"/golang-context/:2:2","tags":["golang"],"title":"Golang Context 底层原理","uri":"/golang-context/"},{"categories":["文档"],"content":"cancelCtx.cancel go-work (figure) func (c *cancelCtx) cancel(removeFromParent bool, err error) { if err == nil { panic(\"context: internal error: missing cancel error\") } c.mu.Lock() if c.err != nil { c.mu.Unlock() return // already canceled } c.err = err d, _ := c.done.Load().(chan struct{}) if d == nil { c.done.Store(closedchan) } else { close(d) } for child := range c.children { // NOTE: acquiring the child's lock while holding parent's lock. child.cancel(false, err) } c.children = nil c.mu.Unlock() if removeFromParent { removeChild(c.Context, c) } } cancelCtx.cancel 方法有两个入参，第一个 removeFromParent 是一个 bool 值，表示当前 context 是否需要从父 context 的 children set 中删除；第二个 err 则是 cancel 后需要展示的错误； 进入方法主体，首先校验传入的 err 是否为空，若为空则 panic； 加锁； 校验 cancelCtx 自带的 err 是否已经非空，若非空说明已被 cancel，则解锁返回； 将传入的 err 赋给 cancelCtx.err； 处理 cancelCtx 的 channel，若 channel 此前未初始化，则直接注入一个 closedChan，否则关闭该 channel； 遍历当前 cancelCtx 的 children set，依次将 children context 都进行 cancel； 解锁. 根据传入的 removeFromParent flag 判断是否需要手动把 cancelCtx 从 parent 的 children set 中移除. removeChild 方法中，观察如何将 cancelCtx 从 parent 的 children set 中移除： func removeChild(parent Context, child canceler) { p, ok := parentCancelCtx(parent) if !ok { return } p.mu.Lock() if p.children != nil { delete(p.children, child) } p.mu.Unlock() } 如果 parent 不是 cancelCtx，直接返回（因为只有 cancelCtx 才有 children set） 加锁； 从 parent 的 children set 中删除对应 child 解锁返回. ","date":"2023-05-29","objectID":"/golang-context/:3:0","tags":["golang"],"title":"Golang Context 底层原理","uri":"/golang-context/"},{"categories":["文档"],"content":"timerCtx类 go-work (figure) type timerCtx struct { cancelCtx timer *time.Timer // Under cancelCtx.mu. deadline time.Time } timerCtx 在 cancelCtx 基础上又做了一层封装，除了继承 cancelCtx 的能力之外，新增了一个 time.Timer 用于定时终止 context；另外新增了一个 deadline 字段用于字段 timerCtx 的过期时间 ","date":"2023-05-29","objectID":"/golang-context/:4:0","tags":["golang"],"title":"Golang Context 底层原理","uri":"/golang-context/"},{"categories":["文档"],"content":"valueCtx go-work (figure) type valueCtx struct { Context key, val any } valueCtx 同样继承了一个 parent context； 一个 valueCtx 中仅有一组 kv 对 ","date":"2023-05-29","objectID":"/golang-context/:5:0","tags":["golang"],"title":"Golang Context 底层原理","uri":"/golang-context/"},{"categories":["文档"],"content":"valueCtx.Value() go-work (figure) func (c *valueCtx) Value(key any) any { if c.key == key { return c.val } return value(c.Context, key) } 假如当前 valueCtx 的 key 等于用户传入的 key，则直接返回其 value； 假如不等，则从 parent context 中依次向上寻找. func value(c Context, key any) any { for { switch ctx := c.(type) { case *valueCtx: if key == ctx.key { return ctx.val } c = ctx.Context case *cancelCtx: if key == \u0026cancelCtxKey { return c } c = ctx.Context case *timerCtx: if key == \u0026cancelCtxKey { return \u0026ctx.cancelCtx } c = ctx.Context case *emptyCtx: return nil default: return c.Value(key) } } } 启动一个 for 循环，由下而上，由子及父，依次对 key 进行匹配； 其中 cancelCtx、timerCtx、emptyCtx 类型会有特殊的处理方式； 找到匹配的 key，则将该组 value 进行返回 ","date":"2023-05-29","objectID":"/golang-context/:5:1","tags":["golang"],"title":"Golang Context 底层原理","uri":"/golang-context/"},{"categories":["文档"],"content":" go-work (figure) 🔥Golang算法- 选择排序（Selection Sort) 在未排序序列中找到最小（大）元素，存放到排序序列的起始位置， 然后，再从剩余未排序元素中继续寻找最小（大）元素， 然后放到已排序序列的末尾。 以此类推，直到所有元素均排序完毕。 示例 package main import \"fmt\" func main() { arr := []int{5, 7, 1, 8, 3, 2, 6, 4, 9} arr = selectionSort(arr) fmt.Println(arr) } func findSmallest(arr []int) int { smallest := arr[0] smallestIndex := 0 for i := 0; i \u003c len(arr); i++ { if arr[i] \u003c smallest { smallest = arr[i] smallestIndex = i } } return smallestIndex } func selectionSort(arr []int) []int { result := []int{} count := len(arr) for i := 0; i \u003c count; i++ { smallestIndex := findSmallest(arr) result = append(result, arr[smallestIndex]) arr = append(arr[:smallestIndex], arr[smallestIndex+1:]...) } return result } 输出结果 go run main.go [1 2 3 4 5 6 7 8 9] ","date":"2023-05-29","objectID":"/golang-selectionsort/:0:0","tags":["golang","算法"],"title":"🔥Golang Selection Sort","uri":"/golang-selectionsort/"},{"categories":["文档"],"content":" 🔥Golang算法- 二分算法（Binary Search） 输入：排好序 的集合 如果要查询的元素在集合中： 返回位置（索引） 否则：返回空 ","date":"2023-05-27","objectID":"/golang-binarysearch/:0:0","tags":["golang","算法"],"title":"🚀Golang BinarySearch","uri":"/golang-binarysearch/"},{"categories":["文档"],"content":"Binary Search 介绍 针对拥有n个元素的一排序的集合 二分查找： log2n 简单查找： n ⚠️注意 二分查找只适用于已排序的集合 示例 package main import ( \"fmt\" \"math/rand\" \"time\" ) func main() { list := make([]int, 1_000_000) for i := 0; i \u003c 1_000_000; i++ { list = append(list, i+1) } rand.Seed(time.Now().UnixNano()) for i := 0; i \u003c 20; i++ { v := rand.Intn(1_000_000-1) + 1 fmt.Printf(\"针对 %d 进行二分查找： \\n\", v) idx := binarySearch(list, v) fmt.Printf(\"%d 的索引位置是：[%d]\\n\", v, idx) fmt.Println(\"-------------------------------\") } } func binarySearch(list []int, target int) int { low := 0 high := len(list) step := 0 for { step = step + 1 if low \u003c= high { mid := (low + high) / 2 guess := list[mid] if guess == target { fmt.Printf(\"共查找了 %d 次\\n\", step) return mid } if guess \u003e target { high = mid + 1 } else { low = mid - 1 } } } } 输出结果 go run main.go 针对 488291 进行二分查找： 共查找了 21 次 488291 的索引位置是：[1488290] ------------------------------- 针对 253611 进行二分查找： 共查找了 19 次 253611 的索引位置是：[1253610] ------------------------------- 针对 92000 进行二分查找： 共查找了 21 次 92000 的索引位置是：[1091999] ------------------------------- 针对 399702 进行二分查找： 共查找了 17 次 399702 的索引位置是：[1399701] ------------------------------- 针对 356459 进行二分查找： 共查找了 17 次 356459 的索引位置是：[1356458] ------------------------------- 针对 533248 进行二分查找： 共查找了 17 次 533248 的索引位置是：[1533247] ------------------------------- 针对 802470 进行二分查找： 共查找了 21 次 802470 的索引位置是：[1802469] ------------------------------- 针对 358966 进行二分查找： 共查找了 21 次 358966 的索引位置是：[1358965] ------------------------------- 针对 325215 进行二分查找： 共查找了 21 次 325215 的索引位置是：[1325214] ------------------------------- 针对 443265 进行二分查找： 共查找了 21 次 443265 的索引位置是：[1443264] ------------------------------- 针对 829382 进行二分查找： 共查找了 18 次 829382 的索引位置是：[1829381] ------------------------------- 针对 499513 进行二分查找： 共查找了 21 次 499513 的索引位置是：[1499512] ------------------------------- 针对 589792 进行二分查找： 共查找了 19 次 589792 的索引位置是：[1589791] ------------------------------- 针对 253817 进行二分查找： 共查找了 21 次 253817 的索引位置是：[1253816] ------------------------------- 针对 624237 进行二分查找： 共查找了 20 次 624237 的索引位置是：[1624236] ------------------------------- 针对 850665 进行二分查找： 共查找了 21 次 850665 的索引位置是：[1850664] ------------------------------- 针对 929880 进行二分查找： 共查找了 21 次 929880 的索引位置是：[1929879] ------------------------------- 针对 422010 进行二分查找： 共查找了 20 次 422010 的索引位置是：[1422009] ------------------------------- 针对 39939 进行二分查找： 共查找了 21 次 39939 的索引位置是：[1039938] ------------------------------- 针对 894763 进行二分查找： 共查找了 19 次 894763 的索引位置是：[1894762] ------------------------------- ","date":"2023-05-27","objectID":"/golang-binarysearch/:1:0","tags":["golang","算法"],"title":"🚀Golang BinarySearch","uri":"/golang-binarysearch/"},{"categories":["文档"],"content":" Golang 命令行参数os.Args 如何制作命令行应用 如何使用os.Args 获取命令行参数 ","date":"2023-05-27","objectID":"/golang-args/:0:0","tags":["golang"],"title":"Golang 命令行参数os.Args","uri":"/golang-args/"},{"categories":["文档"],"content":"使用的知识点 os包提供了用于处理操作系统的相关内容的函数/值 独立于平台的方式 os.Args 变量 获取命令行的参数 它是string slice 第一个值是命令本身 strings.Join 函数 将字符串切片中存在的所有元素连接为单个字符串 示例01 package main import ( \"fmt\" \"os\" ) func main() { var s, sep string for i := 0; i \u003c len(os.Args); i++ { s += sep + os.Args[i] sep = \" \" } fmt.Println(s) } 示例02 package main import ( \"fmt\" \"os\" ) func main() { s, sep := \"\", \"\" for _, args := range os.Args[1:] { s += sep + args sep = \" \" } fmt.Println(s) } 示例03 package main import ( \"fmt\" \"os\" \"strings\" ) func main() { fmt.Println(strings.Join(os.Args[1:], \" \")) } ","date":"2023-05-27","objectID":"/golang-args/:1:0","tags":["golang"],"title":"Golang 命令行参数os.Args","uri":"/golang-args/"},{"categories":["文档"],"content":"用户输入bufio package main import ( \"bufio\" \"fmt\" \"os\" ) func main() { fmt.Println(\"Waht's your name\") reader := bufio.NewReader(os.Stdin) text, _ := reader.ReadString('\\n') fmt.Printf(\"your name: %s\", text) } ","date":"2023-05-27","objectID":"/golang-args/:2:0","tags":["golang"],"title":"Golang 命令行参数os.Args","uri":"/golang-args/"},{"categories":["文档"],"content":" Golang TCP 端口扫描 非并发版 package main import ( \"fmt\" \"net\" ) func main() { for i := 21; i \u003c 120; i++ { address := fmt.Sprintf(\"192.168.0.2:%d\", i) conn, err := net.Dial(\"tcp\", address) if err != nil { fmt.Printf(\"%d 关闭了\\n\", address) continue } conn.Close() fmt.Printf(\"%d dakai了\\n\", address) } } 并发版 package main import ( \"fmt\" \"net\" \"sync\" \"time\" ) func main() { start := time.Now() var wg sync.WaitGroup for i := 21; i \u003c 120; i++ { wg.Add(1) go func(j int) { address := fmt.Sprintf(\"192.168.0.2:%d\", j) conn, err := net.Dial(\"tcp\", address) if err != nil { fmt.Printf(\"%d 关闭了\\n\", address) return } conn.Close() fmt.Printf(\"%d dakai了\\n\", address) }(i) wg.Wait() elapsed := time.Since(start) / 1e9 fmt.Printf(\"\\n\\n%d\\n\", elapsed) } } goroutine 池并发版 TCP 端口扫描器 go-work (figure) package main import ( \"fmt\" \"net\" \"sort\" ) func worker(ports chan int, results chan int) { for p := range ports { address := fmt.Sprintf(\"192.168.1.1:%d\", p) conn, err := net.Dial(\"tcp\", address) if err != nil { results \u003c- 0 fmt.Printf(\"%d 关闭了 \\n\", p) } conn.Close() fmt.Printf(\"%d 打开了\", p) results \u003c- p } } func main() { ports := make(chan int, 100) results := make(chan int) var openports []int var closeports []int for i := 0; i \u003c cap(ports); i++ { go worker(ports, results) } go func() { for i := 1; i \u003c 1024; i++ { ports \u003c- i } }() for i := 1; i \u003c 1024; i++ { port := \u003c- results if port != 0 { openports = append(openports, port) } else { closeports = append(closeports, port) } } close(ports) close(results) sort.Ints(openports) sort.Ints(closeports) for _, port := range openports { fmt.Printf(\"%d open\\n\", port) } for _, port := range closeports { fmt.Printf(\"%d close\\n\", port) } } 参考视频 ","date":"2023-05-26","objectID":"/golang-portscan/:0:0","tags":["golang"],"title":"Golang PortScan","uri":"/golang-portscan/"},{"categories":["文档"],"content":" K8s-GVK\u0026FVR ","date":"2023-05-26","objectID":"/k8s-gvk_gvr/:0:0","tags":["k8s"],"title":"K8s GVK_GVR","uri":"/k8s-gvk_gvr/"},{"categories":["文档"],"content":"GVK \u0026 GVR 介绍 GVK —— group 、version、 kind GVR —— group 、version 、resource ","date":"2023-05-26","objectID":"/k8s-gvk_gvr/:1:0","tags":["k8s"],"title":"K8s GVK_GVR","uri":"/k8s-gvk_gvr/"},{"categories":["文档"],"content":"kind 和 resource 概念 在编码过程中， 资源数据存储都是结构体存储 多版本version存储在（alpha1，beta1，v1等），不同版本中存储结构体的存在着差异 用 Kind 名（如 Deployment），并不能准确获取到其使用哪个版本结构体 采用 GVK 获取到一个具体的 存储结构体，也就是 GVK 的三个信息（group/verion/kind) 确定一个 Go type（结构体） Scheme 存储了 GVK 和 Go type 的映射关系 创建资源， 编写yaml，提交请求 编写 yaml 过程中，我们会写 apiversion 和 kind，其实就是 GVK 与 apiserver 通信是 http 形式，就是将请求发送到某一 http path http path 其实就是 GVR /apis/batch/v1/namespaces/default/job 这个就是表示 default 命名空间的 job 资源 ","date":"2023-05-26","objectID":"/k8s-gvk_gvr/:1:1","tags":["k8s"],"title":"K8s GVK_GVR","uri":"/k8s-gvk_gvr/"},{"categories":["文档"],"content":"相同名称 Kind 可以存在不同组 相同名称的kind不仅可以存在不同的Version， 也可以存在不同的group Ingress, NetworkPolicy同时在这个两个API Group： extensions、 networking.k8s.io Deployment, DaemonSet, ReplicaSet同时在这些API Group中：extensions、 apps Event同时在这些API Group中： core group and events.k8s.io ","date":"2023-05-26","objectID":"/k8s-gvk_gvr/:2:0","tags":["k8s"],"title":"K8s GVK_GVR","uri":"/k8s-gvk_gvr/"},{"categories":["文档"],"content":"API-group 资源分组 各组可以单独打开或者关闭 各组可以有自己独立的版本， 不影响其他组的情况下可以单独衍化 同一个资源可以同时存在于多个不同组中，这样就可以同时支持某个特定资源稳定版本与实验版本 kubernetes API 查看当前 kubernetes 集群支持的 API 版本 $ kubectl api-versions apiextensions.k8s.io/v1beta1 apiregistration.k8s.io/v1beta1 apps/v1beta1 apps/v1beta2 authentication.k8s.io/v1 authentication.k8s.io/v1beta1 authorization.k8s.io/v1 authorization.k8s.io/v1beta1 autoscaling/v1 autoscaling/v2beta1 batch/v1 batch/v1beta1 certificates.k8s.io/v1beta1 custom-metrics.metrics.k8s.io/v1alpha1 extensions/v1beta1 monitoring.coreos.com/v1 networking.k8s.io/v1 policy/v1beta1 rbac.authorization.k8s.io/v1 rbac.authorization.k8s.io/v1beta1 storage.k8s.io/v1 storage.k8s.io/v1beta1 v1 对于同一个资源对象的不同版本，API-Server 负责不同版本之间的无损切换，这点对于客户端来说是完全透明的（无感知）。 事实上，不同版本的同类型的资源在持久化层的数据可能是相同的。 例如，对于同一种资源类型支持 v1 和 v1beta1 两个 API 版本，以 v1beta1 版本创建该资源的对象，后续可以以v1 或者 v1beta1 来更新或者删除该资源对象 ","date":"2023-05-26","objectID":"/k8s-gvk_gvr/:3:0","tags":["k8s"],"title":"K8s GVK_GVR","uri":"/k8s-gvk_gvr/"},{"categories":["文档"],"content":"GVK 与 GVR 映射 kind 是API 顶级 资源对象类型，每个资源对象都需要kind来区分自身代表的类型 kind 字段 该资源对象的类型 单个的资源对象类型（pod） 资源对象列表类型， （podlist 或者 nodelist） 特殊类型以及非吃就好类型 （很多这种类型的资源是 subresource， 例如用于绑定资源的 /binding、更新资源状态的 /status 以及读写资源实例数量的 /scale） Resource 通过http协议以JSON格式发送或者读取的资源展现形式 单个资源 （…/namespaces/default） 列表资源（…/jobs） GVR 常用于组合成 RESTful API 请求路径 GET /apis/apps/v1/namespaces/{namespace}/deployments/{name} ","date":"2023-05-26","objectID":"/k8s-gvk_gvr/:4:0","tags":["k8s"],"title":"K8s GVK_GVR","uri":"/k8s-gvk_gvr/"},{"categories":["文档"],"content":"API 存储 API-Server 是无状态的，它需要与分布式存储系统 etcd 交互来实现资源对象的持久化操作 Kubernetes 资源对象是以JSON或 Protocol Buffers 格式存储在 etcd 中 以通过配置 kube-apiserver 的启动参数 –storage-media-type 来决定想要序列化数据存入 etcd 的格式 创建一个 pod，然后使用 etcdctl 工具来查看存储在 etcd 中数据 $ cat \u003c\u003c EOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: webserver spec: containers: - name: nginx image: nginx ports: - containerPort: 80 EOF pod/webserver created $ etcdctl --endpoints=$ETCD_URL \\ --cert /etc/kubernetes/pki/etcd/server.crt \\ --key /etc/kubernetes/pki/etcd/server.key \\ --cacert /etc/kubernetes/pki/etcd/ca.crt \\ get /registry/pods/default/webserver --prefix -w simple /registry/pods/default/webserver ... 10.244.0.5\" 客户端工具创建资源对象到然后存储到 etcd 的流程 客户端工具(kubectl) 提供一个期望状态的资源对象的序列化表示，是一样yaml格式提供 kubectl 将YAML转换为JSON 格式， 并发送给API -server 对应同类型对象的不同版本，API- SERVER 执行无损转换，对于老版本不存在的字段则存储在annotations中 API- SERVER 将接收到的对象转换为规范存储版本，这个版本由API-Server启动参数指定，通常是最稳定的版本 最后将资源对象通过JSON 或YAML方式解析并通过一个特定的key 存入etcd中 ","date":"2023-05-26","objectID":"/k8s-gvk_gvr/:5:0","tags":["k8s"],"title":"K8s GVK_GVR","uri":"/k8s-gvk_gvr/"},{"categories":["文档"],"content":" Liveness\u0026Readines ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:0:0","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"需求来源 实时观察应用的健康状态 获取应用的资源使用情况 拿到应用的实时日志， 进行问题的诊断和分析 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:1:0","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"Liveness probe 和 Readiness probe 介绍 livenness probe 是就绪指针，判断一个pod是否处于就绪状态。 当一个pod处于就绪状态的时候 才能对外提供对应的服务，接入层的流量才能打入相应的pod 当这个pod不处于就绪状态， 接入层灰吧相应的流量从这个pod上面剔除。 Liveness01 (figure) 这个pod指针一直处于失败， 接入层流量不会打到这个pod 这个 pod 的状态从 FAIL 的状态转换成 success 的状态时，它才能够真实地承载这个流量 这个时候会由上层的判断机制来判断这个 pod 是否需要被重新拉起。那如果上层配置的重启策略是 restart always 的话，那么此时这个 pod 会直接被重新拉起。 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:2:0","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"应用健康状态-使用方式 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:3:0","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"探测方式 liveness指针和Readkiness指针支持三种不同的探测方式 httpGet: 通过发送http 个体请求进行判断， 当返回码是200～399 ，标识这个应用是健康的 Exec: 执行容器中的一个命令判断当前容器是否正常，当返回时0，标识容器时健康的 tcpSocket： 通过探测容器的IP和Port 进行TCP健康检查， TCP能正常建立， 标识这个容器时健康的 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:3:1","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"探测结果 Success: 标识Container 通过健康检查 Failure: container没有通过健康检查，此时会有一个相应的处理，Readiness——（service层将没有通过Readiness的pod进行移除）liveness 将这个pod重新拉起， 或者删除 Unknown: 当前的执行机制没有完成执行（超时或者一些脚本没有及时返回），等待下次的机制来进行校验 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:3:2","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"应用健康状态检查方式-pod Probe spec ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:4:0","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"exec 通过cat 一个具体文件来判断当前Liveness probe 的状态，返回0 这个pod 处于健康状态 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:4:1","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"httpGet ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:4:2","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"tcpSocket 参数说明： initialDelaySeconds： pod启动延迟多久进行一次检查 periodSeconds： 检查的时间间隔，默认值是10秒 timeoutSeconds： 检查的超时时间 successThreshold： pod从探测失败到再一次探测成功， 锁需要的阀值次数 failureThreshold： 探测失败的重启次数， 默认值是3 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:4:3","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"Liveness 与 Readiness 总结 liveness: 判断容器是否处于running ,如果容器不健康，会通过kubelet 杀掉相应的pod Readiness： 判断容器是否启动完成，如果探测一个结果不成功， 会从pod上Endpoint上移除。 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:5:0","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"适用场景 liveness： 支持可以重新拉起的应用 Readiness： 应对 启动后无法立即对外提供服务的应用。 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:5:1","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"应用故障排除——状态机制 k8s 的状态机制， 通过yaml的方式定义个期望到达的状态。 这个yaml的真正执行过程 各种各样的controller来负责整体的状态之间的转换 一个 Pod 的一个生命周期。刚开始它处在一个 pending 的状态，那接下来可能会转换到类似像 running，也可能转换到 Unknown，甚至可以转换到 failed。然后，当 running 执行了一段时间之后，它可以转换到类似像 successded 或者是 failed，然后当出现在 unknown 这个状态时，可能由于一些状态的恢复，它会重新恢复到 running 或者 successded 或者是 failed。 k8s 整体的一个状态就是状态机制的转换 一个pod状态位的展现 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:6:0","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"应用故障排除-场景应用异常 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:7:0","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"pod停留在Pending 标识没有调度器进行介入 可能是资源或端口占用 由于node selector 造成的pod无法调度 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:7:1","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"Pod 停留在 waiting 表示pod的镜像没有正常拉取 可能原因 私有镜像 没有配置pod secret 镜像地址不存在 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:7:2","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"Pod 不断被拉取并且可以看到 crashing 标识pod已经调度完成， 但是启动失败 需要关注应用的自身状态 查看pod的具体日志 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:7:3","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"Pod 处在 Runing 但是没有正常工作 常见问题——yaml信息里可能有拼写错误 apply-validate -f pod.yaml 判断当前的yaml是否正确 如果没有问题，诊断配置的端口是否正常，以及liveness或Readiness 配置是否正确 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:7:4","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"Service 无法正常的工作 因为service和底层pod之间的关联是通过selector的方式进行匹配的， pod上面配置label， 然后service通过match label的方式和pod进行关联 如果lable配置有问题，可能会造成service无法找到后面的endpoint 造成相应的service无法对外提供服务 排除这种问题 首先 看service 后面的是不是有一个真正的endpoint 其次 看这个endpoint 是否可以对外提供正常的服务 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:7:5","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"应用远程调试 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:8:0","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"应用远程调试 - Pod 远程调试 进入容器里进行诊断 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:8:1","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"应用远程调试 - Service 远程调试 service调试 分了两部分 将服务暴露到远程的一个集群之内，让远程集群的一些应用去调用本地的一个服务，这是一条反向的一个链路 让本地的服务能够远程调用服务， 这是一条正向的链路 将 Telepresence 的一个 Proxy 应用部署到远程的 K8s 集群里面。然后将远程单一个 deployment swap 到本地的一个 application，使用的命令就是 Telepresence-swap-deployment 然后以及远程的 DEPLOYMENT_NAME。通过这种方式就可以将本地一个 application 代理到远程的 service 之上、可以将应用在远程集群里面进行本地调试 使用方式是 kubectl port-forward，然后 service 加上远程的 service name，再加上相应的 namespace，后面还可以加上一些额外的参数，比如说端口的一个映射，通过这种机制就可以把远程的一个应用代理到本地的端口之上，此时通过访问本地端口就可以访问远程的服务。 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:8:2","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"kubectl-debug kubectl-debug 这个工具是依赖于 Linux namespace 的方式来去做的，它可以 datash 一个 Linux namespace 到一个额外的 container，然后在这个 container 里面执行任何的 debug 动作，其实和直接去 debug 这个 Linux namespace 是一致的。这里有一个简单的操作 通过 kubectl-debug 这条命令来去诊断远程的一个 pod 执行debug的时候，首先会拉取一些镜像， 这个镜像里会默认带一些诊断工具 当这个镜像启用的时候，它会把这个 debug container 进行启动 这个 container 和相应的你要诊断的这个 container 的 namespace 进行挂靠，类似像网络站，或者是类似像内核的一些参数，可以在这个 debug container 里面实时地进行查看。 去查看类似像 hostname、进程、netstat 等等，这个容器和需要debug的pod在同一个环境里 进行 logout 的话，相当于会把相应的这个 debug pod 杀掉 ","date":"2023-05-24","objectID":"/k8s-liveness_readines/:9:0","tags":["k8s"],"title":"K8s Liveness_Readines","uri":"/k8s-liveness_readines/"},{"categories":["文档"],"content":"Etcd etcd 是一个开源的， 高可用的分布式的 key-value 存储系统， 可以用于配制共享和服务的注册和发现 etcd具有的特点 完全复制： 集群中的每个阶段都可以使用完整的文档 高可用性： Etcd可用于避免硬件的单点故障或网络问题 一致性： 每次读取都会返回跨多主机的最新写入 简单： 包括一个定义良好、面向用户的API（gRPC） 安全： 实现了带有可选的客户端证书身份验证的自动化TLS 快速： 每秒10000次写入的基准速度 可靠： 用Raft算法实现了强一致、高可用的服务存储目录 ","date":"2023-05-23","objectID":"/golang-etcd/:0:0","tags":["golang","etcd"],"title":"Golang Etcd","uri":"/golang-etcd/"},{"categories":["文档"],"content":"etcd 的应用场景 服务发现 服务发现要解决的也是分布式系统中最常见的问题之一，即在同一个分布式集群中的进程或服务，要如何才能找到对方并建立连接。本质上来说，服务发现就是想要了解集群中是否有进程在监听 udp 或 tcp 端口，并且通过名字就可以查找和连接。 Registry (figure) 配置中心 将一些配置存放在etcd 上集中管理 这类场景的使用方式通常是这样：应用在启动的时候主动从 etcd 获取一次配置信息，同时，在 etcd 节点上注册一个 Watcher 并等待，以后每次配置有更新的时候，etcd 都会实时通知订阅者，以此达到获取最新配置信息的目的 分布式锁 因为 etcd 使用 Raft 算法保持了数据的强一致性，某次操作存储到集群中的值必然是全局一致的，所以很容易实现分布式锁。锁服务有两种使用方式，一是保持独占，二是控制时序。 保持独占即所有获取锁的用户最终只有一个可以得到。etcd 为此提供了一套实现分布式锁原子操作 CAS（CompareAndSwap）的 API。通过设置prevExist值，可以保证在多个节点同时去创建某个目录时，只有一个成功。而创建成功的用户就可以认为是获得了锁。 控制时序，即所有想要获得锁的用户都会被安排执行，但是获得锁的顺序也是全局唯一的，同时决定了执行顺序。etcd 为此也提供了一套 API（自动创建有序键），对一个目录建值时指定为POST动作，这样 etcd 会自动在目录下生成一个当前最大的值为键，存储这个新的值（客户端编号）。同时还可以使用 API 按顺序列出所有当前目录下的键值。此时这些键的值就是客户端的时序，而这些键中存储的值可以是代表客户端的编号 etcd_lock (figure) ","date":"2023-05-23","objectID":"/golang-etcd/:1:0","tags":["golang","etcd"],"title":"Golang Etcd","uri":"/golang-etcd/"},{"categories":["文档"],"content":"go 操作etcd ","date":"2023-05-23","objectID":"/golang-etcd/:2:0","tags":["golang","etcd"],"title":"Golang Etcd","uri":"/golang-etcd/"},{"categories":["文档"],"content":"安装 go get go.etcd.io/etcd/client/v3 ","date":"2023-05-23","objectID":"/golang-etcd/:3:0","tags":["golang","etcd"],"title":"Golang Etcd","uri":"/golang-etcd/"},{"categories":["文档"],"content":"put和get 操作 package main import ( \"context\" \"fmt\" \"time\" clientv3 \"go.etcd.io/etcd/client/v3\" ) // etcd client put/get demo // use etcd/clientv3 func main() { cli, err := clientv3.New(clientv3.Config{ Endpoints: []string{\"127.0.0.1:2379\"}, DialTimeout: 5 * time.Second, }) if err != nil { // handle error! fmt.Printf(\"connect to etcd failed, err:%v\\n\", err) return } fmt.Println(\"connect to etcd success\") defer cli.Close() // put ctx, cancel := context.WithTimeout(context.Background(), time.Second) _, err = cli.Put(ctx, \"demo\", \"dsb\") cancel() if err != nil { fmt.Printf(\"put to etcd failed, err:%v\\n\", err) return } // get ctx, cancel = context.WithTimeout(context.Background(), time.Second) resp, err := cli.Get(ctx, \"demo\") cancel() if err != nil { fmt.Printf(\"get from etcd failed, err:%v\\n\", err) return } for _, ev := range resp.Kvs { fmt.Printf(\"%s:%s\\n\", ev.Key, ev.Value) } } ","date":"2023-05-23","objectID":"/golang-etcd/:4:0","tags":["golang","etcd"],"title":"Golang Etcd","uri":"/golang-etcd/"},{"categories":["文档"],"content":"watch操作 watch 用来获取未来更改的通知 package main import ( \"context\" \"fmt\" \"time\" clientv3 \"go.etcd.io/etcd/client/v3\" ) // watch demo func main() { cli, err := clientv3.New(clientv3.Config{ Endpoints: []string{\"127.0.0.1:2379\"}, DialTimeout: 5 * time.Second, }) if err != nil { fmt.Printf(\"connect to etcd failed, err:%v\\n\", err) return } fmt.Println(\"Connect to etcd successful\") defer cli.Close() // watch keys:demo changes rch := cli.Watch(context.Background(), \"demo\") for wresp := range rch { for _, ev := range wresp.Events { fmt.Printf(\"Type: %s Key:%s Value:%s\\n\", ev.Type, ev.Kv.Key, ev.Kv.Value) } } } 这个程序会等待 etcd 中 demo 这个key的变化 打开终端对 demo 这个命令 设置 etcdctl put demo \"dsb01\" OK etcdctl del demo 1 etcdctl put demo \"ddd03\" OK watch 会收到的通知 Connect to etcd successful Type: PUT Key:demo Value:dsb01 Type: DELETE Key:demo Value: Type: PUT Key:demo Value:ddd03 ","date":"2023-05-23","objectID":"/golang-etcd/:4:1","tags":["golang","etcd"],"title":"Golang Etcd","uri":"/golang-etcd/"},{"categories":["文档"],"content":"lease 租约 package main import ( \"context\" \"fmt\" \"log\" \"time\" clientv3 \"go.etcd.io/etcd/client/v3\" ) // watch demo func main() { cli, err := clientv3.New(clientv3.Config{ Endpoints: []string{\"127.0.0.1:2379\"}, DialTimeout: 5 * time.Second, }) if err != nil { fmt.Printf(\"connect to etcd failed, err:%v\\n\", err) return } fmt.Println(\"Connect to etcd successful\") defer cli.Close() //etcd lease // 创建一个5s的租约 resp, err := cli.Grant(context.TODO(), 5) if err != nil { log.Fatal(err.Error()) } // 5s之后， /demo/ 这个key就会被移除 _, err = cli.Put(context.TODO(), \"/demo/\", \"bbb\", clientv3.WithLease(resp.ID)) if err != nil { log.Fatal(err.Error()) } } ","date":"2023-05-23","objectID":"/golang-etcd/:5:0","tags":["golang","etcd"],"title":"Golang Etcd","uri":"/golang-etcd/"},{"categories":["文档"],"content":"keepAlive package main import ( \"context\" \"fmt\" \"log\" \"time\" clientv3 \"go.etcd.io/etcd/client/v3\" ) // watch demo func main() { cli, err := clientv3.New(clientv3.Config{ Endpoints: []string{\"127.0.0.1:2379\"}, DialTimeout: 5 * time.Second, }) if err != nil { fmt.Printf(\"connect to etcd failed, err:%v\\n\", err) return } fmt.Println(\"Connect to etcd successful\") defer cli.Close() //etcd lease // 创建一个5s的租约 resp, err := cli.Grant(context.TODO(), 5) if err != nil { log.Fatal(err.Error()) } // 5s之后， /demo/ 这个key就会被移除 _, err = cli.Put(context.TODO(), \"/demo/\", \"bbb\", clientv3.WithLease(resp.ID)) if err != nil { log.Fatal(err.Error()) } // the key 'foo' will be kept forever ch, haerr := cli.KeepAlive(context.TODO(), resp.ID) if haerr != nil { log.Fatal(err.Error()) } for { ha := \u003c-ch fmt.Println(\"ttl: \", ha.TTL) } } ","date":"2023-05-23","objectID":"/golang-etcd/:6:0","tags":["golang","etcd"],"title":"Golang Etcd","uri":"/golang-etcd/"},{"categories":["文档"],"content":"基于etcd 实现分布式锁 go.etcd.io/etcd/client/v3/concurrency 在etcd之上实现并发操作， 如分布式锁/屏障和选举 倒入包 import \"go.etcd.io/etcd/client/v3/concurrency\" 基于etcd 实现分布式锁的示例 package main import ( \"context\" \"fmt\" \"log\" \"time\" clientv3 \"go.etcd.io/etcd/client/v3\" \"go.etcd.io/etcd/client/v3/concurrency\" ) // watch demo func main() { cli, err := clientv3.New(clientv3.Config{ Endpoints: []string{\"127.0.0.1:2379\"}, DialTimeout: 5 * time.Second, }) if err != nil { fmt.Printf(\"connect to etcd failed, err:%v\\n\", err) return } fmt.Println(\"Connect to etcd successful\") defer cli.Close() // 创建两个单独的会话来演示竞争锁 s1, err := concurrency.NewSession(cli) if err != nil { log.Fatal(err.Error()) } defer s1.Close() m1 := concurrency.NewMutex(s1, \"/my-lock/\") s2, err := concurrency.NewSession(cli) if err != nil { log.Fatal(err.Error()) } defer s2.Close() m2 := concurrency.NewMutex(s2, \"/my-lock/\") // 会话s1 获取锁 if err := m1.Lock(context.TODO()); err != nil { log.Fatal(err.Error()) } fmt.Println(\"acquired lock for s1\") m2Locked := make(chan struct{}) go func() { defer close(m2Locked) // 等待之后会话s1 释放了/my-lock/的锁 if err := m2.Lock(context.TODO()); err != nil { log.Fatal(err.Error()) } fmt.Println(\"acquired lock for s2\") }() if err := m1.Unlock(context.TODO()); err != nil { log.Fatal(err.Error()) } fmt.Println(\"release lock for s1\") \u003c-m2Locked fmt.Println(\"release lock for s2\") } 输出结果 Connect to etcd successful acquired lock for s1 release lock for s1 acquired lock for s2 release lock for s2 参考链接 https://pkg.go.dev/go.etcd.io/etcd/clientv3/concurrency https://github.com/etcd-io/etcd/tree/main/client/v3 ","date":"2023-05-23","objectID":"/golang-etcd/:7:0","tags":["golang","etcd"],"title":"Golang Etcd","uri":"/golang-etcd/"},{"categories":["文档"],"content":"MongoDB ","date":"2023-05-23","objectID":"/golang-mongodb/:0:0","tags":["golang","mongoDB"],"title":"Golang MangoDB","uri":"/golang-mongodb/"},{"categories":["文档"],"content":"MongoDB介绍 mongoDB是基于分布式文件存储的数据库，是一个介于关系型数据库和非关系数据库之间的产品 mongoDB将一条数据存储为一个文档， 数据结构由键值对组成， 其中文档类似我们编辑中用到的JSON对象，文档中的字段值可以包含其他文档/数组及文档数组 MongoDB术语 说明 SQL术语 database 数据库 database collection 集合 table document 文档 row field 字段 column index index 索引 primary key 主键 MongoDB自动将_id字段设置为主键 primary key ","date":"2023-05-23","objectID":"/golang-mongodb/:1:0","tags":["golang","mongoDB"],"title":"Golang MangoDB","uri":"/golang-mongodb/"},{"categories":["文档"],"content":"mongoDB安装 docker pull mongo:latest docker run -itd --name mongo -p 27017:27017 mongo --auth 参数说明 -p 27017:27017 ：映射容器服务的 27017 端口到宿主机的 27017 端口。外部可以直接通过 宿主机 ip:27017 访问到 mongo 的服务。 –auth：需要密码才能访问容器服务。 使用以下命令添加用户和设置密码，并且尝试连接 $ docker exec -it mongo mongosh admin # 创建一个名为 admin，密码为 123456 的用户。 \u003e db.createUser({ user:'admin',pwd:'123456',roles:[ { role:'userAdminAnyDatabase', db: 'admin'},\"readWriteAnyDatabase\"]}); # 尝试使用上面创建的用户信息进行连接。 \u003e db.auth('admin', '123456') 连接 mongo docker exec -it mongo mongosh admin ","date":"2023-05-23","objectID":"/golang-mongodb/:2:0","tags":["golang","mongoDB"],"title":"Golang MangoDB","uri":"/golang-mongodb/"},{"categories":["文档"],"content":"golang 操作mongo DB 安装mongoDB Go驱动包 go get github.com/mongodb/mongo-go-driver Go代码连接mongoDB package main import ( \"context\" \"fmt\" \"log\" \"go.mongodb.org/mongo-driver/mongo\" \"go.mongodb.org/mongo-driver/mongo/options\" ) func main() { // 设置客户端连接配置 clientOptions := options.Client().ApplyURI(\"mongodb://192.168.0.9:27017\") // 连接到MongoDB client, err := mongo.Connect(context.TODO(), clientOptions) if err != nil { log.Fatal(err) } // 检查连接 err = client.Ping(context.TODO(), nil) if err != nil { log.Fatal(err) } fmt.Println(\"Connected to MongoDB!\") } 处理数据集 // 指定获取要操作的数据集 collection := client.Database(\"demo\").Collection(\"student\") 处理完任务之后可以通过下面的命令断开与MongoDB的连接 // 断开连接 err = client.Disconnect(context.TODO()) if err != nil { log.Fatal(err) } fmt.Println(\"Connection to MongoDB closed.\") ","date":"2023-05-23","objectID":"/golang-mongodb/:3:0","tags":["golang","mongoDB"],"title":"Golang MangoDB","uri":"/golang-mongodb/"},{"categories":["文档"],"content":"连接池模式 import ( \"context\" \"time\" \"go.mongodb.org/mongo-driver/mongo\" \"go.mongodb.org/mongo-driver/mongo/options\" ) func ConnectToDB(uri, name string, timeout time.Duration, num uint64) (*mongo.Database, error) { ctx, cancel := context.WithTimeout(context.Background(), timeout) defer cancel() o := options.Client().ApplyURI(uri) o.SetMaxPoolSize(num) client, err := mongo.Connect(ctx, o) if err != nil { return nil, err } return client.Database(name), nil } ","date":"2023-05-23","objectID":"/golang-mongodb/:4:0","tags":["golang","mongoDB"],"title":"Golang MangoDB","uri":"/golang-mongodb/"},{"categories":["文档"],"content":"BSON mongoDB中的JSON文档存储在BSON的二进制表示中 BSON编码拓展了JSON表示，使其包含额外的类型如 int\\long\\date\\浮点数和decimal128， 是应用程序更容易可靠的处理、排序和比较数据 要使用BSON 需要倒入包 import \"go.mongodb.org/mongo-driver/bson\" 使用D类型构建的过滤文档的理智， 它可以用来查询name字段与’张三’或’李四’匹配的文档 bson.D{{ \"name\", bson.D{{ \"$in\", bson.A{\"张三\", \"李四\"}, }}, }} ","date":"2023-05-23","objectID":"/golang-mongodb/:5:0","tags":["golang","mongoDB"],"title":"Golang MangoDB","uri":"/golang-mongodb/"},{"categories":["文档"],"content":"CURD 定义一个Student类型 type Student struct { Name string Age int } 准备数据 s1 := Student{\"张三\", 12} s2 := Student{\"李四\", 10} s3 := Student{\"王五\", 11} 插入一条文档记录 insertResult, err := collection.InsertOne(context.TODO(), s1) if err != nil { log.Fatal(err) } fmt.Println(\"Inserted a single document: \", insertResult.InsertedID) 插入多条文档 students := []interface{}{s2, s3} insertManyResult, err := collection.InsertMany(context.TODO(), students) if err != nil { log.Fatal(err) } fmt.Println(\"Inserted multiple documents: \", insertManyResult.InsertedIDs) ","date":"2023-05-23","objectID":"/golang-mongodb/:6:0","tags":["golang","mongoDB"],"title":"Golang MangoDB","uri":"/golang-mongodb/"},{"categories":["文档"],"content":"更新文档 updateone() 方法允许你更新当个文档，需要一个筛选器文档来批评数据库中的文档， 并需要一个更新文档来描述更新操作， 使用bson.D 类型来构建筛选文档和更新文档 filter := bson.D{{\"name\", \"小兰\"}} update := bson.D{ {\"$inc\", bson.D{ {\"age\", 1}, }}, } updateResult, err := collection.UpdateOne(context.TODO(), filter, update) if err != nil { log.Fatal(err) } fmt.Printf(\"Matched %v documents and updated %v documents.\\n\", updateResult.MatchedCount, updateResult.ModifiedCount) ","date":"2023-05-23","objectID":"/golang-mongodb/:6:1","tags":["golang","mongoDB"],"title":"Golang MangoDB","uri":"/golang-mongodb/"},{"categories":["文档"],"content":"查找文档 找到一个文档，需要一个filter文档， 以及一个指向可以将结构解码为其值的指针 要查找单个文档， 使用collection.FindOne()。这个方法返回一个可以解码为值的结果 // 创建一个Student变量用来接收查询的结果 var result Student err = collection.FindOne(context.TODO(), filter).Decode(\u0026result) if err != nil { log.Fatal(err) } fmt.Printf(\"Found a single document: %+v\\n\", result) 要查找多个文档，请使用collection.Find()。此方法返回一个游标。游标提供了一个文档流，你可以通过它一次迭代和解码一个文档。当游标用完之后，应该关闭游标。下面的示例将使用options包设置一个限制以便只返回两个文档。 // 查询多个 // 将选项传递给Find() findOptions := options.Find() findOptions.SetLimit(2) // 定义一个切片用来存储查询结果 var results []*Student // 把bson.D{{}}作为一个filter来匹配所有文档 cur, err := collection.Find(context.TODO(), bson.D{{}}, findOptions) if err != nil { log.Fatal(err) } // 查找多个文档返回一个光标 // 遍历游标允许我们一次解码一个文档 for cur.Next(context.TODO()) { // 创建一个值，将单个文档解码为该值 var elem Student err := cur.Decode(\u0026elem) if err != nil { log.Fatal(err) } results = append(results, \u0026elem) } if err := cur.Err(); err != nil { log.Fatal(err) } // 完成后关闭游标 cur.Close(context.TODO()) fmt.Printf(\"Found multiple documents (array of pointers): %#v\\n\", results) ","date":"2023-05-23","objectID":"/golang-mongodb/:7:0","tags":["golang","mongoDB"],"title":"Golang MangoDB","uri":"/golang-mongodb/"},{"categories":["文档"],"content":"删除文档 使用collection.DeleteOne()或collection.DeleteMany()删除文档。如果你传递bson.D{{}}作为过滤器参数，它将匹配数据集中的所有文档。还可以使用collection. drop()删除整个数据集。 // 删除名字是小黄的那个 deleteResult1, err := collection.DeleteOne(context.TODO(), bson.D{{\"name\",\"小黄\"}}) if err != nil { log.Fatal(err) } fmt.Printf(\"Deleted %v documents in the trainers collection\\n\", deleteResult1.DeletedCount) // 删除所有 deleteResult2, err := collection.DeleteMany(context.TODO(), bson.D{{}}) if err != nil { log.Fatal(err) } fmt.Printf(\"Deleted %v documents in the trainers collection\\n\", deleteResult2.DeletedCount) 参考链接 https://pkg.go.dev/go.mongodb.org/mongo-driver/mongo ","date":"2023-05-23","objectID":"/golang-mongodb/:8:0","tags":["golang","mongoDB"],"title":"Golang MangoDB","uri":"/golang-mongodb/"},{"categories":["文档"],"content":"Redis ","date":"2023-05-23","objectID":"/golang-redis/:0:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"Redis 介绍 Redis是一个开源的内存数据库，提供了多种不同的数据结构， 通过复制、持久化/客户端分片等特性， 方便的将Redis 拓展成一个包含数百GB的数据， 处理上百万次请求 Redis 支持的数据结构 string hashe list set sortedset bitmap stream Redis 应用场景 缓存系统，减轻mysql的压力 计数系统， 热门排行榜 利用LIST 实现队列功能 利用HyperLog 统计UV、PV等数据 使用geospatial index 进行地理位置相关的查询 ","date":"2023-05-23","objectID":"/golang-redis/:1:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"docker 安装Reids 启动redis docker run --name redis507 -p 6379:6379 -d redis:5.0.7 连接redis server docker run -it --network host --rm redis:5.0.7 redis-cli ","date":"2023-05-23","objectID":"/golang-redis/:2:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"go-redis库 安装 go get github.com/go-redis/redis/v8 连接 ","date":"2023-05-23","objectID":"/golang-redis/:3:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"普通模式连接 使用redis.NewClient 函数连接Redis服务器 rdb := redis.NewClient(\u0026redis.Options{ Addr: \"localhost:6379\", Password: \"\", // 密码 DB: 0, // 数据库 PoolSize: 20, // 连接池大小 }) 谁也redis.ParseURL 函数表示数据源的字符串解析Redis opt, err := redis.ParseURL(\"redis://\u003cuser\u003e:\u003cpass\u003e@localhost:6379/\u003cdb\u003e\") if err != nil { panic(err) } rdb := redis.NewClient(opt) TLS 模式 rdb := redis.NewClient(\u0026redis.Options{ TLSConfig: \u0026tls.Config{ MinVersion: tls.VersionTLS12, // Certificates: []tls.Certificate{cert}, // ServerName: \"your.domain.com\", }, }) Redis Sentinel模式 rdb := redis.NewFailoverClient(\u0026redis.FailoverOptions{ MasterName: \"master-name\", SentinelAddrs: []string{\":9126\", \":9127\", \":9128\"}, }) Redis Cluster模式 rdb := redis.NewClusterClient(\u0026redis.ClusterOptions{ Addrs: []string{\":7000\", \":7001\", \":7002\", \":7003\", \":7004\", \":7005\"}, // 若要根据延迟或随机路由命令，请启用以下命令之一 // RouteByLatency: true, // RouteRandomly: true, }) ","date":"2023-05-23","objectID":"/golang-redis/:3:1","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"基本使用 redis基础操作 // doCommand go-redis基本使用示例 func doCommand() { ctx, cancel := context.WithTimeout(context.Background(), 500*time.Millisecond) defer cancel() // 执行命令获取结果 val, err := rdb.Get(ctx, \"key\").Result() fmt.Println(val, err) // 先获取到命令对象 cmder := rdb.Get(ctx, \"key\") fmt.Println(cmder.Val()) // 获取值 fmt.Println(cmder.Err()) // 获取错误 // 直接执行命令获取错误 err = rdb.Set(ctx, \"key\", 10, time.Hour).Err() // 直接执行命令获取值 value := rdb.Get(ctx, \"key\").Val() fmt.Println(value) } 执行任意命令 Do方法可以形象任意命令 // doDemo rdb.Do 方法使用示例 func doDemo() { ctx, cancel := context.WithTimeout(context.Background(), 500*time.Millisecond) defer cancel() // 直接执行命令获取错误 err := rdb.Do(ctx, \"set\", \"key\", 10, \"EX\", 3600).Err() fmt.Println(err) // 执行命令获取结果 val, err := rdb.Do(ctx, \"get\", \"key\").Result() fmt.Println(val, err) } Redis.Nil Nil错误来表示Key不存在的错误 // getValueFromRedis redis.Nil判断 func getValueFromRedis(key, defaultValue string) (string, error) { ctx, cancel := context.WithTimeout(context.Background(), 500*time.Millisecond) defer cancel() val, err := rdb.Get(ctx, key).Result() if err != nil { // 如果返回的错误是key不存在 if errors.Is(err, redis.Nil) { return defaultValue, nil } // 出其他错了 return \"\", err } return val, nil } ","date":"2023-05-23","objectID":"/golang-redis/:4:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"zset示例 go-redis 操作zset // zsetDemo 操作zset示例 func zsetDemo() { // key zsetKey := \"language_rank\" // value languages := []*redis.Z{ {Score: 90.0, Member: \"Golang\"}, {Score: 98.0, Member: \"Java\"}, {Score: 95.0, Member: \"Python\"}, {Score: 97.0, Member: \"JavaScript\"}, {Score: 99.0, Member: \"C/C++\"}, } ctx, cancel := context.WithTimeout(context.Background(), 500*time.Millisecond) defer cancel() // ZADD err := rdb.ZAdd(ctx, zsetKey, languages...).Err() if err != nil { fmt.Printf(\"zadd failed, err:%v\\n\", err) return } fmt.Println(\"zadd success\") // 把Golang的分数加10 newScore, err := rdb.ZIncrBy(ctx, zsetKey, 10.0, \"Golang\").Result() if err != nil { fmt.Printf(\"zincrby failed, err:%v\\n\", err) return } fmt.Printf(\"Golang's score is %f now.\\n\", newScore) // 取分数最高的3个 ret := rdb.ZRevRangeWithScores(ctx, zsetKey, 0, 2).Val() for _, z := range ret { fmt.Println(z.Member, z.Score) } // 取95~100分的 op := \u0026redis.ZRangeBy{ Min: \"95\", Max: \"100\", } ret, err = rdb.ZRangeByScoreWithScores(ctx, zsetKey, op).Result() if err != nil { fmt.Printf(\"zrangebyscore failed, err:%v\\n\", err) return } for _, z := range ret { fmt.Println(z.Member, z.Score) } } ","date":"2023-05-23","objectID":"/golang-redis/:5:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"扫描遍历所有Key 使用KEYS prefix:* 获取前缀获取所有key vals, err := rdb.Keys(ctx, \"prefix*\").Result() // scanKeysDemo1 按前缀查找所有key示例 func scanKeysDemo1() { ctx, cancel := context.WithTimeout(context.Background(), 500*time.Millisecond) defer cancel() var cursor uint64 for { var keys []string var err error // 按前缀扫描key keys, cursor, err = rdb.Scan(ctx, cursor, \"prefix:*\", 0).Result() if err != nil { panic(err) } for _, key := range keys { fmt.Println(\"key\", key) } if cursor == 0 { // no more keys break } } } 简化 // scanKeysDemo2 按前缀扫描key示例 func scanKeysDemo2() { ctx, cancel := context.WithTimeout(context.Background(), 500*time.Millisecond) defer cancel() // 按前缀扫描key iter := rdb.Scan(ctx, 0, \"prefix:*\", 0).Iterator() for iter.Next(ctx) { fmt.Println(\"keys\", iter.Val()) } if err := iter.Err(); err != nil { panic(err) } } 支持将所有匹配指定的模式key删除的示例 // delKeysByMatch 按match格式扫描所有key并删除 func delKeysByMatch(match string, timeout time.Duration) { ctx, cancel := context.WithTimeout(context.Background(), timeout) defer cancel() iter := rdb.Scan(ctx, 0, match, 0).Iterator() for iter.Next(ctx) { err := rdb.Del(ctx, iter.Val()).Err() if err != nil { panic(err) } } if err := iter.Err(); err != nil { panic(err) } } 支持 set 、 hash、zset 数据类型， 遍历 iter := rdb.SScan(ctx, \"set-key\", 0, \"prefix:*\", 0).Iterator() iter := rdb.HScan(ctx, \"hash-key\", 0, \"prefix:*\", 0).Iterator() iter := rdb.ZScan(ctx, \"sorted-hash-key\", 0, \"prefix:*\", 0).Iterator( ","date":"2023-05-23","objectID":"/golang-redis/:6:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"Pipeline Redis Pipeline 允许通过使用单个 client-server-client 往返执行多个命令来提高性能。区别于一个接一个地执行100个命令，你可以将这些命令放入 pipeline 中，然后使用1次读写操作像执行单个命令一样执行它们。这样做的好处是节省了执行命令的网络往返时间（RTT）。 pipe := rdb.Pipeline() incr := pipe.Incr(ctx, \"pipeline_counter\") pipe.Expire(ctx, \"pipeline_counter\", time.Hour) cmds, err := pipe.Exec(ctx) if err != nil { panic(err) } // 在执行pipe.Exec之后才能获取到结果 fmt.Println(incr.Val()) 使用Pipelined 方法，它会在函数退出时调用 Exec var incr *redis.IntCmd cmds, err := rdb.Pipelined(ctx, func(pipe redis.Pipeliner) error { incr = pipe.Incr(ctx, \"pipelined_counter\") pipe.Expire(ctx, \"pipelined_counter\", time.Hour) return nil }) if err != nil { panic(err) } // 在pipeline执行后获取到结果 fmt.Println(incr.Val()) 遍历 pipeline 命令的返回值依次获取每个命令的结果。下方的示例代码中使用pipiline一次执行了100个 Get 命令，在pipeline 执行后遍历取出100个命令的执行结果。 cmds, err := rdb.Pipelined(ctx, func(pipe redis.Pipeliner) error { for i := 0; i \u003c 100; i++ { pipe.Get(ctx, fmt.Sprintf(\"key%d\", i)) } return nil }) if err != nil { panic(err) } for _, cmd := range cmds { fmt.Println(cmd.(*redis.StringCmd).Val()) } ","date":"2023-05-23","objectID":"/golang-redis/:7:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"事务 使用 TxPipeline 或 TxPipelined 方法将 pipeline 命令使用 MULTI 和EXEC包裹起来。 // TxPipeline demo pipe := rdb.TxPipeline() incr := pipe.Incr(ctx, \"tx_pipeline_counter\") pipe.Expire(ctx, \"tx_pipeline_counter\", time.Hour) _, err := pipe.Exec(ctx) fmt.Println(incr.Val(), err) // TxPipelined demo var incr2 *redis.IntCmd _, err = rdb.TxPipelined(ctx, func(pipe redis.Pipeliner) error { incr2 = pipe.Incr(ctx, \"tx_pipeline_counter\") pipe.Expire(ctx, \"tx_pipeline_counter\", time.Hour) return nil }) fmt.Println(incr2.Val(), err) ","date":"2023-05-23","objectID":"/golang-redis/:8:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"Watch 搭配 WATCH命令来执行事务操作。从使用WATCH命令监视某个 key 开始，直到执行EXEC命令的这段时间里，如果有其他用户抢先对被监视的 key 进行了替换、更新、删除等操作，那么当用户尝试执行EXEC的时候，事务将失败并返回一个错误，用户可以根据这个错误选择重试事务或者放弃事务。 Watch方法接收一个函数和一个或多个key作为参数。 Watch(fn func(*Tx) error, keys ...string) error Watch 方法搭配 TxPipelined 的使用示例 // watchDemo 在key值不变的情况下将其值+1 func watchDemo(ctx context.Context, key string) error { return rdb.Watch(ctx, func(tx *redis.Tx) error { n, err := tx.Get(ctx, key).Int() if err != nil \u0026\u0026 err != redis.Nil { return err } // 假设操作耗时5秒 // 5秒内我们通过其他的客户端修改key，当前事务就会失败 time.Sleep(5 * time.Second) _, err = tx.TxPipelined(ctx, func(pipe redis.Pipeliner) error { pipe.Set(ctx, key, n+1, time.Hour) return nil }) return err }, key) } ","date":"2023-05-23","objectID":"/golang-redis/:9:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"一个 go-redis 官方文档中使用 GET 、SET和WATCH命令实现一个 INCR 命令的完整示例。 const routineCount = 100 increment := func(key string) error { txf := func(tx *redis.Tx) error { // 获得当前值或零值 n, err := tx.Get(key).Int() if err != nil \u0026\u0026 err != redis.Nil { return err } // 实际操作（乐观锁定中的本地操作） n++ // 仅在监视的Key保持不变的情况下运行 _, err = tx.Pipelined(func(pipe redis.Pipeliner) error { // pipe 处理错误情况 pipe.Set(key, n, 0) return nil }) return err } for retries := routineCount; retries \u003e 0; retries-- { err := rdb.Watch(txf, key) if err != redis.TxFailedErr { return err } // 乐观锁丢失 } return errors.New(\"increment reached maximum number of retries\") } var wg sync.WaitGroup wg.Add(routineCount) for i := 0; i \u003c routineCount; i++ { go func() { defer wg.Done() if err := increment(\"counter3\"); err != nil { fmt.Println(\"increment error:\", err) } }() } wg.Wait() n, err := rdb.Get(\"counter3\").Int() fmt.Println(\"ended with\", n, err) 在这个示例中使用了 redis.TxFailedErr 来检查事务是否失败。 参考链接 https://pkg.go.dev/github.com/go-redis/redis https://www.liwenzhou.com/posts/Go/redis/#autoid-1-2-0 ","date":"2023-05-23","objectID":"/golang-redis/:10:0","tags":["golang","redis"],"title":"Golang Redis","uri":"/golang-redis/"},{"categories":["文档"],"content":"sqlx 库的使用 ","date":"2023-05-23","objectID":"/golang-sqlx/:0:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"sqlx 的介绍 sqlx 是内置database/sql 软件包的基础上提供了一组拓展 兼容sql原生包，提供了更加优雅的查询，插入函数 ","date":"2023-05-23","objectID":"/golang-sqlx/:1:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"sqlx 安装 go get github.com/jmoiron/sqlx ","date":"2023-05-23","objectID":"/golang-sqlx/:2:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"连接数据库 var db *sqlx.DB func initDB() (err error) { dsn := \"user:password@tcp(127.0.0.1:3306)/sql_test?charset=utf8mb4\u0026parseTime=True\" // 也可以使用MustConnect连接不成功就panic db, err = sqlx.Connect(\"mysql\", dsn) if err != nil { fmt.Printf(\"connect DB failed, err:%v\\n\", err) return } // 设置与数据库最大的打开连接数 db.SetMaxOpenConns(20) // 设置空闲的最大连接数 db.SetMaxIdleConns(10) return } ","date":"2023-05-23","objectID":"/golang-sqlx/:3:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"查询 查询单行的数据示例 // 查询单条数据示例 func queryRowDemo() { sqlStr := \"select id, name, age from user where id=?\" var u user err := db.Get(\u0026u, sqlStr, 1) if err != nil { fmt.Printf(\"get failed, err:%v\\n\", err) return } fmt.Printf(\"id:%d name:%s age:%d\\n\", u.ID, u.Name, u.Age) } 查询多条数据 // 查询多条数据示例 func queryMultiRowDemo() { sqlStr := \"select id, name, age from user where id \u003e ?\" var users []user err := db.Select(\u0026users, sqlStr, 0) if err != nil { fmt.Printf(\"query failed, err:%v\\n\", err) return } fmt.Printf(\"users:%#v\\n\", users) } ","date":"2023-05-23","objectID":"/golang-sqlx/:4:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"插入、更新和删除 sqlx的exec方法与原生sql中的exec使用一致 插入数据 // 插入数据 func insertRowDemo() { sqlStr := \"insert into user(name, age) values (?,?)\" ret, err := db.Exec(sqlStr, \" 张三\", 18) if err != nil { fmt.Printf(\"insert failed, err:%v\\n\", err) return } theID, err := ret.LastInsertId() // 新插入数据的id if err != nil { fmt.Printf(\"get lastinsert ID failed, err:%v\\n\", err) return } fmt.Printf(\"insert success, the id is %d.\\n\", theID) } 更新数据 // 更新数据 func updateRowDemo() { sqlStr := \"update user set age=? where id = ?\" ret, err := db.Exec(sqlStr, 18, 1) if err != nil { fmt.Printf(\"update failed, err:%v\\n\", err) return } n, err := ret.RowsAffected() // 操作影响的行数 if err != nil { fmt.Printf(\"get RowsAffected failed, err:%v\\n\", err) return } fmt.Printf(\"update success, affected rows:%d\\n\", n) } 删除数据 // 删除数据 func deleteRowDemo() { sqlStr := \"delete from user where id = ?\" ret, err := db.Exec(sqlStr, 6) if err != nil { fmt.Printf(\"delete failed, err:%v\\n\", err) return } n, err := ret.RowsAffected() // 操作影响的行数 if err != nil { fmt.Printf(\"get RowsAffected failed, err:%v\\n\", err) return } fmt.Printf(\"delete success, affected rows:%d\\n\", n) } ","date":"2023-05-23","objectID":"/golang-sqlx/:5:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"NameExec DB.NameExec 方法用来绑定的SQL语句与结构体或map中的同名字段 func insertUserDemo()(err error){ sqlStr := \"INSERT INTO user (name,age) VALUES (:name,:age)\" _, err = db.NamedExec(sqlStr, map[string]interface{}{ \"name\": \"张三\", \"age\": 18, }) return } ","date":"2023-05-23","objectID":"/golang-sqlx/:6:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"NameQuery 使用此数据库进行命名查询。任何命名的占位符参数都被arg中的字段替换。 func namedQuery() { sqlStr := \"SELECT * FROM user WHERE name=:name\" // 使用map做命名查询 rows, err := db.NamedQuery(sqlStr, map[string]interface{}{\"name\": \"张三\"}) if err != nil { fmt.Printf(\"db.NamedQuery failed, err:%v\\n\", err) return } defer rows.Close() for rows.Next() { var u user err := rows.StructScan(\u0026u) if err != nil { fmt.Printf(\"scan failed, err:%v\\n\", err) continue } fmt.Printf(\"user:%#v\\n\", u) } u := user{ Name: \"张三\", } // 使用结构体命名查询，根据结构体字段的 db tag进行映射 rows, err = db.NamedQuery(sqlStr, u) if err != nil { fmt.Printf(\"db.NamedQuery failed, err:%v\\n\", err) return } defer rows.Close() for rows.Next() { var u user err := rows.StructScan(\u0026u) if err != nil { fmt.Printf(\"scan failed, err:%v\\n\", err) continue } fmt.Printf(\"user:%#v\\n\", u) } } ","date":"2023-05-23","objectID":"/golang-sqlx/:7:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"事务操作 对于事物操作， 使用sqlx中提供 db.Begin() 和 tx.Exec() 方法 func transactionDemo2()(err error) { tx, err := db.Beginx() // 开启事务 if err != nil { fmt.Printf(\"begin trans failed, err:%v\\n\", err) return err } defer func() { if p := recover(); p != nil { tx.Rollback() panic(p) // re-throw panic after Rollback } else if err != nil { fmt.Println(\"rollback\") tx.Rollback() // err is non-nil; don't change it } else { err = tx.Commit() // err is nil; if Commit returns error update err fmt.Println(\"commit\") } }() sqlStr1 := \"Update user set age=20 where id=?\" rs, err := tx.Exec(sqlStr1, 1) if err!= nil{ return err } n, err := rs.RowsAffected() if err != nil { return err } if n != 1 { return errors.New(\"exec sqlStr1 failed\") } sqlStr2 := \"Update user set age=50 where i=?\" rs, err = tx.Exec(sqlStr2, 5) if err!=nil{ return err } n, err = rs.RowsAffected() if err != nil { return err } if n != 1 { return errors.New(\"exec sqlStr1 failed\") } return err } ","date":"2023-05-23","objectID":"/golang-sqlx/:8:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"sqlx.In sqlx.In的批量插入示例 创建一个user表 CREATE TABLE `user` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `name` VARCHAR(20) DEFAULT '', `age` INT(11) DEFAULT '0', PRIMARY KEY(`id`) )ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4; ","date":"2023-05-23","objectID":"/golang-sqlx/:9:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"结构体 定义一个结构体 type User struct { Name string `db:\"name\"` Age int `db:\"age\"` } ","date":"2023-05-23","objectID":"/golang-sqlx/:9:1","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"bingvars (绑定变量) 查询占位符**？**在内部成为 bindvars（查询占位符） MySQL中使用? PostgreSQL使用枚举的$1、$2等bindvar语法 SQLite中?和$1的语法都支持 Oracle中使用:name的语法 ","date":"2023-05-23","objectID":"/golang-sqlx/:9:2","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"自己拼接语句 // BatchInsertUsers 自行构造批量插入的语句 func BatchInsertUsers(users []*User) error { // 存放 (?, ?) 的slice valueStrings := make([]string, 0, len(users)) // 存放values的slice valueArgs := make([]interface{}, 0, len(users) * 2) // 遍历users准备相关数据 for _, u := range users { // 此处占位符要与插入值的个数对应 valueStrings = append(valueStrings, \"(?, ?)\") valueArgs = append(valueArgs, u.Name) valueArgs = append(valueArgs, u.Age) } // 自行拼接要执行的具体语句 stmt := fmt.Sprintf(\"INSERT INTO user (name, age) VALUES %s\", strings.Join(valueStrings, \",\")) _, err := DB.Exec(stmt, valueArgs...) return err } ","date":"2023-05-23","objectID":"/golang-sqlx/:9:3","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"使用sqlx.In 实现批量插入 实现结构体 driver.Valuer 接口 func (u User) Value() (driver.Value, error) { return []interface{}{u.Name, u.Age}, nil } // BatchInsertUsers2 使用sqlx.In帮我们拼接语句和参数, 注意传入的参数是[]interface{} func BatchInsertUsers2(users []interface{}) error { query, args, _ := sqlx.In( \"INSERT INTO user (name, age) VALUES (?), (?), (?)\", users..., // 如果arg实现了 driver.Valuer, sqlx.In 会通过调用 Value()来展开它 ) fmt.Println(query) // 查看生成的querystring fmt.Println(args) // 查看生成的args _, err := DB.Exec(query, args...) return err } ","date":"2023-05-23","objectID":"/golang-sqlx/:9:4","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"使用NamedExec实现批量插入 // BatchInsertUsers3 使用NamedExec实现批量插入 func BatchInsertUsers3(users []*User) error { _, err := DB.NamedExec(\"INSERT INTO user (name, age) VALUES (:name, :age)\", users) return err } ","date":"2023-05-23","objectID":"/golang-sqlx/:9:5","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"三种方法的示例 func main() { err := initDB() if err != nil { panic(err) } defer DB.Close() u1 := User{Name: \"张三\", Age: 18} u2 := User{Name: \"李四\", Age: 19} u3 := User{Name: \"王五\", Age: 20} // 方法1 users := []*User{\u0026u1, \u0026u2, \u0026u3} err = BatchInsertUsers(users) if err != nil { fmt.Printf(\"BatchInsertUsers failed, err:%v\\n\", err) } // 方法2 users2 := []interface{}{u1, u2, u3} err = BatchInsertUsers2(users2) if err != nil { fmt.Printf(\"BatchInsertUsers2 failed, err:%v\\n\", err) } // 方法3 users3 := []*User{\u0026u1, \u0026u2, \u0026u3} err = BatchInsertUsers3(users3) if err != nil { fmt.Printf(\"BatchInsertUsers3 failed, err:%v\\n\", err) } } ","date":"2023-05-23","objectID":"/golang-sqlx/:10:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"sqlx.in 的查询示例 ","date":"2023-05-23","objectID":"/golang-sqlx/:11:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"in查询 查询id在给定id集合的数据 // QueryByIDs 根据给定ID查询 func QueryByIDs(ids []int)(users []User, err error){ // 动态填充id query, args, err := sqlx.In(\"SELECT name, age FROM user WHERE id IN (?)\", ids) if err != nil { return } // sqlx.In 返回带 `?` bindvar的查询语句, 我们使用Rebind()重新绑定它 query = DB.Rebind(query) err = DB.Select(\u0026users, query, args...) return } ","date":"2023-05-23","objectID":"/golang-sqlx/:11:1","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"in查询和FIND_IN_SET 函数 查询ID在给定ID集合的数据并维持给定id集合的顺序 // QueryAndOrderByIDs 按照指定id查询并维护顺序 func QueryAndOrderByIDs(ids []int)(users []User, err error){ // 动态填充id strIDs := make([]string, 0, len(ids)) for _, id := range ids { strIDs = append(strIDs, fmt.Sprintf(\"%d\", id)) } query, args, err := sqlx.In(\"SELECT name, age FROM user WHERE id IN (?) ORDER BY FIND_IN_SET(id, ?)\", ids, strings.Join(strIDs, \",\")) if err != nil { return } // sqlx.In 返回带 `?` bindvar的查询语句, 我们使用Rebind()重新绑定它 query = DB.Rebind(query) err = DB.Select(\u0026users, query, args...) return } 参考链接 https://www.liwenzhou.com/posts/Go/sqlx/ https://pkg.go.dev/github.com/jmoiron/sqlx ","date":"2023-05-23","objectID":"/golang-sqlx/:12:0","tags":["golang","mysql"],"title":"Golang Sqlx","uri":"/golang-sqlx/"},{"categories":["文档"],"content":"Go 操作 sql server docker 安装 sqlserver #拉取镜像 docker pull mcr.microsoft.com/azure-sql-edge #启动容器 docker run --name azuresqledge -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=yourStrong(!)Password' -d -p 1433:1433 mcr.microsoft.com/azure-sql-edge 连接SQL数据库，要加载目标数据库的驱动，驱动里面包含了与数据库交互的逻辑 sql.Open() 数据库驱动的名称 数据源名称 得到一个指向sql.DB这个struct的指针 sql.DB 时用来操作数据库的，它代码0个或多个底层连接的池，这些连接由sql包来维护，sql包自动的创建和释放这些连接 它对于多个goroutine并发的使用时安全的 package main import ( \"context\" \"database/sql\" \"fmt\" \"log\" _ \"github.com/denisenkom/go-mssqldb\" ) var db *sql.DB const ( server = \"localhost\" port = \"1433\" user = \"sa\" password = \"Password\" database = \"go-db\" ) func main() { connStr := fmt.Sprintf(\"server=%s;user id=%s;password=%s;port=%s;database=%s;\", server, user, password, port, database) db, err := sql.Open(\"sqlserver\", connStr) if err != nil { log.Fatalln(err.Error()) } ctx := context.Background() err = db.PingContext(ctx) if err != nil { log.Fatalln(err.Error()) } fmt.Println(\"Connected!\") } note Open() 函数并不会连接到数据库，甚至不会验证参数，它只是把后续连接到数据库锁必须的struct给设置好 而真正的连接时在被需要的时候才进行懒设置的 sql.DB 不需要进行关闭 它就是用来连接数据库的， 而不是实际的连接 这个抽象保护了数据库连的池， 对此进行维护 使用sql.DB 的时候，可以定义它的全局变量进行使用， 也可以将它传递给函数/方法里 ","date":"2023-05-21","objectID":"/golang-mysql-crud/:0:0","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"如何获取驱动 正常的做法是使用sql.Register() 函数， 数据库驱动的名称和一个实现了driver.Driver 接口的struct， 来注册数据的驱动 sql.Register(“sqlserver”, \u0026drv{}) 为什么例子中没有这句话 因为 sql 驱动， 在这个包别引入的时候进行自我注册 _ \"github.com/denisenkom/go-mssqldb\" ","date":"2023-05-21","objectID":"/golang-mysql-crud/:1:0","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"驱动自我注册 在go-mssqldb包被引入的时候， 它的init函数将会运行并进行自我注册 在引入go-mssqldb 包的时候， 把该包的名称设置为下划线_，这是因为我们不能直接使用数据库驱动 未来升级驱动， 无需改变代码 Go语言没有提供官方的数据库驱动， 所有的数据库驱动都是第三方驱动，都遵循sql.driver包里面定义的接口 ","date":"2023-05-21","objectID":"/golang-mysql-crud/:2:0","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"func (db *DB) PingContext(ctx context.Context) PingContext函数用来验证与数据库的连接是否仍然生效， ，如有必要则建立一个连接 这个韩式需要一个Context（上下文）类型的参数， 这种类型可以携带 截止时间。取消信号和其他请求范围的值，并且可以横跨API边界和进程 上例中， 创建context使用的context.Background()函数， 该函数返回一个非nil的空context，它不会被取消，它没有值，没有截止时间 通常在main函数， 初始化或测试中 作为传入请求的context ","date":"2023-05-21","objectID":"/golang-mysql-crud/:3:0","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"查询 sql.Db 类型用于查询的方法有 Query QueryRow QueryContext QueryRowContext ","date":"2023-05-21","objectID":"/golang-mysql-crud/:4:0","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"Rows 返回类型 type Rows struct { // contains filtered or unexported fields } Rows的方法 func (*Rows) Close error func (rs *Rows) ColumnTypes() ([]*ColumnType, error) func (rs *Rows) Columns() ([]string, error) func (rs *Rows) Err() error func (rs *Rows) Next() bool func (rs *Rows) NextResultSet() bool func (rs *Rows) Scan(dest …any) error ","date":"2023-05-21","objectID":"/golang-mysql-crud/:4:1","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"QueryRow 返回类型是 type Row struct { // contains filtered or unexported fields } Row的方法 func (r *Row) Err() error func (r *Row) Scan(dest …any) error ","date":"2023-05-21","objectID":"/golang-mysql-crud/:4:2","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"单条查询 func getone(id int)(a app,err error) { a = app{} err = db.QueryRow(\"SELECT Id, Name, Status, Level, Order From dbo.App WHERE Id=@Id\", sql.Named(\"Id\", id)). Scan(\u0026a.ID, \u0026a.name, \u0026a.status, \u0026a.level, \u0026a.order) return } ","date":"2023-05-21","objectID":"/golang-mysql-crud/:4:3","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"多条查询 func getMany(id int) (apps []app, err error) { rows, err := db.Query(\"SELECT Id, Name, Status, Level, Order From dbo.App WHERE Id\u003e@Id\", sql.Named(\"Id\", id)) for rows.Next() { a := app{} err = rows.Scan(\u0026a.ID, \u0026a.name, \u0026a.status, \u0026a.level, \u0026a.order) if err != nil { log.Fatalln(err.Error()) } apps = append(apps, a) } return } ","date":"2023-05-21","objectID":"/golang-mysql-crud/:4:4","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"更新 sql.DB 类型上用于更新的方法 Exec ExecContext func (a *app) update() (err error) { _, err = db.Exec(\"UPDATE dbo.App SET Name=@Name, Order=@Order WHERE Id=@Id\", sql.Named(\"Name\", a.name), sql.Named(\"Order\", a.order), sql.Named(\"Id\", a.ID)) if err != nil { log.Fatalln(err.Error()) } return } ","date":"2023-05-21","objectID":"/golang-mysql-crud/:5:0","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"删除 func (a *app) delete() (err error) { _, err = db.Exec(\"DELETE FROM dbo.App WHERE Id=@Id\", sql.Named(\"Id\", a.ID)) if err != nil { log.Fatalln(err.Error()) } return } ","date":"2023-05-21","objectID":"/golang-mysql-crud/:6:0","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"其他 Prepare prepareContext Transactions Begin BeginTx ","date":"2023-05-21","objectID":"/golang-mysql-crud/:7:0","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"Prepare // 预处理查询示例 func prepareQueryDemo() { sqlStr := \"select id, name, age from user where id \u003e ?\" stmt, err := db.Prepare(sqlStr) if err != nil { fmt.Printf(\"prepare failed, err:%v\\n\", err) return } defer stmt.Close() rows, err := stmt.Query(0) if err != nil { fmt.Printf(\"query failed, err:%v\\n\", err) return } defer rows.Close() // 循环读取结果集中的数据 for rows.Next() { var u user err := rows.Scan(\u0026u.id, \u0026u.name, \u0026u.age) if err != nil { fmt.Printf(\"scan failed, err:%v\\n\", err) return } fmt.Printf(\"id:%d name:%s age:%d\\n\", u.id, u.name, u.age) } } ","date":"2023-05-21","objectID":"/golang-mysql-crud/:7:1","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"Transactions // 事务操作示例 func transactionDemo() { tx, err := db.Begin() // 开启事务 if err != nil { if tx != nil { tx.Rollback() // 回滚 } fmt.Printf(\"begin trans failed, err:%v\\n\", err) return } sqlStr1 := \"Update user set age=30 where id=?\" ret1, err := tx.Exec(sqlStr1, 2) if err != nil { tx.Rollback() // 回滚 fmt.Printf(\"exec sql1 failed, err:%v\\n\", err) return } affRow1, err := ret1.RowsAffected() if err != nil { tx.Rollback() // 回滚 fmt.Printf(\"exec ret1.RowsAffected() failed, err:%v\\n\", err) return } sqlStr2 := \"Update user set age=40 where id=?\" ret2, err := tx.Exec(sqlStr2, 3) if err != nil { tx.Rollback() // 回滚 fmt.Printf(\"exec sql2 failed, err:%v\\n\", err) return } affRow2, err := ret2.RowsAffected() if err != nil { tx.Rollback() // 回滚 fmt.Printf(\"exec ret1.RowsAffected() failed, err:%v\\n\", err) return } fmt.Println(affRow1, affRow2) if affRow1 == 1 \u0026\u0026 affRow2 == 1 { fmt.Println(\"事务提交啦...\") tx.Commit() // 提交事务 } else { tx.Rollback() fmt.Println(\"事务回滚啦...\") } fmt.Println(\"exec trans success!\") } ","date":"2023-05-21","objectID":"/golang-mysql-crud/:7:2","tags":["golang","mysql"],"title":"Golang Mysql CRUD","uri":"/golang-mysql-crud/"},{"categories":["文档"],"content":"docker常用命令整理 ","date":"2023-05-21","objectID":"/docker-command/:0:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker images docker image pull ：下载镜像 docker image ls：列出本地存储的镜像,参数 –digests查看镜像的SHA26签名 docker image inspect:展示镜像细节。包括镜像层数据和元数据。 docker image rm：删除镜像。 docker提供参数 –filter 来过滤docker image ls 命令返回镜像列表的内容 返回悬虚(dangling)镜像 lhf@lhf-virtual-machine:~$ docker image ls --filter dangling=true REPOSITORY TAG IMAGE ID CREATED SIZE \u003cnone\u003e \u003cnone\u003e 没有标签的镜像称为悬虚镜像，列表显示: 使用docker image prune 移除全部悬虚镜像。加-a参数，Docker会额外移除没有被使用的镜像。 ","date":"2023-05-21","objectID":"/docker-command/:1:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker支持的过滤方式 dangling： 可以指定true或false，返回悬虚镜像(true)和非悬虚镜像(false). before: 需要镜像名称和ID作为参数，返回在之前被创建的镜像 since: before类似，返回需要指定镜像之后创建的全部镜像 label: 根据备注（label）的名称或者值 reference: 过滤标签lastest镜像 lhf@lhf-virtual-machine:~$ docker image ls --filter=reference=\"*:latest\" REPOSITORY TAG IMAGE ID CREATED SIZE test latest e63fd667d16a 2 days ago 71.4MB alpine latest 965ea09ff2eb 4 days ago 5.55MB ubuntu latest cf0f3ca922e0 7 days ago 64.2MB centos latest 0f3e07c0138f 3 weeks ago 220MB 通过–format 参数来通过go模板对输出内容格式化 只返回docker主机上镜像的大小属性 lhf@lhf-virtual-machine:~$ docker image ls --format \"{{.Size}}\" 71.4MB 5.55MB 64.2MB 220MB 只返回显示仓库、标签和大小的信息 lhf@lhf-virtual-machine:~$ docker image ls --format \"{{.Repository}}:{{.Tag}}:{{.Size}}\" test:latest:71.4MB alpine:latest:5.55MB ubuntu:latest:64.2MB centos:latest:220MB ","date":"2023-05-21","objectID":"/docker-command/:1:1","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"通过CLI方式搜索Docker Hub lhf@lhf-virtual-machine:~$ docker search alpine NAME DESCRIPTION STARS OFFICIAL AUTOMATED alpine A minimal Docker image based on Alpine Linux… 5757 [OK] mhart/alpine-node Minimal Node.js built on Alpine Linux 444 anapsix/alpine-java Oracle Java 8 (and 7) with GLIBC 2.28 over A… 427 \u003csnip\u003e lhf@lhf-virtual-machine:~$ docker search alpine --filter \"is-official=true\" NAME DESCRIPTION STARS OFFICIAL AUTOMATED alpine A minimal Docker image based on Alpine Linux… 5757 [OK] 使用参数 –digests 在本地查看镜像摘要 lhf@lhf-virtual-machine:~$ docker image ls --digests alpine REPOSITORY TAG DIGEST IMAGE ID CREATED SIZE alpine latest sha256:c19173c5ada610a5989151111163d28a67368362762534d8a8121ce95cf2bd5a 965ea09ff2eb 4 days ago 5.55MB 删除docker主机的全部镜像 lhf@lhf-virtual-machine:~$ docker image rm $(docker image ls -q) -f ","date":"2023-05-21","objectID":"/docker-command/:1:2","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker container docker container run:启动rongq Ctrl-PQ:断开Shell与容器的连接 docker container ls:列出运行状态的容器 docker container exec:允许用户在运行状态的容器，启动一个新的进程。 docker container stop：停止运行中的容器。 docker container start:重启处于停止状态的容器。 docker container rm：删除停止状态的容器。 docker container inspect:显示容器的配置细节和运行时信息。 -it参数： 使当前重点连接到容器的shell终端上 $ docker container run -it ubuntu /bin/bash 快速清理容器 $ docker container rm $(docker container ls -aq) -f ","date":"2023-05-21","objectID":"/docker-command/:2:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker应用容器化 docker image build ：读取Dockerfile文件,将应用程序容器化 使用-t参数文件镜像打标签 使用-f参数指定任意路径下的Dockerfile Dockerfile中FROM指令，指定构建镜像的一个基础层 Dockerfile中RUN指令，在镜像中执行命令，创建新的镜像层 Dockerfile中COPY指令，将文件作为新的层添加到镜像中 Dockerfile中EXPOSR指令，记录应用所使用的的网络端口 Dockerfile中ENTRYPOINT指令，指定镜像已容器的方式启动后默认运行程序 查看镜像构建执行了那些指令 $ docker image history lhfdocker/web:latest 查看镜像的构建详情 $ docker image inspect lhfdocker/web:latest ","date":"2023-05-21","objectID":"/docker-command/:3:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"Docker Compose docker-compose up ：部署一个compose应用。默认读取docker-compose.yml文件，可以使有-f参数指定文件,-d 参数在后台启动 docker-compose stop：停止compose应用的相关容器。可以通过docker-compose restart重新启动 docker-compose rm：删除已停止的compose应用的容器，会删除容器和网络，不会删除卷和镜像。 docker-compose restart 重启compose应用 如果compose应用进行了变更,需要重启才能生效 docker-compose ps:列出compose应用的容器 输出内容包括：状态、容器的运行命令，已经网络端口 docker-compose down:停止并删除运行中compose 的应用。会删除容器和网络，不会删除卷和镜像 ","date":"2023-05-21","objectID":"/docker-command/:4:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker Swarm docker swarm init 创建一个新的swarm,执行这个命令的节点称为管理节点 docker swarm join-token 查询接入管理节点和工作节点到现有swarm时所使用的命令和Token 增加管理节点——docker swarm join-token manager 增加工作节点——docker swarm join-token work docker node ls 列出swarm所有节点及相关信息 docker service create 创建新服务 docker service ls 列出swarm中运行的服务 docker service ps 获取更多关于服务服务的信息 docker service inspect 获取服务的详细信息 docker service scale 用于对服务副本数量进行增减 docker service update 对运行中的服务进行属性变更 docker service logs 查看服务的日志 docker service rm 从swarm删除服务 ","date":"2023-05-21","objectID":"/docker-command/:5:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker network docker network ls ：列出运行在本地的docker主机的全部网络 docker network create :创建新的docker网络。默认采用的是bridge 加-d参数指定(网络类型) docker network inspect:提供docker网络的详细配置信息。 docker network prune:删除docker主机上全部未使用的网络。 docker network rm :删除docker主机上指定的网络 ","date":"2023-05-21","objectID":"/docker-command/:6:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"docker volume docker volume create ：创建新卷，默认使用的local驱动，加-d参数指定不同驱动 docker volume ls :查看docker主机的全部卷 docker volume inspect: 查看卷的详细信息 docker volume prune ：删除未被容器和服务使用的卷 docker volume rm:删除指定卷 ","date":"2023-05-21","objectID":"/docker-command/:7:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":"Docker Stack docker stack deploy： 用于根据stack文件部署和更新stack服务 docker stack ls ：列出swarm集群中所有的stack docker stack ps: 列出某个已经部署的stack的相关信息。 docker stack rm：从swarm集群中移除stack ","date":"2023-05-21","objectID":"/docker-command/:8:0","tags":["docker"],"title":"Docker Command","uri":"/docker-command/"},{"categories":["文档"],"content":" Lighthouse (figure) PV、PVC、StorageClass Kubernetes 处理容器持久化存储的核心原理 PV： 持久化存储数据卷 pv 一般有运维人员事先创建之后使用， 定义一个NFS类型的PV apiVersion: v1 kind: PersistentVolume metadata: name: nfs spec: storageClassName: manual capacity: storage: 1Gi accessModes: - ReadWriteMany nfs: server: 10.244.1.4 path: \"/\" PVC： pod所希望使用的持久化存储的属性 一般有开发人员创建， Volume存储的大小，可读写的权限 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs spec: accessModes: - ReadWriteMany storageClassName: manual resources: requests: storage: 1Gi ","date":"2023-05-20","objectID":"/sc-pv-pvc/:0:0","tags":["k8s"],"title":"Kubernets-Sc\u0026Pv\u0026Pvc","uri":"/sc-pv-pvc/"},{"categories":["文档"],"content":"PVC与PV绑定使用的条件 PV和PVC的spec字段，PV的存储大小， 就必须满足PVC的要求 PV 和 PVC 的 storageClassName 字段必须一样 YAML 文件里声明使用这个 PVC 了 apiVersion: v1 kind: Pod metadata: labels: role: web-frontend spec: containers: - name: web image: nginx ports: - name: web containerPort: 80 volumeMounts: - name: nfs mountPath: \"/usr/share/nginx/html\" volumes: - name: nfs persistentVolumeClaim: claimName: nfs ","date":"2023-05-20","objectID":"/sc-pv-pvc/:1:0","tags":["k8s"],"title":"Kubernets-Sc\u0026Pv\u0026Pvc","uri":"/sc-pv-pvc/"},{"categories":["文档"],"content":"PersistentVolumeController 专门处理持久化存储的控制器， 会不断地查看当前每一个 PVC，是不是已经处于 Bound（已绑定）状态。如果不是，那它就会遍历所有的、可用的 PV，并尝试将其与这个“单身”的 PVC 进行绑定 所谓容器的 Volume，其实就是将一个宿主机上的目录，跟一个容器里的目录绑定挂载在了一起 而所谓的“持久化 Volume”，指的就是这个宿主机上的目录，具备“持久性” 这个准备“持久化”宿主机目录的过程，我们可以形象地称为“两阶段处理 一个Pod调度到一个节点上之后， kubelet就要负责这个Pod创建的它的Volume目录，默认这个情况下，kubelet 为 Volume 创建的目录是如下所示的一个宿主机上的路径： /var/lib/kubelet/pods/\u003cPod 的 ID\u003e/volumes/kubernetes.io~\u003cVolume 类型 \u003e/\u003cVolume 名字 \u003e 这一步为虚拟机挂载远程磁盘的操作，对应的正是“两阶段处理”的第一阶段。在 Kubernetes 中，我们把这个阶段称为 Attach 将磁盘设备格式化并挂载到 Volume 宿主机目录的操作，对应的正是“两阶段处理”的第二个阶段，我们一般称为：Mount。 在这一步，kubelet 需要作为 client，将远端 NFS 服务器的目录（比如：“/”目录），挂载到 Volume 的宿主机目录上，即相当于执行如下所示的命令： $ mount -t nfs \u003cNFS 服务器地址 \u003e:/ /var/lib/kubelet/pods/\u003cPod 的 ID\u003e/volumes/kubernetes.io~\u003cVolume 类型 \u003e/\u003cVolume 名字 \u003e kubelet 只要把这个 Volume 目录通过 CRI 里的 Mounts 参数，传递给 Docker，然后就可以为 Pod 里的容器挂载这个“持久化”的 Volume 了。其实，这一步相当于执行了如下所示的命令 $ docker run -v /var/lib/kubelet/pods/\u003cPod 的 ID\u003e/volumes/kubernetes.io~\u003cVolume 类型 \u003e/\u003cVolume 名字 \u003e:/\u003c 容器内的目标目录 \u003e 我的镜像 ... PV 的“两阶段处理”流程，是靠独立于 kubelet 主控制循环（Kubelet Sync Loop）之外的两个控制循环来实现的 StorageClass k8s提供了一套自动创建PV的机制， Dynamic Provisioning storageClass对象的作用， 其实就是创建PV的模板 主要定义两部分 PV的属性， （存储类型， Volume的大小） 创建这种PV需要用到的存储插件， （nfs、Ceph） apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: block-service provisioner: kubernetes.io/gce-pd parameters: type: pd-ssd apiVersion: ceph.rook.io/v1beta1 kind: Pool metadata: name: replicapool namespace: rook-ceph spec: replicated: size: 3 --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: block-service provisioner: ceph.rook.io/block parameters: pool: replicapool #The value of \"clusterNamespace\" MUST be the same as the one in which your rook cluster exist clusterNamespace: rook-ceph Lighthouse (figure) PVC描述的， 是Pod想要使用的持久化存储的属性（存储的大小、读写权限） PV的描述， 一个具体的Volume的属性， （Volume的类型， 挂载目录。 远程存储服务地址） StorageClass 的作用， 充当PV的模板， 只有属于一个StorageClass的PV和PVC才可以绑定子啊一起 ","date":"2023-05-20","objectID":"/sc-pv-pvc/:2:0","tags":["k8s"],"title":"Kubernets-Sc\u0026Pv\u0026Pvc","uri":"/sc-pv-pvc/"},{"categories":["文档"],"content":"Operator 工作原理解读 operator的工作原理和编写方法 ","date":"2023-05-20","objectID":"/operator/:0:0","tags":["k8s"],"title":"Kubernets-Operator","uri":"/operator/"},{"categories":["文档"],"content":"第一步，将这个 Operator 的代码 Clone 到本地： git clone https://github.com/coreos/etcd-operator ","date":"2023-05-20","objectID":"/operator/:0:1","tags":["k8s"],"title":"Kubernets-Operator","uri":"/operator/"},{"categories":["文档"],"content":"第二步，将这个 Etcd Operator 部署在 Kubernetes 集群里 example/rbac/create_role.sh 这个脚本为 Etcd Operator 创建 RBAC 规则， 因为 Etcd Operator 需要访问Kubernetes的APIServer来创建对象 对Pod， service，PVC， Deployment， Secret等API对象， 有所有权限 对CRD对象， 有所有权限 对属于etcd.database.coreos.com 这个 API Group 的 CR（Custom Resource）对象，有所有权限。 Etcd Operator 本身 是一个Deployment apiVersion: extensions/v1beta1 kind: Deployment metadata: name: etcd-operator spec: replicas: 1 template: metadata: labels: name: etcd-operator spec: containers: - name: etcd-operator image: quay.io/coreos/etcd-operator:v0.9.2 command: - etcd-operator env: - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name 创建 Etcd Operator kubectl create -f example/deployment.yaml pod进入Running 状态， 有一个CRD被自动创建出来 $ kubectl get pods NAME READY STATUS RESTARTS AGE etcd-operator-649dbdb5cb-bzfzp 1/1 Running 0 20s $ kubectl get crd NAME CREATED AT etcdclusters.etcd.database.coreos.com 2018-09-18T11:42:55Z 通过 kubectl describe 命令看到它的细节，如下所示： $ kubectl describe crd etcdclusters.etcd.database.coreos.com ... Group: etcd.database.coreos.com Names: Kind: EtcdCluster List Kind: EtcdClusterList Plural: etcdclusters Short Names: etcd Singular: etcdcluster Scope: Namespaced Version: v1beta2 ... ","date":"2023-05-20","objectID":"/operator/:1:0","tags":["k8s"],"title":"Kubernets-Operator","uri":"/operator/"},{"categories":["文档"],"content":"编写EtcdClutser的Yaml， $ kubectl apply -f example/example-etcd-cluster.yaml 这个 example-etcd-cluster.yaml 文件里描述的，是一个 3 个节点的 Etcd 集群 kubectl get pods NAME READY STATUS RESTARTS AGE example-etcd-cluster-dp8nqtjznc 1/1 Running 0 1m example-etcd-cluster-mbzlg6sd56 1/1 Running 0 2m example-etcd-cluster-v6v6s6stxd 1/1 Running 0 2m apiVersion: \"etcd.database.coreos.com/v1beta2\" kind: \"EtcdCluster\" metadata: name: \"example-etcd-cluster\" spec: size: 3 version: \"3.2.13\" ","date":"2023-05-20","objectID":"/operator/:2:0","tags":["k8s"],"title":"Kubernets-Operator","uri":"/operator/"},{"categories":["文档"],"content":"Operator 的工作原理： 实际上利用了Kubernetes的自定义API资源（CRD），来描述我们想要不熟的“有状态应用”，然后再自定义控制器里， 根据自定义API对象的变化， 来完成具体的部署和运维工作 tcd Operator 在业务逻辑的实现方式上 第一个工作只在该CLuster对象第一次被创建的时候会执行， 这个工作， 就是我们前面提到Bootstrap，即：创建一个单节点的种子集群。 Bootstrap，即：创建一个单节点的种子集群。 ","date":"2023-05-20","objectID":"/operator/:3:0","tags":["k8s"],"title":"Kubernets-Operator","uri":"/operator/"},{"categories":["文档"],"content":"基于角色的权限控制：RBAC 在kubernetes项目中 负责完成授权的（Authorization）工作的记住， 就是RBAC， 基于角色的访问控制（Role-based Access Control） 三个基本概念 Role： 角色，它其实是一组规则， 定义了一组对Kubernetes API对象的操作权限 Subject： 被作用者，即可以是“人”， 也可以是“机器” RoleBinding： 定义了“被作用者”和“角色”的绑定关系 ","date":"2023-05-20","objectID":"/rbac/:0:0","tags":["k8s"],"title":"Kubernets-Rbac","uri":"/rbac/"},{"categories":["文档"],"content":"Role kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: mynamespace name: example-role rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] ","date":"2023-05-20","objectID":"/rbac/:0:1","tags":["k8s"],"title":"Kubernets-Rbac","uri":"/rbac/"},{"categories":["文档"],"content":"Subject是如何指定的 通过RoleBinding来实现的 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-rolebinding namespace: mynamespace subjects: - kind: User name: example-user apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: example-role apiGroup: rbac.authorization.k8s.io ","date":"2023-05-20","objectID":"/rbac/:1:0","tags":["k8s"],"title":"Kubernets-Rbac","uri":"/rbac/"},{"categories":["文档"],"content":"User是从哪里来的？ kubernetes 的User ， 只是一个授权系统的系统里的逻辑概念。 通过外部认证服务，比如 keystone 直接给APIServer指定一个用户名、密码文件 RoleBinding对象就可以直接通过名字， 来引用前面定义的Role对象， 从而定义了“被作用者（Subject）”和“角色（Role）”之间的绑定关系 ","date":"2023-05-20","objectID":"/rbac/:2:0","tags":["k8s"],"title":"Kubernets-Rbac","uri":"/rbac/"},{"categories":["文档"],"content":"ClusterRole 和 ClusterRolebind kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-clusterrole rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] 赋予用户所有权限 verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"] 针对某一具体对象进行权限设置 rules: - apiGroups: [\"\"] resources: [\"configmaps\"] resourceNames: [\"my-config\"] verbs: [\"get\"] kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-clusterrolebinding subjects: - kind: User name: example-user apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: example-clusterrole apiGroup: rbac.authorization.k8s.io 这个由 Kubernetes 负责管理的“内置用户”，正是我们前面曾经提到过的：ServiceAccount。 ","date":"2023-05-20","objectID":"/rbac/:3:0","tags":["k8s"],"title":"Kubernets-Rbac","uri":"/rbac/"},{"categories":["文档"],"content":"定义一个ServiceAccount apiVersion: v1 kind: ServiceAccount metadata: namespace: mynamespace name: example-sa 编写RoleBinding的Yaml 文件， 为这个ServiceAccount分配权限 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-rolebinding namespace: mynamespace subjects: - kind: ServiceAccount name: example-sa namespace: mynamespace roleRef: kind: Role name: example-role apiGroup: rbac.authorization.k8s.io 创建这个对象 $ kubectl create -f svc-account.yaml $ kubectl create -f role-binding.yaml $ kubectl create -f role.yaml 查看详情 $ kubectl get sa -n mynamespace -o yaml - apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: 2018-09-08T12:59:17Z name: example-sa namespace: mynamespace resourceVersion: \"409327\" ... secrets: - name: example-sa-token-vmfg6 ","date":"2023-05-20","objectID":"/rbac/:4:0","tags":["k8s"],"title":"Kubernets-Rbac","uri":"/rbac/"},{"categories":["文档"],"content":"声明式API与Kubernetes编码范式 kubectl create 再replace的操作， 称为命令式配置文件操作 声明式API https://hugbz2.51cg3.co/archives/25451 /https://hugbz2.51cg3.co/archives/4081/ kuberctl apply 命令就是 声明式API ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:0:0","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":["文档"],"content":"apply 与 replace命令的本质区别 replace： 是使用新的YAML文件中的API对象，替换原来的API对象 apply： 执行了一个对原有的API对象的PATCH操作 对于kube-apiserver在响应命令式请求的时候 replace： 只能处理一个写请求， 否则会有产生冲突的可能 apply： 一次能处理多个写操作， 并且具备Merge能力 Istio Istio最根本的组件， 是运行在每一个应用Pod里的Envoy容器 Istio项目， 代理服务以sidecar容器的方式， 运行在每一个被治理的应用Pod中， Envoy容器就能够通过配置Pod里的iptables规则， 把整个Pod的进出流量接管下来 Istio 的控制层里的Pilot组件，就能够调用每个Envoy容器的API， 对这个Envoy代理进行配置， 从而实现微服务治理。 ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:0:1","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":["文档"],"content":"Istio项目使用的， 是kubernetes中一个非常重要的功能Dynamic Adminssion Control Kubernetes 项目为我们额外提供了一种“热插拔”式的 Admission 机制，它就是 Dynamic Admission Control，也叫作：Initializer。 现在，我给你举个例子。比如，我有如下所示的一个应用 Pod： apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! \u0026\u0026 sleep 3600'] Istio项目要做的， 就是在这个Pod Yaml被提交给kubernetes之后， 它对应的API对象字段加上Envoy容器的配置， apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! \u0026\u0026 sleep 3600'] - name: envoy image: lyft/envoy:845747b88f102c0fd262ab234308e9e22f693a1 command: [\"/usr/local/bin/envoy\"] ... Istio 要做的，就是编写一个用来为 Pod“自动注入”Envoy 容器的 Initializer。 首先， Istio会将这个Envoy容器本身的定义， 以ConfigMap方式保存在Kubernetes当前 apiVersion: v1 kind: ConfigMap metadata: name: envoy-initializer data: config: | containers: - name: envoy image: lyft/envoy:845747db88f102c0fd262ab234308e9e22f693a1 command: [\"/usr/local/bin/envoy\"] args: - \"--concurrency 4\" - \"--config-path /etc/envoy/envoy.json\" - \"--mode serve\" ports: - containerPort: 80 protocol: TCP resources: limits: cpu: \"1000m\" memory: \"512Mi\" requests: cpu: \"100m\" memory: \"64Mi\" volumeMounts: - name: envoy-conf mountPath: /etc/envoy volumes: - name: envoy-conf configMap: name: envoy Initializer 更新用户的P大对象的时候， 必须使用PATCH API 来完成， 而这种PATCH API 正式声明式API的主要能力 Istio 将一个编写好的Initializer， 作为一个Pod部署在kubernetes中 apiVersion: v1 kind: Pod metadata: labels: app: envoy-initializer name: envoy-initializer spec: containers: - name: envoy-initializer image: envoy-initializer:0.0.1 imagePullPolicy: Always 不断获取“实际状态”， 然后“期望状态”做对比 initializer的控制器， 不断获取的“实际状态”用户新创建的Pod “期望状态” 就是破的被添加的Envoy容器的定义 有了这个 TwoWayMergePatch 之后，Initializer 的代码就可以使用这个 patch 的数据，调用 Kubernetes 的 Client，发起一个 PATCH 请求 Istio项目的核心， 就是由无数个运行在应用Pod中的Envoy容器组成的服务代理网格。 ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:0:2","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":["文档"],"content":"kubernetes “声明式API”的独到之处 首先， 所谓“声明式”， 我们定义个定义好的API对象来“声明”， 所期望的状态是什么样子的 其次， “声明式API”允许有多个API写端， 以PATCH的方式对API对象进行修改，而无需关心本地原始的YAML文件的内容 最后， Kubernetes项目才可以基于对API对象的增、删、该、查， 在完全无需外界干扰的情况下， 完成对“实际状态”和“期望状态”的协调过程 ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:1:0","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":["文档"],"content":"Kubernetes 编程范式 如何使用控制器模式，同Kubernetes里API对象“增、删、改、查”进行协作，完成用户业务逻辑的编写过程 ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:2:0","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":["文档"],"content":"为 Network 这个自定义 API 对象编写一个自定义控制器（Custom Controller） 你好， 监控看到15、16、17这三台机器资源使用率和负载都太不高。 目前现在我们这边资源需求较多，计划3月8号（下周三）回收这3台GPU机器。请知悉。 ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:3:0","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":["文档"],"content":"自定义控制器的原理 控制器的工作流程 从kubernetes的APISerer里获取它所关心的对象， 就是自定义的Network informer与API 对象时——对应的， 所以我传递给自定义控制器的， 是Network对象informer Network Informer跟APIServer建立连接， 是informer所使用的Reflector包 Reflector 使用的ListAndWatch的方法，来“获取”并“监听”这些Network对象实例的变化 在ListAndWatch机制下， 一旦APIServer端有新的Network实例被创建、删除或者更新Reflector都会收到“事件通知”， 会放进一个Delta FIFO Queue（增量先进先出队列）中 informe会不断从这个Delta FIFO Queue 读取增量， 每拿到一个增量， informer 判断这个增量的事件类型。 然后创建或者更新本地对象的缓存。（这个缓存在kubernetes里叫Store） informer职责 同步本地缓存的工作 根据这些事件的类型，触发事先注册号的ResourceEventHandler informer 是一个钓友本地缓存和索引机制的 可以注册的EventHandler的client ","date":"2023-05-20","objectID":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/:4:0","tags":["k8s"],"title":"Kubernets-声明式API","uri":"/%E5%A3%B0%E6%98%8E%E5%BC%8Fapi/"},{"categories":["文档"],"content":"Job 与 CronJob ","date":"2023-05-20","objectID":"/job_cronjob/:0:0","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"Job API apiVersion: batch/v1 kind: Job metadata: name: pi spec: template: spec: containers: - name: pi image: resouer/ubuntu-bc command: [\"sh\", \"-c\", \"echo 'scale=10000; 4*a(1)' | bc -l \"] restartPolicy: Never backoffLimit: 4 计算π值的容器。而通过 scale=10000，我指定了输出的小数点后的位数是 10000 ","date":"2023-05-20","objectID":"/job_cronjob/:0:1","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"创建job $ kubectl create -f job.yaml 查看job 对象 $ kubectl describe jobs/pi Name: pi Namespace: default Selector: controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495 Labels: controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495 job-name=pi Annotations: \u003cnone\u003e Parallelism: 1 Completions: 1 .. Pods Statuses: 0 Running / 1 Succeeded / 0 Failed Pod Template: Labels: controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495 job-name=pi Containers: ... Volumes: \u003cnone\u003e Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 1m 1m 1 {job-controller } Normal SuccessfulCreate Created pod: pi-rq5rl pod模板， 会自动加上一个controller-uid= 随机字符串 job对象本身， 也会自动加上label的对应的Selector， 保证job与他所管理的Pod之间的匹配关系 这种自动生成的Label对用户来说并不友好， 所以不太适合推广到Deployment等长作业编排对象上。 事实上， restartPolicy在job对象里只允许被设置为Never 和 OnFailure 而在deployment对象里， restartPolicy则只允许被设置为Always ","date":"2023-05-20","objectID":"/job_cronjob/:0:2","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"查看pod日志 $ kubectl logs pi-rq5rl 3.141592653589793238462643383279... ","date":"2023-05-20","objectID":"/job_cronjob/:0:3","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"离线作业失败了怎么办？ job 定义了 restartPolicy=Never， 那么离线作业失败后 job Controller 就会不断尝试创建一个新的pod 在job对象的spec.backoffLimit字段字段里定义了重试次数为 4（即，backoffLimit=4）， 默认为6 重新创建Pod的间隔是呈指数增加的， 重新创建Pod的发生在 10 s、20 s、40 s 定义 restartPolicy=OnFailure, 那么离线作业失败后， JOb Controller就不会尝试创建新的Pod， 但是会不断尝试重启Pod的容器 在 spec.activeDeadlineSeconds 字段可以设置最长运行时间 spec: backoffLimit: 5 activeDeadlineSeconds: 100 一旦运行了100s, 这个Job的所有Pod都会被终止 你可以在 Pod 的状态里看到终止的原因是 reason: DeadlineExceeded。 ","date":"2023-05-20","objectID":"/job_cronjob/:0:4","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"Job Controller 对并行作业的控制方法 负责控制并行控制的两个参数 spec.parallelism: 定义一个Job在任意时间最多可以启动多少个Pod同事运行 spec.completions: 定义Job至少要完成的Pod数目， 即Job的最小完成数 创建一个参数例子 apiVersion: batch/v1 kind: Job metadata: name: pi spec: parallelism: 2 completions: 4 template: spec: containers: - name: pi image: resouer/ubuntu-bc command: [\"sh\", \"-c\", \"echo 'scale=5000; 4*a(1)' | bc -l \"] restartPolicy: Never backoffLimit: 4 创建job $ kubectl create -f job.yaml 查看job $ kubectl get job NAME DESIRED SUCCESSFUL AGE pi 4 0 3s DESIRED的值 正是completions定义的最小完成数 $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-gmcq5 0/1 Completed 0 40s pi-84ww8 0/1 Pending 0 0s pi-5mt88 0/1 Completed 0 41s pi-62rbt 0/1 Pending 0 0s $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-gmcq5 0/1 Completed 0 40s pi-84ww8 0/1 ContainerCreating 0 0s pi-5mt88 0/1 Completed 0 41s pi-62rbt 0/1 ContainerCreating 0 0s 由于所有Pod均成功退出, job执行完成, 看懂SuCCESSFUL为4 $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-5mt88 0/1 Completed 0 5m pi-62rbt 0/1 Completed 0 4m pi-84ww8 0/1 Completed 0 4m pi-gmcq5 0/1 Completed 0 5m $ kubectl get job NAME DESIRED SUCCESSFUL AGE pi 4 4 5m ","date":"2023-05-20","objectID":"/job_cronjob/:1:0","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"Job Controller的工作原理 首先 job Controller的控制对象直接就是 Pod 其次, job Controller在控制循环中进行的协调操作, 是根据实际的Running状态Pod额数目,已经成功退出的Pod的数目, 以及 parallelism、 conpletions 参数的值共同计算出在这个周期里，应该创建或者删除的Pod数目，然后调用Kubernetes API执行这个操作 ","date":"2023-05-20","objectID":"/job_cronjob/:1:1","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"第一种： 外部管理器+Job模板 apiVersion: batch/v1 kind: Job metadata: name: process-item-$ITEM labels: jobgroup: jobexample spec: template: metadata: name: jobexample labels: jobgroup: jobexample spec: containers: - name: c image: busybox command: [\"sh\", \"-c\", \"echo Processing item $ITEM \u0026\u0026 sleep 5\"] restartPolicy: Never 控制这种 Job 时，我们只要注意如下两个方面即可： 创建 Job 时，替换掉 $ITEM 这样的变量； 所有来自于同一个模板的 Job，都有一个 jobgroup: jobexample 标签，也就是说这一组 Job 使用这样一个相同的标识。 $ mkdir ./jobs $ for i in apple banana cherry do cat job-tmpl.yaml | sed \"s/\\$ITEM/$i/\" \u003e ./jobs/job-$i.yaml done $ kubectl create -f ./jobs $ kubectl get pods -l jobgroup=jobexample NAME READY STATUS RESTARTS AGE process-item-apple-kixwv 0/1 Completed 0 4m process-item-banana-wrsf7 0/1 Completed 0 4m process-item-cherry-dnfu9 0/1 Completed 0 4m ","date":"2023-05-20","objectID":"/job_cronjob/:1:2","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"第二种： 拥有固定任务数目的并行Job apiVersion: batch/v1 kind: Job metadata: name: job-wq-1 spec: completions: 8 parallelism: 2 template: metadata: name: job-wq-1 spec: containers: - name: c image: myrepo/job-wq-1 env: - name: BROKER_URL value: amqp://guest:guest@rabbitmq-service:5672 - name: QUEUE value: job1 restartPolicy: OnFailure ","date":"2023-05-20","objectID":"/job_cronjob/:1:3","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"第三种： 指定并行度（parallelism）， 但不设置固定的completions 这种情况 ， 任务的总数是未知的， 所以不仅需要一个工作队列来负责任务分发， 还需要判断工作列表已经为空。 apiVersion: batch/v1 kind: Job metadata: name: job-wq-2 spec: parallelism: 2 template: metadata: name: job-wq-2 spec: containers: - name: c image: gcr.io/myproject/job-wq-2 env: - name: BROKER_URL value: amqp://guest:guest@rabbitmq-service:5672 - name: QUEUE value: job2 restartPolicy: OnFailure ","date":"2023-05-20","objectID":"/job_cronjob/:1:4","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"CronJob（定时任务） CronJob是一个专门用来管理Job对象的控制器， 它的创建和删除job的依据， 是schedule字段的定义一个标准的Unix Cron格式的表达式。 apiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \"*/1 * * * *\" jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure 由于定时任务的特殊性， 某个job还没有执行完， 另一个新的job就产生了， 通过spec.concurrencyPolicy 字段来定义具体的处理策略 concurrencyPolicy=Allow， 默认情况， 意味着这些job可以同时存在 concurrencyPolicy=Forbid， 意味着不会创建新的pod， 该创建周期被跳过 concurrencyPolicy=Replace， 意味着新产生的job会替换旧的、没有执行完的job 如果某一次 Job 创建失败，这次创建就会被标记为“miss”。当在指定的时间窗口内，miss 的数目达到 100 时，那么 CronJob 会停止再创建这个 Job 这个时间窗口，可以由 spec.startingDeadlineSeconds 字段指定。比如 startingDeadlineSeconds=200，意味着在过去 200 s 里，如果 miss 的数目达到了 100 次，那么这个 Job 就不会被创建执行了。 ","date":"2023-05-20","objectID":"/job_cronjob/:2:0","tags":["k8s"],"title":"Kubernets-Job_Cronjob","uri":"/job_cronjob/"},{"categories":["文档"],"content":"StatefulSet ","date":"2023-05-20","objectID":"/stateful/:0:0","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":["文档"],"content":"概念 StatefulSet 的设计其实非常容器理解， 它把真实世界里的应用状态， 抽象为两个情况 拓扑状态：这种情况意味着， 应用的多个实例之间不是完全对等的关系， 这些应用实例， 不行按照某些顺序启动，比如应用的主节点A要先于节点B启动， 而如果你把A和B两个Pod删除调， 他们再次被创建出来也必须严格安装这个顺序才行， 并且， 新创建出来的Pod，必须和原理的Pod的网络标识一样， 这样原先的访问者才能使用同样的方法， 访问到这个新Pod。 存储状态：这种情况， 应用的多个实例分别绑定了不同的存储数据， 对于这些应用实例来说，Pod A第一次读取到的数据， 和隔了十分钟之后再次读取到的数据， 应该是同一份，哪怕再次期间Pod A被重新创建过， 这个情况 就是一个数据库应用的多个存储实例 StatefulSet的核心功能， 就是通过某种方式记录这些状态，然后再Pod被重新创建时， 能够为新的Pod恢复这些状态 ","date":"2023-05-20","objectID":"/stateful/:0:1","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":["文档"],"content":"Headless Service Service的VIP方式：当我访问 10.0.23.1 这个 Service 的 IP 地址时，10.0.23.1 其实就是一个 VIP，它会把请求转发到该 Service 所代理的某一个 Pod 上 **Service的DNS方式：**这时候，只要我访问“my-svc.my-namespace.svc.cluster.local”这条 DNS 记录，就可以访问到名叫 my-svc 的 Service 所代理的某一个 Pod。 Normal Service： 你访问“my-svc.my-namespace.svc.cluster.local”解析到的，正是 my-svc 这个 Service 的 VIP Headless Service： 你访问“my-svc.my-namespace.svc.cluster.local”解析到的，就是my-svc代理的某个Pod的IP地址， 区别在于Headless Service不需要分配一个VIP， 而是直接以DNS记录方式解析出被代理的Pod的IP地址 ","date":"2023-05-20","objectID":"/stateful/:0:2","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":["文档"],"content":"Headless Service 对应的 YAML 文件： apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx Headless Service \u003cpod-name\u003e.\u003csvc-name\u003e.\u003cnamespace\u003e.svc.cluster.local ","date":"2023-05-20","objectID":"/stateful/:0:3","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":["文档"],"content":"StatefulSet 又是如何使用这个DNS记录来维持Pod的拓扑状态的？ apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \"nginx\" replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.9.1 ports: - containerPort: 80 name: web serviceName=nginx 字段，告诉 StatefulSet控制器， 在执行控制循环（Control Loop）的时候， 请使用nginx 这个 Headless Service 来保证 Pod 的“可解析身份”。 $ kubectl run -i --tty --image busybox dns-test --restart=Never --rm /bin/sh $ nslookup web-0.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-0.nginx Address 1: 10.244.1.8 $ nslookup web-1.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-1.nginx Address 1: 10.244.2.8 Kubernetes 就是成功地将Pod的拓扑状态（比如：哪个节点先启动，哪个节点后启动），按照 Pod 的“名字 + 编号”的方式固定了下来 ","date":"2023-05-20","objectID":"/stateful/:0:4","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":["文档"],"content":"总结 StatefulSet 这个控制器主要作用之一： 就是使用Pod模板创建Pod的时候，对他们进行编号， 并且按照编号顺序逐一完成创建工作， 而当StatefulSet 的”控制循环”发现Pod的“实际状态”与“期望状态”不一致， 需要新建或者删除Pod进行 “调谐”的时候，它会严格按照这些 Pod 编号的顺序，逐一完成这些操作。 深入理解StatefulSet（二）：存储状态 ","date":"2023-05-20","objectID":"/stateful/:0:5","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":["文档"],"content":"工作原理 首信， StatefulSet的控制器直接管理的是Pod， ， 因为StatefulSet里的不同的Pod实例， 不想ReplicaSet中那样都是完全一样的， 而是有了细微区别的， 比如， 每个Pod的hostanme、名字等都是不同的， 携带了标红。 而statefulSet，区分这些实例的方式， 通过在Pod的名字里加上事先约定号的编号。 其次， Kubernetes通过headless ， 为这些有编号的Pod， 在DNS服务器中生成带有同样标红的DNS记录， 只有StatefulSet能够保证这些Pod的名字的编号不变， 那么Service里类似于 web-0.nginx.default.svc.cluster.local 这样的 DNS 记录也就不会变， 而这条记录解析出来的Pod的IP地址， 则会随着后端Pod的删除和再创建而自动更新， 这当然是Service机制本身的能力， 不需要StatefulSet操心 最后， StatefulSet 还为每个Pod分配并创建一个同样编号的PVC，， 这样 kubernetes 就可以通过Persistent Volume 机制为这个PVC绑定上毒药的PV， 从而保证了每个Pode都拥有一个独立的Volume ","date":"2023-05-20","objectID":"/stateful/:0:6","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":["文档"],"content":"总结 StatefulSet 其实就是一种特殊的 Deployment，而其独特之处在于，它的每个 Pod 都被编号了。而且，这个编号会体现在 Pod 的名字和 hostname 等标识信息上，这不仅代表了 Pod 的创建顺序，也是 Pod 的重要网络标识（即：在整个集群里唯一的、可被的访问身份）。 有了这个编号后，StatefulSet 就使用 Kubernetes 里的两个标准功能：Headless Service 和 PV/PVC，实现了对 Pod 的拓扑状态和存储状态的维护。 ","date":"2023-05-20","objectID":"/stateful/:0:7","tags":["k8s"],"title":"Kubernets-StatefulSet","uri":"/stateful/"},{"categories":["文档"],"content":"DaemonSet ","date":"2023-05-20","objectID":"/daemonset/:0:0","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":["文档"],"content":"主要作用 在kubernetes集群里， 运行一个Daemon Pod， 所以， 这个Pod有如下三个特征 这个Pod运载kubernetes集群里的每个节点（Node） 上 每个节点上只有一个这样的Pod实例 当有新的节点加入Kubernetes集群后， 该Pod 会自动的再新节点上被创建出来， 而当节点被删除后， 它上面的Pod也相应会被回收调。 ","date":"2023-05-20","objectID":"/daemonset/:0:1","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":["文档"],"content":"Daemon Pod 的例子 各种网络插件的Agent组件， 都必须运行在每个节点上， 用来处理这个节点上的容器网络 各种存储的插件的Agent组件， 也必须运行在每个节点上， 用来在这个节点上挂载远程存储目录，操作容器的Volume目录 各种监控组件和日志组件， 也必须运行在每个节点上，复制这个节点上监控信息和日志搜索 API apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd-elasticsearch image: k8s.gcr.io/fluentd-elasticsearch:1.20 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers ","date":"2023-05-20","objectID":"/daemonset/:0:2","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":["文档"],"content":"Daemonset 如何确保每个Node上有且只有一个被管理的Pod？ Daemonset Controller ， 首先从Etcd 里获取所有的Node列表，然后遍历所有的Node， 这时， 它就可以很容器的去检查， 当前这个Node上是不是携带了name=fluentd-elasticsearch 标签的 Pod 在运行。 检查结果有三种情况 没有每种Pod， 那么就意味着这个Node创建这样一个Pod 有这种Pod， 但是数量大于1， 那就说明 多余的Pod从这个Node删除掉 正好只有一个这种Pod， 说明这个节点是正常的 ","date":"2023-05-20","objectID":"/daemonset/:1:0","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":["文档"],"content":"nodeAffinity apiVersion: v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: metadata.name operator: In values: - node-geektime ","date":"2023-05-20","objectID":"/daemonset/:2:0","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":["文档"],"content":"含义 requiredDuringSchedulingIgnoredDuringExecution：这个nodeAffunuty 必须在每个调度的时候给予考虑， 同时 意味着可以设置在某个情况下不考虑这个nodeAffinity 这个Pod, 将来只允许允许在“metadata.name是“node-geektime”的节点上 Daemonset Controller 会在创建Pod的时候， 自动在这个Pod的API对象里， 加上这样一个nodeAffinity定义 ","date":"2023-05-20","objectID":"/daemonset/:2:1","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":["文档"],"content":"tolerations 这个字段： 意味着这个Pod， 会“容忍”（Toleration）某些Node的污点（Taint） apiVersion: v1 kind: Pod metadata: name: with-toleration spec: tolerations: - key: node.kubernetes.io/unschedulable operator: Exists effect: NoSchedule ","date":"2023-05-20","objectID":"/daemonset/:3:0","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":["文档"],"content":"含义 toleration： “容忍”所有被标记为unschedulable“污点”的Node， “容忍”的效果是允许调度 在 Kubernetes 项目中，当一个节点的网络插件尚未安装时，这个节点就会被自动加上名为node.kubernetes.io/network-unavailable的“污点”。 而通过这样一个 Toleration，调度器在调度这个 Pod 的时候，就会忽略当前节点上的“污点”，从而成功地将网络插件的 Agent 组件调度到这台机器上启动起来。 ","date":"2023-05-20","objectID":"/daemonset/:3:1","tags":["k8s"],"title":"Kubernets-DaemonSet","uri":"/daemonset/"},{"categories":["文档"],"content":"Deployment ","date":"2023-05-20","objectID":"/deployment/:0:0","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":["文档"],"content":"Deployment Pod 的 “水平扩展、收缩” ","date":"2023-05-20","objectID":"/deployment/:0:1","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":["文档"],"content":"ReplicaSet apiVersion: apps/v1 kind: ReplicaSet metadata: name: nginx-set labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 一个ReplicaSet对象， 其实就是由副本数目的定义和一个Pod模板组合， 更重要的是 Deployment控制器实际操纵的， 正是ReplicasSet对象， 而不是Pod对象 ","date":"2023-05-20","objectID":"/deployment/:0:2","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":["文档"],"content":"Deployment apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 ","date":"2023-05-20","objectID":"/deployment/:0:3","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":["文档"],"content":"Deployment 、 ReplicaSet、 Pod的关系 ","date":"2023-05-20","objectID":"/deployment/:0:4","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":["文档"],"content":"水平扩展、收缩的操作 $ kubectl scale deployment nginx-deployment --replicas=4 deployment.apps/nginx-deployment scaled ","date":"2023-05-20","objectID":"/deployment/:0:5","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":["文档"],"content":"滚动更新 $ kubectl create -f nginx-deployment.yaml --record -record 参数： 记录每次操作的执行命令， 方便后面查看 检查一下 nginx-deployment 创建后的状态信息 $ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 0 0 0 1s 状态含义 DESIERD：用户期望的Pod 副本个数 CURRENT：当前处于Running状态的Pod的个数 UP-TO-DATE： 当前处于最新版的Pod的个数， （Pod的Spec部分与Deployment的Pod模板的定义的一致） AVALABLE： 当前已经可用的Pod的个数， 既是 Running 状态，又是最新版本，并且已经处于 Ready（健康检查正确）状态的 Pod 的个数。 查看Deployment对象的状态变化kubectl rollout status $ kubectl rollout status deployment/nginx-deployment Waiting for rollout to finish: 2 out of 3 new replicas have been updated... deployment.apps/nginx-deployment successfully rolled out ","date":"2023-05-20","objectID":"/deployment/:0:6","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":["文档"],"content":"Deployment对应进行版本控制的具体原理 这个镜像名字修改成为了一个错误的名字，比如：nginx:1.91。这样，这个 Deployment 就会出现一个升级失败的版本。 $ kubectl set image deployment/nginx-deployment nginx=nginx:1.91 deployment.extensions/nginx-deployment image updated 由于这个 nginx:1.91 镜像在 Docker Hub 中并不存在，所以这个 Deployment 的“滚动更新”被触发后，会立刻报错并停止。 这时，我们来检查一下 ReplicaSet 的状态，如下所示： $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-1764197365 2 2 2 24s nginx-deployment-3167673210 0 0 0 35s nginx-deployment-2156724341 2 2 0 7s 回滚到一起的旧版本 $ kubectl rollout undo deployment/nginx-deployment deployment.extensions/nginx-deployment 要使用 kubectl rollout history 命令，查看每次 Deployment 变更对应的版本 $ kubectl rollout history deployment/nginx-deployment deployments \"nginx-deployment\" REVISION CHANGE-CAUSE 1 kubectl create -f nginx-deployment.yaml --record 2 kubectl edit deployment/nginx-deployment 3 kubectl set image deployment/nginx-deployment nginx=nginx:1.91 查看每个版本对应的Deployment的API对象的细节 $ kubectl rollout history deployment/nginx-deployment --revision=2 可以在kubectl roolout undo 命令加上回滚的版本号， 指定版本回滚 $ kubectl rollout undo deployment/nginx-deployment --to-revision=2 deployment.extensions/nginx-deployment 对Deployment的多次更新操作， 最后只生成一个ReplicaSet， $ kubectl rollout pause deployment/nginx-deployment deployment.extensions/nginx-deployment paused 是Deployment进入一个暂定状态， 可以随意使用 kubectl edit 或者 kubectl set image 指令，修改这个 Deployment 的内容了。 操作完成之后将Deployment 恢复回来 $ kubectl rollout resume deploy/nginx-deployment deployment.extensions/nginx-deployment resumed ","date":"2023-05-20","objectID":"/deployment/:0:7","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":["文档"],"content":"如何控制“历史的” ReplicaSet数量 Deployment 对象有一个字段，叫作 spec.revisionHistoryLimit，就是 Kubernetes 为 Deployment 保留的“历史版本”个数。所以，如果把它设置为 0，你就再也不能做回滚操作了 ","date":"2023-05-20","objectID":"/deployment/:0:8","tags":["k8s"],"title":"Kubernets-Deployment","uri":"/deployment/"},{"categories":["文档"],"content":"pod ","date":"2023-05-17","objectID":"/pod/:0:0","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"pod 的属性 凡是调度、网络、存储、以及安全相关的属性， 都是pod级别的 ","date":"2023-05-17","objectID":"/pod/:0:1","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"NodeSelector 用户将Pod 与Node进行绑定的字段 apiVersion: v1 kind: Pod ... spec: nodeSelector: disktype: ssd ","date":"2023-05-17","objectID":"/pod/:0:2","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"NodeName 一旦pod 这个字段被赋值， Kubernetes项目就会被认为这个POd以及经过调度， 调度的结果就是复制的节点名字 ","date":"2023-05-17","objectID":"/pod/:0:3","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"HostAliases 定义了Pod的Hosts文件**（比如 /etc/hosts）里的内容**，用法如下： apiVersion: v1 kind: Pod ... spec: hostAliases: - ip: \"10.1.2.3\" hostnames: - \"foo.remote\" - \"bar.remote\" ... pod启动之后、 /etc/hosts 文件内容如下 cat /etc/hosts # Kubernetes-managed hosts file. 127.0.0.1 localhost ... 10.244.135.10 hostaliases-pod 10.1.2.3 foo.remote 10.1.2.3 bar.remote ","date":"2023-05-17","objectID":"/pod/:0:4","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"shareProcessNamespace=true： apiVersion: v1 kind: Pod metadata: name: nginx spec: shareProcessNamespace: true containers: - name: nginx image: nginx - name: shell image: busybox stdin: true tty: true pod 的容器共享 PID Namespace ","date":"2023-05-17","objectID":"/pod/:0:5","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"ImagePullPolicy 默认是Always ，每次都出重新拉取 Never： 意味着Pod永远不会主动拉取这个镜像， IfNotPresent： 只在宿主机不存在这个镜像时才拉取 ","date":"2023-05-17","objectID":"/pod/:0:6","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"lifecycle 在容器发生变化的时候触发一系列“钩子” apiVersion: v1 kind: Pod metadata: name: lifecycle-demo spec: containers: - name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: [\"/bin/sh\", \"-c\", \"echo Hello from the postStart handler \u003e /usr/share/message\"] preStop: exec: command: [\"/usr/sbin/nginx\",\"-s\",\"quit\"] ","date":"2023-05-17","objectID":"/pod/:0:7","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"pod对象在kubernetes中的生命周期 Pending：这个状态意味着， Pod的Yaml 文件以及提交给了kubernetes， API对象已经被创建并保存在Etcd 当中， 但是， 这个Pod 里有些容器因为某些原因不能被顺利创建， 比如， 调度不成功 Running： Pod已经调度成功， 跟一个具体的节点绑定，它包含的容器已经被创建，并且至少有一个已经运行成功 Succeeded： Pod的所有容器都运行成功， 并且已经退出， Failed： 这个状态下， Pod 里至少有一个容器以不正常的状态退出， 这个砖头的出现， 意味着你想办法Debug这个容器的应用， unknown： 异常状态， 意味着Pod的状态不能持续的被kubelet汇报给kube-apiserver， 很有可能是主从节点的通信出现了问题 ","date":"2023-05-17","objectID":"/pod/:0:8","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"projected Volume 四种 Secret ConfigMap Downward API ServiceAccountToken ","date":"2023-05-17","objectID":"/pod/:0:9","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"secret 把Pod想要访问的加密数据存在Etcd 中， 可以通过Pod的容器挂载Volume的方式， 访问这些Secret 保存的信息 ","date":"2023-05-17","objectID":"/pod/:0:10","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"configmap 保存的是不需要加密的， 应用所需的配置信息 ","date":"2023-05-17","objectID":"/pod/:0:11","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"Downward API 让Pod里的容器能够直接获取到这个Pod API对象本身的信息 apiVersion: v1 kind: Pod metadata: name: test-downwardapi-volume labels: zone: us-est-coast cluster: test-cluster1 rack: rack-22 spec: containers: - name: client-container image: k8s.gcr.io/busybox command: [\"sh\", \"-c\"] args: - while true; do if [[ -e /etc/podinfo/labels ]]; then echo -en '\\n\\n'; cat /etc/podinfo/labels; fi; sleep 5; done; volumeMounts: - name: podinfo mountPath: /etc/podinfo readOnly: false volumes: - name: podinfo projected: sources: - downwardAPI: items: - path: \"labels\" fieldRef: fieldPath: metadata.labels 支持的字段 spec.nodeName - 宿主机名字 status.hostIP - 宿主机 IP metadata.name - Pod 的名字 metadata.namespace - Pod 的 Namespace status.podIP - Pod 的 IP ","date":"2023-05-17","objectID":"/pod/:0:12","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"Service Account Service Account 对象的作用，就是 Kubernetes 系统内置的一种“服务账户”，它是 Kubernetes 进行权限分配的对象 Service Account 的授权信息和文件，实际上保存在它所绑定的一个特殊的 Secret 对象里的。这个特殊的 Secret 对象，就叫作ServiceAccountToken 这种把 Kubernetes 客户端以容器的方式运行在集群里，然后使用 default Service Account 自动授权的方式，被称作“InClusterConfig”，也是我最推荐的进行 Kubernetes API 编程的授权方式。 ","date":"2023-05-17","objectID":"/pod/:0:13","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"容器的健康检查和恢复机制 容器定义一个监控检查“探针”， kubelet就会根据这个Probe的返回值决定这个容器的状态，而不是直接以容器进行是否运行（来自 Docker 返回的信息）作为依据。这种机制，是生产环境中保证应用健康存活的重要手段。 apiVersion: v1 kind: Pod metadata: labels: test: liveness name: test-liveness-exec spec: containers: - name: liveness image: busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 ## 在容器启动后5s后开始执行 periodSeconds: 5 ## 每5s执行一次 Kubernetes 里的Pod 恢复机制，也叫 restartPolicy ","date":"2023-05-17","objectID":"/pod/:0:14","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"基本原理 只有Pod 的 restartPolicy 指定的策略允许重启的容器， 那么这个Pod就会保持Running状态，并进行容器重启 对于包含多个容器的Pod，只有它里面所有的容器都进入异常状态后，Pod 才会进入 Failed 状态 。在此之前，Pod 都是 Running 状态。此时，Pod 的 READY 字段会显示正常容器的个数 ","date":"2023-05-17","objectID":"/pod/:0:15","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"},{"categories":["文档"],"content":"PodPreset PodPreset里定义的内容， 只会在Pod API对象被创建之前追加这个对象本身上， 而不会影响热河Pod的控制的定义 PodPreset 这样专门用来对 Pod 进行批量化、自动化修改的工具对象 ","date":"2023-05-17","objectID":"/pod/:0:16","tags":["k8s"],"title":"Kubernets-Pod","uri":"/pod/"}]